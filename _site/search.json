[
  {
    "objectID": "live_coding/week_1/analysis_of_airbnb_data.html",
    "href": "live_coding/week_1/analysis_of_airbnb_data.html",
    "title": "Problem Class 1",
    "section": "",
    "text": "We have seen a few short examples in this week’s lectures that illustrated the application of some functions in the dplyr R package. In this problem class we now study a more complex data set. We will also consider a few functions we did not cover so far, but which are useful in a wide range of applications.\nOverview and Aims\nIn April 2018, all the information for properties listed on the online platform Airbnb for Rio de Janeiro, Brazil, was extracted. The data set is stored in the file “Data/Airbnb Rio April 2018.csv” on Moodle and contains six variables:\n\nhost_id - ID of the host on Airbnb\nhost_since - Date the host started to list their property on Airbnb\nneighbourhood - Neighbourhood where the property is located\nguests_included - Number of people included in the price\nprice - Price per night\nreview_scores_rating - Average rating for the property\n\nOur aim is to address two research questions using the methods we covered so far:\n\nWhich neighbourhoods have a high number of listed properties?\nHow does the price one would expect to pay vary across neighbourhoods?\n\nBefore starting the exercise, let’s load the dplyr R package:\n\nlibrary( dplyr )\n\nData Cleaning\nWe start by loading the data into our R Workspace using the read.csv() function and looking at the loaded data,\n\nAirbnb_raw &lt;- read.csv( \"Data/Airbnb Rio April 2018.csv\", header=TRUE )\nglimpse( Airbnb_raw )\n\nRows: 39,743\nColumns: 6\n$ host_id              &lt;int&gt; 53598, 68997, 99249, 102840, 135635, 153232, 1536…\n$ host_since           &lt;chr&gt; \"12/11/2009\", \"08/01/2010\", \"26/03/2010\", \"03/04/…\n$ neighbourhood        &lt;chr&gt; \"Botafogo\", \"Copacabana\", \"Ipanema\", \"Copacabana\"…\n$ guests_included      &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 1, 1, 7, 1, 8, 2, 4, 4, 1, 2…\n$ price                &lt;chr&gt; \"$133.00\", \"$270.00\", \"$222.00\", \"$161.00\", \"$222…\n$ review_scores_rating &lt;int&gt; 91, 93, 95, 94, 96, 94, 97, NA, 80, 87, 97, 80, 9…\n\n\nTask 1: Which variables are required for addressing the research questions? Define a data frame Airbnb which only contains the necessary variables.\n\nAirbnb &lt;- Airbnb_raw %&gt;%\n  select( neighbourhood, guests_included, price )\nglimpse( Airbnb )\n\nRows: 39,743\nColumns: 3\n$ neighbourhood   &lt;chr&gt; \"Botafogo\", \"Copacabana\", \"Ipanema\", \"Copacabana\", \"Ip…\n$ guests_included &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 1, 1, 7, 1, 8, 2, 4, 4, 1, 2, 4, …\n$ price           &lt;chr&gt; \"$133.00\", \"$270.00\", \"$222.00\", \"$161.00\", \"$222.00\",…\n\n\nTask 2: Convert the remaining variables to the correct type. Hint: You may want to consider the function gsub() to remove special symbols from a string of characters.\n\nAirbnb &lt;- Airbnb %&gt;%\n  mutate( price = gsub(replacement = \"\", pattern=\"\\\\$\", price) )\nglimpse( Airbnb )\n\nRows: 39,743\nColumns: 3\n$ neighbourhood   &lt;chr&gt; \"Botafogo\", \"Copacabana\", \"Ipanema\", \"Copacabana\", \"Ip…\n$ guests_included &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 1, 1, 7, 1, 8, 2, 4, 4, 1, 2, 4, …\n$ price           &lt;chr&gt; \"133.00\", \"270.00\", \"222.00\", \"161.00\", \"222.00\", \"308…\n\n\n\nAirbnb &lt;- Airbnb %&gt;%\n  mutate( price = gsub(replacement = \"\", pattern=\",\", price) )\nglimpse( Airbnb )\n\nRows: 39,743\nColumns: 3\n$ neighbourhood   &lt;chr&gt; \"Botafogo\", \"Copacabana\", \"Ipanema\", \"Copacabana\", \"Ip…\n$ guests_included &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 1, 1, 7, 1, 8, 2, 4, 4, 1, 2, 4, …\n$ price           &lt;chr&gt; \"133.00\", \"270.00\", \"222.00\", \"161.00\", \"222.00\", \"308…\n\n\n\nAirbnb &lt;- Airbnb %&gt;%\n  mutate( price = as.numeric(price) )\nglimpse( Airbnb )\n\nRows: 39,743\nColumns: 3\n$ neighbourhood   &lt;chr&gt; \"Botafogo\", \"Copacabana\", \"Ipanema\", \"Copacabana\", \"Ip…\n$ guests_included &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 1, 1, 7, 1, 8, 2, 4, 4, 1, 2, 4, …\n$ price           &lt;dbl&gt; 133, 270, 222, 161, 222, 308, 219, 150, 120, 3241, 106…\n\n\nResearch Question 1\nWith the data now being ready, we can turn to our first research question:\nTask 3: Use the functions in the dplyr package to extract the number of properties per neighbourhood. Hint: You may want to consider the function n() which returns the number of rows in a data frame when combined with summarise(). Alternatively, you can also look up the function count().\n\nAirbnb_Numbers &lt;- Airbnb %&gt;%\n  group_by( neighbourhood ) %&gt;%\n  summarise( \"Number\" = n() )\nglimpse( Airbnb_Numbers )\n\nRows: 156\nColumns: 2\n$ neighbourhood &lt;chr&gt; \"Abolição\", \"Alto da Boa Vista\", \"Anchieta\", \"Andaraí\", …\n$ Number        &lt;int&gt; 8, 59, 9, 102, 87, 3, 17, 4528, 66, 4, 8, 13, 8, 1930, 1…\n\n\nTask 4: Sort the neighbourhoods based on the number of listed properties. For all neighbourhoods with more than 1000 listed properties, print the name of the neighbourhood and the number of listed properties. What do we conclude?\n\nAirbnb_Numbers %&gt;%\n  arrange( desc(Number) ) %&gt;%\n  filter( Number&gt;1000 )\n\n# A tibble: 10 × 2\n   neighbourhood            Number\n   &lt;chr&gt;                     &lt;int&gt;\n 1 Copacabana                 9806\n 2 Barra da Tijuca            4528\n 3 Ipanema                    3351\n 4 Jacarepaguá                2382\n 5 Recreio dos Bandeirantes   2096\n 6 Botafogo                   1930\n 7 Leblon                     1816\n 8 Santa Teresa               1312\n 9 Centro                     1049\n10 Flamengo                   1035\n\n\nResearch Question 2\nThis research question is a bit more open-ended than Research Question 1. So we should first consider which information may be best suited for addressing the research question.\nOne possible answer is to study price per guest, as properties that can house a larger number of guests are likely to be more expensive. Summaries that may be helpful to extract are the average and median price per guest for a listed property within the neighbourhood.\nTask 5: Why may it be good to also consider the median, instead of just the mean?\nTask 6: Create a data frame which provides the number of listed properties, and the average and median price per guest, for each neighbourhood with more than 1000 listed properties. What do we conclude?\n\nAirbnb &lt;- Airbnb %&gt;%\n  mutate( Price_per_Guest = price/guests_included )\nglimpse( Airbnb )\n\nRows: 39,743\nColumns: 4\n$ neighbourhood   &lt;chr&gt; \"Botafogo\", \"Copacabana\", \"Ipanema\", \"Copacabana\", \"Ip…\n$ guests_included &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 1, 1, 7, 1, 8, 2, 4, 4, 1, 2, 4, …\n$ price           &lt;dbl&gt; 133, 270, 222, 161, 222, 308, 219, 150, 120, 3241, 106…\n$ Price_per_Guest &lt;dbl&gt; 66.500, 135.000, 111.000, 80.500, 111.000, 154.000, 10…\n\n\n\nAirbnb %&gt;%\n  group_by( neighbourhood ) %&gt;%\n  summarise( \"Number\"=n(),\n             Average = mean(Price_per_Guest),\n             Median = quantile(Price_per_Guest, 0.5) ) %&gt;%\n  filter( Number&gt;1000 )\n\n# A tibble: 10 × 4\n   neighbourhood            Number Average Median\n   &lt;chr&gt;                     &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 Barra da Tijuca            4528    897.   349 \n 2 Botafogo                   1930    331.   167.\n 3 Centro                     1049    252.   120 \n 4 Copacabana                 9806    360.   180.\n 5 Flamengo                   1035    356.   151 \n 6 Ipanema                    3351    496.   249 \n 7 Jacarepaguá                2382    725.   360 \n 8 Leblon                     1816    599.   301 \n 9 Recreio dos Bandeirantes   2096    668.   332 \n10 Santa Teresa               1312    344.   130"
  },
  {
    "objectID": "live_coding/week_3/analysis_of_fire_data.html",
    "href": "live_coding/week_3/analysis_of_fire_data.html",
    "title": "Problem Class 3",
    "section": "",
    "text": "library(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(tidyr)"
  },
  {
    "objectID": "live_coding/week_3/analysis_of_fire_data.html#background",
    "href": "live_coding/week_3/analysis_of_fire_data.html#background",
    "title": "Problem Class 3",
    "section": "Background",
    "text": "Background\nThe Utopian Fire Department has gathered data on their activities for 2022. They also managed to provide us with access to some data for the houses in Utopia. We are asked to use the data to address the following questions:\n\nFor each cause, explore how the frequency of fires varied across the time of day.\nAre there any differences in the risk of fire for the different types of property?\nWhat is the relation between the year a property was built and the risk of fire?\n\nWe will explore how we can apply the techniques seen so far to address the three questions."
  },
  {
    "objectID": "live_coding/week_3/analysis_of_fire_data.html#data-descriptions",
    "href": "live_coding/week_3/analysis_of_fire_data.html#data-descriptions",
    "title": "Problem Class 3",
    "section": "Data Descriptions",
    "text": "Data Descriptions\nWe are provided with two data files for the analysis:\nData/Fires.csv: List of fires recorded by the Utopian Fire Department for 2022\n\nDate: Day and time the fire was reported\nCause: Cause the fire was attributed to (“Cooking”, “Electrical Fault”, “Heating” or “Other”)\nRegisterNumber: Identification number of the property as listed in the register of houses/properties\n\nData/Housing Register.csv: Register of houses/properties for Utopia at the beginning of 2022\n\nID: Identification number of the property\nYear: Year the property was built\nType: Type of property (“Flat”, “Terraced”, “Semi-detached” or “Detached”)\nBedroom: Number of bedrooms (studio apartments are counted as having 1 bedroom)"
  },
  {
    "objectID": "live_coding/week_3/analysis_of_fire_data.html#number-of-fires-over-time",
    "href": "live_coding/week_3/analysis_of_fire_data.html#number-of-fires-over-time",
    "title": "Problem Class 3",
    "section": "Number of fires over time",
    "text": "Number of fires over time\nWe start by loading the data set “Data/Fires.csv” and converting the variable Date to the correct type:\n\nFires &lt;- read.csv(\"data/fires.csv\" )\nFires &lt;- Fires %&gt;% mutate( Date = ymd_hm( Date ) )\n\n\nglimpse( Fires )\n\nRows: 33,118\nColumns: 3\n$ Date           &lt;dttm&gt; 2022-01-01 00:11:00, 2022-01-01 00:34:00, 2022-01-01 1…\n$ Cause          &lt;chr&gt; \"Cooking\", \"Electrical Fault\", \"Other\", \"Heating\", \"Hea…\n$ RegisterNumber &lt;int&gt; 8557338, 7861834, 4013959, 1640021, 3688588, 5304925, 8…\n\n\nTask 1: Create a facet plot, where each subplot provides a histogram of the number of fires per hour of the day for a different cause.\n\nggplot( Fires, aes( x=hour(Date) ) ) + \n  geom_histogram( bins=24 ) + coord_polar() + \n  facet_wrap( ~Cause ) + labs( x=\"Hour of the Day\", y=\"Number of Fires\")\n\n\n\n\n\n\n\nTask 2: What do you conclude from your plot produced in Task 1?"
  },
  {
    "objectID": "live_coding/week_3/analysis_of_fire_data.html#fire-risk-across-type-of-property",
    "href": "live_coding/week_3/analysis_of_fire_data.html#fire-risk-across-type-of-property",
    "title": "Problem Class 3",
    "section": "Fire risk across type of property",
    "text": "Fire risk across type of property\nWe now move onto analyzing the risk of fire.\nTask 3: How would you define “risk of fire” in this context?\nOne way is to consider the proportion of properties that reported fire amongst a subset of properties.\nLet’s load the data from the housing register and combine the two data sets\n\nHouses    &lt;- read.csv( \"Data/Housing Register.csv\" )\nFire_Risk &lt;- full_join( Fires, Houses, by=c(\"RegisterNumber\"=\"ID\") )\n\n\nglimpse(Fire_Risk)\n\nRows: 3,472,972\nColumns: 6\n$ Date           &lt;dttm&gt; 2022-01-01 00:11:00, 2022-01-01 00:34:00, 2022-01-01 1…\n$ Cause          &lt;chr&gt; \"Cooking\", \"Electrical Fault\", \"Other\", \"Heating\", \"Hea…\n$ RegisterNumber &lt;int&gt; 8557338, 7861834, 4013959, 1640021, 3688588, 5304925, 8…\n$ Year           &lt;int&gt; 2002, 1996, 1963, 1938, 1960, 1974, 1927, 1946, 1989, 2…\n$ Type           &lt;chr&gt; \"Terraced\", \"Terraced\", \"Terraced\", \"Terraced\", \"Terrac…\n$ Bedrooms       &lt;int&gt; 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 1, 3, 2, 3, 2, 2, 3, 4, 2…\n\n\n\nFire_Risk[1000000,]\n\n        Date Cause RegisterNumber Year     Type Bedrooms\n1000000 &lt;NA&gt;  &lt;NA&gt;        2811652 1951 Terraced        1\n\n\nTask 4: Create a variable to indicate whether a house was affected by a fire in 2022 or not.\n\nFire_Risk &lt;- Fire_Risk %&gt;%\n  mutate( Fire = case_when( is.na(Date) == TRUE ~ 0, is.na(Date) == FALSE ~ 1 ) )\n\nTask 5: For each type of property, calculate the proportion of properties which reported a fire in 2022. What do you conclude?\n\nFire_Risk %&gt;% \n  group_by( Type ) %&gt;%\n  summarise( Proportion = round( mean(Fire),4 ) )\n\n# A tibble: 4 × 2\n  Type          Proportion\n  &lt;chr&gt;              &lt;dbl&gt;\n1 Detached          0.0079\n2 Flat              0.009 \n3 Semi-detached     0.0079\n4 Terraced          0.0107\n\n\nThe lowest proportions of 0.8% are observed for Detached and semi-detached properties, while 1.1% of terraced houses reported a fire.\nTask 6: Explore whether some causes of fire are more frequent for certain types of property."
  },
  {
    "objectID": "live_coding/week_3/analysis_of_fire_data.html#relation-between-risk-of-fire-and-year-built",
    "href": "live_coding/week_3/analysis_of_fire_data.html#relation-between-risk-of-fire-and-year-built",
    "title": "Problem Class 3",
    "section": "Relation between risk of fire and year built",
    "text": "Relation between risk of fire and year built\nTask 7: Create a data graphic to explore the relation between risk of fire and the year a property was built. What do you conclude?\n\nggplot( Fire_Risk, aes(x=Year, y=Fire) ) + \n  geom_smooth() + facet_wrap( ~Type ) + \n  labs( x=\"Year Built\", y=\"Fire Risk\" )\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'"
  },
  {
    "objectID": "live_coding/week_4/analysis_of_jane_austen_(complete).html",
    "href": "live_coding/week_4/analysis_of_jane_austen_(complete).html",
    "title": "Problem Class 4 - Solution",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(tidytext)\nlibrary(stringr)"
  },
  {
    "objectID": "live_coding/week_4/analysis_of_jane_austen_(complete).html#background",
    "href": "live_coding/week_4/analysis_of_jane_austen_(complete).html#background",
    "title": "Problem Class 4 - Solution",
    "section": "Background",
    "text": "Background\nJane Austen wrote seven novels, and we consider six of these: “Sense and Sensibility”, “Pride and Prejudice”, “Mansfield Park”, “Emma”, “Northanger Abbey” and “Persuasion”.\nAll novels are available via Project Gutenberg and we will analyze similarities and differences of the different books in the following. We start by loading the text data which is available in the file “Data/JaneAusten.csv” on Moodle:\n\nJaneAusten_raw &lt;- read.csv(\"data/janeausten.csv\" )\n\nWe will analyze the six books and\n\nIdentify the most common words (except stop words) for each book\nCompare the six books in terms of their sentiment"
  },
  {
    "objectID": "live_coding/week_4/analysis_of_jane_austen_(complete).html#word-frequency-analysis",
    "href": "live_coding/week_4/analysis_of_jane_austen_(complete).html#word-frequency-analysis",
    "title": "Problem Class 4 - Solution",
    "section": "Word frequency analysis",
    "text": "Word frequency analysis\nAs before, we first have to bring the data into a usable format and remove stop words.\nTask 1: Split the lines of text into individual words and remove all stop words and underscores.\n\nWe copy the code we used for the analysis of the book “Jane Eyre”:\n\n\nJaneAusten &lt;- JaneAusten_raw %&gt;%\n  unnest_tokens( word, text ) %&gt;%\n  mutate( word = gsub( \"\\\\_\", \"\", word ) ) %&gt;%\n  anti_join( stop_words, by=\"word\" )\n\nWith the data in the desired format, we are ready to identify the most common words:\nTask 2: Extract the ten most common words (excluding stop words) for each book.\n\nSince we want to consider each book separately, we need to first use the group_by() function before using the functions we used for extracting counts:\n\n\nJaneAusten_Count &lt;- JaneAusten %&gt;%\n  group_by( title ) %&gt;%\n  count( word, name = \"Count\" ) %&gt;%\n  slice_max( Count, n=10 )\n\nAfter identifying the most frequent words, let’s visualize them. The following piece of code produces for each book a bar plot which visualizes the number of occurrences of the words identified in Task 2, i.e., the bar plot for a book provides information on the ten most common words (excluding stop words) contained in that book.\n\nggplot( JaneAusten_Count, \n        aes( x=Count, y=reorder_within(word,Count,title), fill=title ) ) + \n  facet_wrap( ~title, scales = \"free\" ) + \n  geom_col( show.legend = FALSE ) + \n  scale_y_reordered() + theme_bw() + \n  labs( x=\"Count\", y=\"Word\" )\n\n\n\nBar plots illustrating the frequency of the ten most common words for each book.\n\n\n\nTask 3: What is the benefit of using the functions reorder_within() and scale_y_reordered()? What do we conclude from the plot?\n\nWe have seen that we can use reorder() to order words according to their frequency. The functions reorder_within() and scale_y_reordered() allow us to do the same, but for subgroups. This is useful because we want to have a separate plot for each book.\n\n\nThe bar plots show that the names of the characters are used the most often, but a close inspection shows that “time” and “miss” are included in all bar plots.\n\nTo conclude our analysis on the words used within the books, we want to calculate the tf-idf values for each term.\nTask 4: Calculate the tf-idf value for each word and book. What do we conclude?\n\nWe apply the code as used in the analysis for the books by Charles Dickens:\n\n\nJaneAusten_tf.idf &lt;- JaneAusten %&gt;% \n  count( word, title, sort=T ) %&gt;%\n  bind_tf_idf( word, title, n ) %&gt;%\n  arrange( desc(tf_idf) )\nslice_head( JaneAusten_tf.idf, n=10 )\n\n        word                 title   n          tf      idf     tf_idf\n1     elinor Sense and Sensibility 685 0.018812996 1.791759 0.03370836\n2   marianne Sense and Sensibility 566 0.015544753 1.791759 0.02785246\n3     elliot            Persuasion 289 0.011328003 1.791759 0.02029706\n4      darcy   Pride and Prejudice 432 0.011019284 1.791759 0.01974391\n5     weston                  Emma 440 0.009446114 1.791759 0.01692516\n6     bennet   Pride and Prejudice 339 0.008647077 1.791759 0.01549348\n7  wentworth            Persuasion 218 0.008544998 1.791759 0.01531058\n8  knightley                  Emma 389 0.008351224 1.791759 0.01496338\n9      elton                  Emma 387 0.008308287 1.791759 0.01488645\n10   bingley   Pride and Prejudice 310 0.007907356 1.791759 0.01416808\n\n\n\nWe can also visualize the words with highest tf-idf value by adapting the code used to produce bar plots for word frequency:\n\n\nJaneAusten_tf.idf%&gt;%\n  group_by( title ) %&gt;%\n  slice_max( order_by = tf_idf, n=10 ) %&gt;%\n  ggplot( aes( x=tf_idf, y=reorder_within(word,tf_idf,title), fill=title ) ) + \n  facet_wrap( ~title, scales=\"free_y\") + \n  geom_col( show.legend = FALSE ) + \n  scale_y_reordered() + theme_bw() +\n  labs( x=\"tf-idf\", y=\"\" )\n\n\n\nBar plots illustrating the ten words with the highest tf-idf values for each book.\n\n\n\n\nThe displayed words are, as measured by the tf-idf, the most important to each novel and most readers would likely agree. We can also conclude that Jane Austen used similar language across her six novels, because more common words, such as “time” or “sister”, are no longer in the plot. Consequently, what distinguishes one novel from the rest within the collection of her works are the names of people and places. This is the point of tf-idf, it identifies words that are important to one document within a collection of documents."
  },
  {
    "objectID": "live_coding/week_4/analysis_of_jane_austen_(complete).html#sentiment-analysis",
    "href": "live_coding/week_4/analysis_of_jane_austen_(complete).html#sentiment-analysis",
    "title": "Problem Class 4 - Solution",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\nLet’s study the sentiment of the books using the AFINN sentiment lexicon:\n\nAFINN &lt;- get_sentiments( \"afinn\" )\n\nWe want to derive the sentiment score for each chapter in each book. To do this, we first need to identify which chapter each line belongs to. We can do this by adapting the code from Section 3.2 in the lecture notes:\n\nJaneAusten_chapters &lt;- JaneAusten_raw %&gt;%\n  group_by( title ) %&gt;%\n  mutate( chapter = cumsum( str_detect(\n    text, regex(\"^chapter \", ignore_case = TRUE)\n  ) ) ) %&gt;%\n  filter( chapter &gt; 0 ) %&gt;%\n  ungroup()\n\nThe next step is to split the lines of text into individual words and to match the words in the AFINN sentiment lexicon with the words in the books. As for Jane Eyre, we remove the word “miss” from the analysis\n\nJaneAusten_AFINN &lt;- JaneAusten_chapters %&gt;%\n  filter( chapter &gt; 0 ) %&gt;%\n  unnest_tokens( word, text ) %&gt;%\n  mutate( word = gsub( \"_\", \"\", word ) ) %&gt;%\n  inner_join( AFINN, by = \"word\" ) %&gt;% \n  filter( word != \"miss\" )\n\nTask 5: Derive the aggregated sentiment score for each chapter using the AFINN sentiment lexicon.\n\nWe calculate the aggregate sentiment score as follows:\n\n\nJaneAusten_AFINN   &lt;- JaneAusten_AFINN %&gt;%\n  group_by( title, chapter ) %&gt;%\n  summarise( sentiment = sum( value ) )\n\nHaving derived the sentiment score for each book and chapter, we visualize the results:\n\nggplot( JaneAusten_AFINN, aes( x=chapter, y=sentiment ) ) +\n  facet_wrap( ~title, scales=\"free_x\" ) +\n  geom_col( aes( fill=title ), show.legend = FALSE ) + \n  theme_bw() + labs( x=\"Chapter\", y=\"AFINN sentiment score\" ) \n\n\n\n\n\n\n\nTask 6: What do we conclude?\n\nWe see that almost all chapters have a positive sentiment score, i.e., this indicates that Jane Austen used a lot of words that are associated with a “positive” sentiment. However, we should keep in mind that not all parts of these books are actually happy stories, illustrating the limitations of our approach to measure sentiment. However, we can still comment on the differences in sentiment of chapters."
  },
  {
    "objectID": "live_coding/week_5/analysis_of_sea_surface_temperature.html",
    "href": "live_coding/week_5/analysis_of_sea_surface_temperature.html",
    "title": "Problem Class 5",
    "section": "",
    "text": "Sea surface temperature is the temperature of the top millimeter of the ocean’s surface. Sea surface temperatures influence weather, including hurricanes, as well as plant and animal life in the ocean. Like Earth’s land surface, sea surface temperatures are warmer near the equator and colder near the poles. Currents like giant rivers move warm and cold water around the world’s oceans. Some of these currents flow on the surface, and they are obvious in sea surface temperature images.\nIn this short exercise we consider the average sea surface temperature for the North Atlantic in August 2022 as recorded by NASA. The data is produced by placing a fine grid over the globe and then deriving the average temperature for each pixel.\nLet’s import the data\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nSea_raw &lt;- read.csv(\"data/seasurface.csv\" )\nglimpse(Sea_raw)\n\nRows: 40,256\nColumns: 3\n$ lat         &lt;dbl&gt; 69.875, 69.625, 69.375, 69.125, 68.875, 68.625, 68.375, 68…\n$ lon         &lt;dbl&gt; -64.875, -64.875, -64.875, -64.875, -64.875, -64.875, -64.…\n$ Temperature &lt;dbl&gt; 2.22, 2.22, 2.66, 3.10, 2.08, 2.37, 2.52, 3.54, 4.12, 2.81…\n\n\nOur aim is to visualize the spatial distribution, and we load the necessary packages\n\nlibrary(ggplot2)\nlibrary(sf)\n\nConsider the following tasks."
  },
  {
    "objectID": "live_coding/week_5/analysis_of_sea_surface_temperature.html#background",
    "href": "live_coding/week_5/analysis_of_sea_surface_temperature.html#background",
    "title": "Problem Class 5",
    "section": "",
    "text": "Sea surface temperature is the temperature of the top millimeter of the ocean’s surface. Sea surface temperatures influence weather, including hurricanes, as well as plant and animal life in the ocean. Like Earth’s land surface, sea surface temperatures are warmer near the equator and colder near the poles. Currents like giant rivers move warm and cold water around the world’s oceans. Some of these currents flow on the surface, and they are obvious in sea surface temperature images.\nIn this short exercise we consider the average sea surface temperature for the North Atlantic in August 2022 as recorded by NASA. The data is produced by placing a fine grid over the globe and then deriving the average temperature for each pixel.\nLet’s import the data\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nSea_raw &lt;- read.csv(\"data/seasurface.csv\" )\nglimpse(Sea_raw)\n\nRows: 40,256\nColumns: 3\n$ lat         &lt;dbl&gt; 69.875, 69.625, 69.375, 69.125, 68.875, 68.625, 68.375, 68…\n$ lon         &lt;dbl&gt; -64.875, -64.875, -64.875, -64.875, -64.875, -64.875, -64.…\n$ Temperature &lt;dbl&gt; 2.22, 2.22, 2.66, 3.10, 2.08, 2.37, 2.52, 3.54, 4.12, 2.81…\n\n\nOur aim is to visualize the spatial distribution, and we load the necessary packages\n\nlibrary(ggplot2)\nlibrary(sf)\n\nConsider the following tasks."
  },
  {
    "objectID": "live_coding/week_5/analysis_of_sea_surface_temperature.html#tasks",
    "href": "live_coding/week_5/analysis_of_sea_surface_temperature.html#tasks",
    "title": "Problem Class 5",
    "section": "Tasks",
    "text": "Tasks\nTask 1: What is the type of spatial data we are working with?\nTask 2: Convert the data into a spatial object using the st_as_sf() function:\n\nSea &lt;- st_as_sf( Sea_raw, coords=c(\"lon\", \"lat\"), crs=\"WGS84\" )\n\nTask 3: Visualize the data using the WGS84 coordinate system. What do you conclude?\n\nggplot( Sea ) + geom_sf( aes(color=Temperature) ) + theme_bw()\n\n\n\n\n\n\n\nTask 4: Visualize the data using coordinate system with CRS=3347. Which aspects are better represented by this projection?\n\nggplot( Sea ) + geom_sf( aes(color=Temperature) ) + theme_bw() + \n  coord_sf( crs=st_crs( 3347) )"
  },
  {
    "objectID": "live_coding/week_2/analysis_of_pet_data.html",
    "href": "live_coding/week_2/analysis_of_pet_data.html",
    "title": "Problem Class 2",
    "section": "",
    "text": "library(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(patchwork)"
  },
  {
    "objectID": "live_coding/week_2/analysis_of_pet_data.html#background",
    "href": "live_coding/week_2/analysis_of_pet_data.html#background",
    "title": "Problem Class 2",
    "section": "Background",
    "text": "Background\nThe Utopian charity Respect for Pets has collected data on cats and dogs for 1990-2023. Utopia only allows three dog breeds: Beagle, Dachshund and Maltese. In Utopia all pets have to be registered. The charity would like to gain some insight regarding the following questions:\n\nHow have the numbers of dogs and cats changed over time?\nHow has the popularity of the different dog breeds evolved since 1990?\nMaltese are known to experience respiratory issues, such as wheezing or asthma. How does temperature affect the risk of a Maltese experiencing these issues?\n\nThis problem class is inspired by a question on last year’s MA20277 Coursework 1, which almost used the exact same data. We will explore how we can apply the techniques seen so far to solve the three questions."
  },
  {
    "objectID": "live_coding/week_2/analysis_of_pet_data.html#data-descriptions",
    "href": "live_coding/week_2/analysis_of_pet_data.html#data-descriptions",
    "title": "Problem Class 2",
    "section": "Data Descriptions",
    "text": "Data Descriptions\nWe are provided with two data files for the analysis:\nData/Pets.csv: Information on the number of registered pets at the beginning of each month for January 1990 until September 2023:\n\nYear, Month: Year and month\nBeagles: Number of beagles\nDachshund: Number of dachshunds\nMaltese: Number of Maltese\nCats: Number of cats\n\nData/Cases.csv: Number of Maltese treated by a vet for respiratory issues for 1990-2022:\n\nDate: Date (day, month and year)\nTemperature: Daily maximum temperature in degree Celsius\nNumber: Number of Maltese admitted with respiratory issues"
  },
  {
    "objectID": "live_coding/week_2/analysis_of_pet_data.html#number-of-dogs-and-cats-over-time",
    "href": "live_coding/week_2/analysis_of_pet_data.html#number-of-dogs-and-cats-over-time",
    "title": "Problem Class 2",
    "section": "Number of dogs and cats over time",
    "text": "Number of dogs and cats over time\nWe start by loading the data into our R Workspace using the read.csv() function and looking at the loaded data,\n\nPets &lt;- read.csv(\"data/pets.csv\" )\nglimpse( Pets )\n\nRows: 405\nColumns: 6\n$ Year      &lt;int&gt; 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, …\n$ Month     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, …\n$ Beagles   &lt;int&gt; 81817, 81975, 82169, 82064, 81965, 82169, 82036, 81923, 8209…\n$ Dachshund &lt;int&gt; 108739, 109039, 109362, 109221, 109081, 109298, 109163, 1090…\n$ Maltese   &lt;int&gt; 73322, 73498, 73632, 73529, 73414, 73640, 73551, 73440, 7362…\n$ Cats      &lt;dbl&gt; 283178, 283367, 283692, 284040, 283864, 284058, 284653, 2850…\n\n\nTask 1: Which type of plot may be useful to explore how the numbers of dogs and cats have changed over time?\nBefore we create the plot, we need to do some data cleaning and wrangling. We start by converting the information on year and month into a variable of type Date. We can use the function ym() from the lubridate package for this:\n\nPets &lt;- Pets %&gt;% \n  mutate( Date = paste( Year, Month, sep=\"-\" ) ) %&gt;%\n  mutate( Date = ym( Date ) )\n\nSince we want to plot the total number of dogs, we further create a variable Dogs which contains this information:\n\nPets &lt;- Pets %&gt;% mutate( Dogs = Beagles + Dachshund + Maltese )\n\nTask 2: Produce a plot which illustrates how the numbers of cats and dogs have changed over time.\nTask 3: What do we conclude from the plot produced in Task 2?"
  },
  {
    "objectID": "live_coding/week_2/analysis_of_pet_data.html#popularity-of-dog-breeds",
    "href": "live_coding/week_2/analysis_of_pet_data.html#popularity-of-dog-breeds",
    "title": "Problem Class 2",
    "section": "Popularity of dog breeds",
    "text": "Popularity of dog breeds\nWe now turn our attention to the second research question.\nTask 4: How would you define popularity of a dog breed?\nTask 5: Create a plot (or plots) to explore how popularity for the three different dog breeds has evolved over time.\nTask 6: What do you conclude from your plot(s) created in Task 5?"
  },
  {
    "objectID": "live_coding/week_2/analysis_of_pet_data.html#effect-of-temperature-on-respiratory-issues",
    "href": "live_coding/week_2/analysis_of_pet_data.html#effect-of-temperature-on-respiratory-issues",
    "title": "Problem Class 2",
    "section": "Effect of temperature on respiratory issues",
    "text": "Effect of temperature on respiratory issues\nThe file “Data/Cases.csv” provides information on the daily temperature and number of Maltese with respiratory issues. So we start by loading this file:\n\nCases &lt;- read.csv(\"data/cases.csv\", header=TRUE )\n\nTask 7: Explain why plotting the daily temperature against the number of dogs admitted with respiratory issues is not a good approach. What may be a better approach to address this question?\nOne possible approach is the following:\n\nMaltese &lt;- Pets %&gt;% select( Year, Month, Maltese )\n\nCases &lt;- Cases %&gt;% \n  mutate( Year=year(Date), Month=month(Date) ) %&gt;%\n  full_join( Maltese, by=c( \"Year\"=\"Year\", \"Month\"=\"Month\" ) ) %&gt;%\n  mutate( Proportion = Number / Maltese )\n\nggplot( Cases, aes(x=Temperature, y=Proportion) ) + \n  geom_point() + geom_smooth() + \n  labs( y=\"Proportion of Maltese with respiratory issues\" )\n\n\n\n\n\n\n\nTask 8: What do we conclude from the plot? Which assumptions did we make in our analysis?"
  },
  {
    "objectID": "practice/week_7/lab_7.html",
    "href": "practice/week_7/lab_7.html",
    "title": "MA22019 2025 - Computer Lab 7",
    "section": "",
    "text": "Overview\nTutorial Question 1 considers an example similar to that in the lecture notes, and Tutorial Question 2 focuses on the use of projections for visualizing data.\nYou may want to load the following packages before starting the exercises:\n\nlibrary( dplyr )\nlibrary( ggplot2 )\nlibrary( sf )\nlibrary( ggspatial )\nlibrary( prettymapr )\n\nTutorial Question 1 - Temperature across north-eastern Brazil\nThe file “Brazil.csv” contains average daily temperatures between 2000 and 2021 for 149 weather stations in north-eastern Brazil. You also given a shapefile for Brazil. We want to analyze the data using the techniques for point-referenced data introduced in the lecture.\n\nVisualize the locations of the weather stations and average daily temperature using the provided shapefile. Use the functions xlim() and ylim() to zoom in on the area of Brazil the stations are located in.\nCreate a map with the ggspatial R package that illustrates the spatial locations of the weather stations and the recorded average daily temperatures. What do you conclude?\nUse inverse distance weighting to predict average daily temperature across north-eastern Brazil. Make sure to choose a suitable power parameter. Comment on the reliability of our estimates.\nTutorial Question 2 - Alaskan horsehair crab landings\nIn Section 4.1.3 we covered the topic of projections. The following tasks showcase one important aspect we should keep in mind when working with projections and demonstrates how we may handle it.\nThe data file “Crabs.csv” contains catch per unit effort (CPUE) data of commercial horsehair crab landings for various locations across the Alaskan Eastern Bering Sea for 2010-2018. CPUE is an indirect measure of the abundance of a species. We want to visualize the data using the shapefile for Alaska:\n\nAlaska &lt;- read_sf( \"Data/Shapefiles/Alaska.shp\" )\n\n\nCreate a plot of the provided shapefile. Is the WGS84 coordinate reference system being used for the plot?\n\nWe now want to explore the spatial data contained in “Crabs.csv”. Run the following piece of R code\n\nCrabs &lt;- read.csv(\"data/crabs.csv\" )\nggplot( data=Alaska ) + theme_bw() + geom_sf() + \n  geom_point( data=Crabs, aes(x=longitude, y=latitude), colour=2 ) \n\n\nConsider the data in “Crabs.csv” and discuss whether the map we produced is correct.\n\nLet’s change the projection of the shapefile with the coord_sf() and st_crs() functions covered in Section 4.1.2:\n\nggplot( data=Alaska ) + theme_bw() + geom_sf() + \n  coord_sf( crs=st_crs(4326) ) + xlim(-180,-140) + \n  geom_point( data=Crabs, aes(x=longitude, y=latitude) ) \n\n\nIs this plot more realistic than that in part b)? Adapt the code such that the recorded CPUE is shown in the plot. What do you conclude from the plot?\n\nInstead of changing the projection of the map, let’s define spatial data points using the data in “Crabs.csv”:\n\nCrabs_map &lt;- Crabs %&gt;% \n  st_as_sf( coords = c(\"longitude\", \"latitude\"), crs = \"WGS84\" )\nggplot( data=Alaska ) + theme_bw() + geom_sf() + \n  geom_sf( data=Crabs_map, aes(color=cpue) ) +\n  labs( x=\"Longitude\", y=\"Latitude\" )\n\n\nDo the locations of the points in the created plot match with the data shown in part c)? What can we learn from the considered R code about the use of projections?"
  },
  {
    "objectID": "practice/week_7/quiz_7.html",
    "href": "practice/week_7/quiz_7.html",
    "title": "MA22019 2025 - Quiz 7",
    "section": "",
    "text": "Overview\nThis week’s exercises help you with revising Sections 4.1 and 4.2 in the lecture notes. Exercises 1 asks you to identify the correct type of spatial data. Exercises 2 and 3 provide additional opportunities to visualize spatial data using shapefiles and maps, and to interpret the plots.\nYou may want to load the following packages before starting the exercises:\n\nlibrary( dplyr )\nlibrary( ggplot2 )\nlibrary( sf )\nlibrary( ggspatial )\nlibrary( prettymapr )\n\nExercise 1 - Types of spatial data\nFor each of the following applications, decide whether the data will be point-referenced, point pattern or lattice/areal data:\n\nOccurrence of tornadoes across Europe\nAir pollution levels across a city\nStrength of mobile phone signal\nDamage caused by the great spruce bark beetle across Western England\nSightings of Type Ia supernova\nExercise 2 - Rainfall across New Zealand\nThe file “NewZealandRain.csv” provides the aggregated rainfall between 01/09/2019 and 30/11/2019 for various locations across New Zealand. A shapefile for New Zealand is provided in the file “ShapeFileNZ.Rdata”.\n\nLoad the shapefile and create a plot of New Zealand.\nLoad the data in “NewZealandRain.csv”. Add points to the plot in part a) which represent the locations contained in “NewZealandRain.csv”, and use the visual cue colour to visualize the amount of precipitation on logarithmic scale recorded at the various locations\nUse inverse distance weighting with power parameter \\(p=2\\) to estimate the amount of precipitation between 01/09/2019 and 30/11/2019 for the following two locations: (i) 177°E Longitude and 38°S Latitude; (ii) 166°E Longitude and 51°S Latitude. You should use the original data and not apply the logarithmic scale from part b). Submit your answers (to two decimal places) in the Moodle quiz.\nExercise 3 - Earthquakes across Japan\nThe files “Japan earthquakes.csv” contains data for earthquakes which affected Japan between 2011 and 2018. The data has been collected by the United States Geological Survey. We want to use spatial data analysis to investigate the locations of the strongest earthquakes.\n\nLoad the data and extract the earthquakes with a magnitude of 6 or higher.\nCreate a map to visualize the locations and magnitude of the earthquakes extracted in part a).\nGo to Moodle and complete the quiz."
  },
  {
    "objectID": "practice/week_9/homework_9.html",
    "href": "practice/week_9/homework_9.html",
    "title": "MA22019 2025 - Homework 9",
    "section": "",
    "text": "Overview\nYour answer to the Homework Question can be submitted on Moodle to your tutor for feedback. The submission deadline is 17:00 on Thursday 24 April 2025. You should submit a single PDF or Word file that provides your R code, any created R output and all your comments.\nYou may want to load the following packages before starting the exercise:\n\nlibrary( dplyr )\nlibrary( ggplot2 )\nlibrary( tidyr )\nlibrary( sf )\nlibrary( ggspatial )\nlibrary( prettymapr )\nlibrary( spatstat )\n\nWhen working on a University PC, you will have to first install some of these packages.\nHomework Question - Tracking grey squirrels across the UK\nThe grey squirrel is classified as an invasive species in the UK, and it has displaced the native red squirrel across large parts of the UK. A wildlife conservation charity has collected data on reported sightings of grey squirrels for 2020-2022. The data they provided includes the following:\nGreySquirrels.csv: Sightings of grey squirrels reported to the wildlife conservation charity for 2020-2022:\n\nYear: Year the sighting happened\nLon: Longitude coordinate of the sighting\nLat: Latitude coordinate of the sighting\nAdmin: Name of the administrative area, where the grey squirrel was sighted\n\nUK Shapefile: Folder containing shapefiles for the UK\n\nUK.shp: Shapefile without administrative boundaries\nUK admin.shp: Shapefile with boundaries for the administrative areas named in the file “GreySquirrels.csv”\n\nThe charity assured us that the data is representative of the spatial distribution of squirrels across Great Britain for all years. They ask you to use the data to investigate the following aspects:\n\nWhat can we say about the spatial distribution of grey squirrels across Great Britain in 2022?\nAre there any areas of Great Britain that saw a notable change in the number of grey squirrels when we compare the data for 2020 and 2022?"
  },
  {
    "objectID": "practice/week_8/quiz_8.html",
    "href": "practice/week_8/quiz_8.html",
    "title": "MA22019 2025 - Quiz 8",
    "section": "",
    "text": "Overview\nThis week’s problem sheet focuses on the methods for analyzing spatial dependence introduced in Sections 4.3 and 4.4 in the lecture notes. Exercises 1-2 help you with revising the content of the lecture in Week 8.\nYou may want to load the following packages before starting the exercise:\n\nlibrary( dplyr )\nlibrary( lubridate )\nlibrary( ggplot2 )\nlibrary( tidyr )\nlibrary( sf )\nlibrary( ggspatial )\nlibrary( prettymapr )\nlibrary( sp )\nlibrary( gstat )\n\nWhen working on a University PC, you will have to first install some of these packages.\nExercise 1 - Exploring spatial structure in river flow data\nA data scientist used principal component analysis to explore the spatial structure / dependence in the daily maximum river flow values across 45 gauges in Northern England and the South of Scotland. They identified that the first 3 components explain more than 90% of the variation in the data and created the following plots representing the first three eigenvectors:\n\n\n\n\n\n\n\n\nProvide an interpretation for the three plots and then go to Moodle and complete the quiz.\nExercise 2 - Concentrations of zinc in the top soil\nThe file “Meuse.csv” gives topsoil zinc concentrations (in mg/kg) collected across 155 locations in a flood plain of the river Meuse. Specifically, we are given\n\nLon,Lat - Longitude and latitude coordinate of the locations\nZinc - Zinc concentration on logarithmic scale\n\nPerform the following steps to explore the spatial structure in the data:\n\nVisualize the zinc concentration on logarithmic scale using the ggspatial package. What do you conclude?\nExplore the spatial dependence in the zinc concentration on logarithmic scale by estimating the semi-variogram for the data. Go to Moodle and answer the quiz."
  },
  {
    "objectID": "practice/week_8/lab_8.html",
    "href": "practice/week_8/lab_8.html",
    "title": "MA22019 2025 - Computer Lab 8",
    "section": "",
    "text": "Overview\nTutorial Questions 1 and 2 each consider one of the two techniques we covered in Week 8.\nYou may want to load the following packages before starting the exercise:\n\nlibrary( dplyr )\nlibrary( lubridate )\nlibrary( ggplot2 )\nlibrary( tidyr )\nlibrary( sf )\nlibrary( ggspatial )\nlibrary( prettymapr )\nlibrary( sp )\nlibrary( gstat )\n\nWhen working on a University PC, you will have to first install some of these packages.\nTutorial Question 1 - Spatial dependence in soil moisture\nThe file “SoilMoisture.csv” contains soil moisture measurements for 1 July 2022 for the Iberian peninsula (i.e. Spain and Portugal). Data were collected using satellites and each point corresponds to the soil moisture recorded for an area of \\(0.25°\\times 0.25°\\). We want to visualize the data and estimate the semi-variogram:\n\nLoad the data into R and create a spatial plot which shows the soil moisture for each point, that is, the \\(x\\) and \\(y\\) axis should correspond to longitude and latitude respectively, with colour being used as a visual cue representing soil moisture. What do you conclude?\nRemove any grid cells with missing data from the data set using the function na.omit(). Why is this step necessary for estimating the semi-variogram?\nEstimate the semi-variogram using the function variogram() from the gstat R package. Visualize your estimate. What do you conclude?\nDiscuss whether it is reasonable to assume that the value of the semi-variogram is fully specified by the distance between spatial sites, which is the key assumption we make when using the variogram() function.\nTutorial Question 2 - Maximum monthly temperature across Germany\nThe file “Temperature Germany.csv” contains maximum monthly temperatures recorded for 20 sites in the northern half of Germany for 2000-2023. The longitude and latitude coordinates for the sites can be found in the file “Sites Germany.csv”. In the following we will visualize the data and analyse its spatial structure.\n\nFor all sites calculate their median monthly maximum temperature for June across the period 2000-2023. Create a plot to visualize the values you obtained. What do you conclude?\n\nWe now want to use principal component analysis to analyse the spatial structure in the data. One difference to the example from the lecture notes is that we standardize the data per month and site, rather than just per site. Formally, we standardize the data using [ {i,j,t} = (x{i,j,t} - {x}{i,j})/{i,j}, ] where \\(x_{i,j,t}\\) is the maximum temperature for site \\(i\\) in month \\(j\\) and year \\(t\\). We will have to perform this standardization “by hand” and can no longer rely on prcomp() to do it for us. Perform the following steps to perform PCA in this case and interpret your outputs:\n\nStandardize the data per site and month using the equation above.\nUse the pivot_wider() function to convert the data to a data frame such that each column contains the standardized observations for one side. Use the function na.omit() to remove any rows with missing data.\nApply the prcomp() function using the values obtained in Step 2.\nUsing the eigenvalues, decide on the number \\(m\\) of eigenvectors we should study.\nVisualize the first \\(m\\) eigenvectors, where \\(m\\) is the number you decided on in part Step 4. What do you conclude?"
  },
  {
    "objectID": "practice/week_1/editing_rmarkdown.html",
    "href": "practice/week_1/editing_rmarkdown.html",
    "title": "Editing the R Markdown file",
    "section": "",
    "text": "Changing the header and adding text\nAt the top of the file you find the header, which looks something like:\n\n---\ntitle: \"My title\"\nauthor: \"Christian Rohrbeck\"\ndate: \"07/07/2023\"\nformat: pdf\n---\n\nThis is where you can edit the header (title, author and date) of your document. You do not have to change the line starting with “output” to knit to HTML or Word.\nSection headings are specified using hashes, such as\n\n# R Markdown\n\nwith # being used for top level sections and ## for sub-sections, should you want to use them.\nTo add text to the various sections of your document, you just have to write it like in a Word document. The difference to Word is that you will not see the layout until you knit to your output file. In RStudio you will see the buttons “Source” and “Visual” to the upper left of your file. By switching to “Visual” you will have a limited preview.\nIf you want to start a new paragraph, you have to leave a blank line - it’s not enough to just start a new line.\nYou can use LaTeX in R Markdown documents to display equations. For example to show \\(\\sqrt{X}\\) you can use $\\(\\backslash\\)sqrt{X}$. For more details on using LaTeX equations in R Markdown, see here.\nInserting R code\nBetween the different lines of text (or paragraphs), we want to insert bits of R code, for instance, to create a plot that is inserted at this point of the document.\nWe can insert R code chunks using the keyboard short cut \\(\\mathrm{\\texttt{Ctrl + Alt + I}}\\) (\\(\\mathrm{\\texttt{Cmd + Option + I }}\\) on macOS). This will produce an R chunk that is presented as\n```{r}\n\n```\nYou can also type this directly into the R Markdown file.\nEverything that is typed within this chunk will be evaluated using R. For instance, we can generate two samples from a Normal(1,4) distribution using\n```{r}\nrnorm( 2, mean=1, sd=2 )\n```\n\n\n[1] 0.1808714 3.0330095\n\n\nTip: You will find a small green play button in the right upper corner of each R code chunk. You can use it to test the R chunk individually and to verify that your code is working correctly.\nLoading R packages\nIt is important to remember that R Markdown ignores any packages or variables loaded within your RStudio workspace when knitting to the PDF/HTML/Word files. Hence you have to load them separately within the document. This ensures reproducibility of your results.\nWhen loading R packages, it is best to suppress any messages because some messages may cause issues when knitting. To do this, we use the options warning=FALSE and message=FALSE when loading packages\n```{r, warning=FALSE, message=FALSE}\nlibrary(dplyr)\n```\nPlease also remember that any packages you load must be installed on your device.\nFigures in R Markdown\nBy putting the code for producing our data graphics into the R Markdown file, the figures are automatically generated and inserted into the knitted text document. However, the data graphics may be quite large, which is troubling when we have to stick to a page limit or are concerned about presentation.\nThere are multiple options we can use to control the size and position of the data graphic within the text document:\n\n\\(\\mathrm{\\texttt{out.width}}\\): Controls the width of the data graphic, relative to the width of the text.\n\\(\\mathrm{\\texttt{fig.height}}\\) and \\(\\mathrm{\\texttt{fig.width}}\\): Width and height of the plot - the size of the plot may still be changed using \\(\\mathrm{\\texttt{out.width}}\\)\n\\(\\mathrm{\\texttt{fig.align}}\\): Specifies alignment of the figure. Possible values are ‘default’, ‘left’, ‘right’, and ‘center’.\n\\(\\mathrm{\\texttt{fig.cap}}\\): Gives your graphic a caption.\n\\(\\mathrm{\\texttt{warning=FALSE}}\\) and \\(\\mathrm{\\texttt{message=FALSE}}\\) to suppress R warnings and messages. Only use this to save space in the final document and do not use from the start.\n\nThere are many more chunk options, which you can find in the R Markdown documentation. For the purposes of this course, the options above are the important ones.\nImportant: Please remember that you cannot have a line break when you specify options, no matter how many you set.\nExample: Let’s start by generating some random data samples in one chunk of R code\n```{r}\nset.seed(2022)\nx &lt;- rnorm( 100, mean=0, sd=1 )\ny &lt;- rnorm( 100, mean=0, sd=1 )\nobs &lt;- data.frame( \"x\"=x, \"y\"=y )\n```\nThis piece of R code does not produce any output because we stored everything as variables, and the generated data is stored in the data frame obs.\nTo visualize the data, we create a scatter plot of the two samples in a new R chunk\n```{r, fig.align='center', out.width='40%', fig.width=5, fig.height=5, fig.cap='Scatter plot of X against Y.'}\nplot( obs$x, obs$y, xlab=\"Samples for X\", ylab=\"Samples for Y\" )}\n```\nWhen knitting the document, these two chunks of R code produce the following output in the text document\n\nset.seed(2022)\nx &lt;- rnorm( 100 )\ny &lt;- rnorm( 100 )\nobs &lt;- data.frame( \"x\"=x, \"y\"=y )\n\n\nplot( obs$x, obs$y, xlab=\"Samples for X\", ylab=\"Samples for Y\" )\n\n\n\n\n\n\nScatter plot of X against Y."
  },
  {
    "objectID": "practice/week_6/homework_6.html",
    "href": "practice/week_6/homework_6.html",
    "title": "MA22019 2025 - Homework 6",
    "section": "",
    "text": "Overview\nYour answer to the Homework Question can be submitted on Moodle to your tutor for feedback. The submission deadline is 17:00 on Thursday 20 March 2025. You should submit a single PDF or Word file that provides your R code, any created R output and all your comments.\nYou may want to load the following packages before starting the exercise:\n\nlibrary( dplyr )\nlibrary( ggplot2 )\nlibrary( tidytext )\nlibrary( stringr )\nlibrary( tidyr )\nlibrary( topicmodels )\n\nWhen working on a University PC, you have to first install the tidytext package and any dependencies using\n\ninstall.packages( \"tidytext\", dependencies = TRUE )\n\nFor the sentiment analysis you can load the sentiment lexicons using\n\nAFINN &lt;- read.csv( \"data/AFINN Sentiment Lexicon.csv\" )\nBing &lt;- read.csv( \"data/Bing Sentiment Lexicon.csv\" )\n\nHomework Question - Sentiment Analysis vs Latent Dirichlet Allocation\nSo far we have used sentiment analysis to explore whether a statement has a positive or negative emotional intent. The purpose of this exercise is to apply sentiment analysis to another data set and to explore whether Latent Dirichlet Allocation (LDA) is able to identify differences in the language used for positive and negative reviews.\nWe will be working with customer reviews for British Airways. The reviews are stored in the file “British Airways Reviews.csv” and the following information is provided:\n\nrating - Score given by the customer (1=“very poor”, 10=“very good”)\ncountry - The country where the customer resides\nreview - Written comment provided by the customer\n\nPerform the following tasks using the techniques introduced in Chapter 3 of the lecture notes:\n\nDerive a sentiment score for each review based on the written comments. Explore how the score you obtain compares with the numerical rating given by the customer.\nEstimate a LDA model with \\(K=2\\) topics. Explore the relation between the proportions \\(\\psi_{1,1},\\ldots,\\psi_{N,1}\\) and the numerical score given by the customer. In your analysis you should carefully consider which words to include in the analysis.\nBased on your results in parts a) and b), discuss the performance of sentiment analysis and LDA in terms of identifying whether a review by a British Airways customer is more positive or more negative."
  },
  {
    "objectID": "practice/week_3/lab_3.html",
    "href": "practice/week_3/lab_3.html",
    "title": "MA22019 2025 -Computer Lab 3",
    "section": "",
    "text": "Overview\nThe tutorial questions ask you to apply functions from the dplyr, lubridate and ggplot2 packages. You should prioritize Tutorial Question 1, and only after completing it consider Tutorial Questions 2 and 3. Please make sure to use R Markdown for all your analyses.\nBefore starting the questions, please make sure to load the relevant R packages:\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nTutorial Question 1 - Utopia’s Ambulance Service\nThe health department of Utopia has collected data on the response time of their ambulance service, including whether patients were admitted to the country’s hospitals and how long they stayed. All the data are stored across the files “Ambulance.csv” and “Hospital.csv”. The variables in “Ambulance.csv” are\n\nCall - Day and time at which the ambulance service was notified\nCategory - Ambulance response category (1=”Life threatening”, 4=”non-urgent”)\nArrival - Time of arrival of the ambulance at the patient’s location\nPatientID - Health insurance number of the patient\nHospital - Time patient arrived at the hospital (missing values indicate that no hospitalization was required)\n\nThe file “Hospitals.csv” provides the following information:\n\nPatientID - Health insurance number of the patient\nAge - Age of the patient\nLength - Number of days the patient stayed in hospital\n\nWe are asked by the Utopian government to perform an analysis which only considers the patients who were brought to hospital by the ambulance service. For these patients, they ask us to address the following two questions:\n\nWhen considering patients within the different response categories, are there any differences in terms of time spent in hospital?\nWhat is the relation between a patient’s waiting time for the ambulance and their length of stay in hospital? Hint: You may want to consult the lubridate cheat sheet to help with converting dates to their correct type.\nTutorial Question 2 - 2021/22 UEFA Champions League\nThe files “UEFA goals.csv” and “UEFA attacking.csv” contain all the player stats on goals and assists for the UEFA Champions League season 2021-22. Use the data to extract the following information:\n\nWhich two teams scored the most goals in the 2021/22 UEFA Champions League? How many goals did they score?\nWhen ranking attackers, the sum of goals and assists is a widely used metric. Extract the top 10 players in terms of this metric. Hint: Not every player in the data recorded at least one goal and one assist.\nTutorial Question 3 - Working with discrete variables\nWhen plotting values of discrete variables (such as number of sales) some information may not be visible because points may lie on top of each other. We will explore how the functions geom_count() and geom_jitter() facilitate handling such situations.\nNOAA in the United States has recorded 332 billion-dollar natural disasters between 1980 and 2022, that is, each event caused overall damages in excess of $1 billion US dollars. The types of natural disasters include flooding, severe storms, droughts, etc. In this analysis we focus on the number of natural disasters per type and year and the data are provided in the file “US Billion Dollar Disasters.csv”.\n\nCreate a scatter plot for number of flood events against severe storms per year. Discuss which information can be drawn from the plot?\nReplace the function geom_point() in part a) by geom_count(). Which conclusions can be drawn from this new plot?\nReplace the function geom_point() in part a) by geom_jitter(). What does this function do? Which conclusions can be drawn from this new plot?"
  },
  {
    "objectID": "practice/week_3/quiz_3.html",
    "href": "practice/week_3/quiz_3.html",
    "title": "Quiz 3",
    "section": "",
    "text": "Overview\nExercises 1-3 help you with revising the techniques covered in the lectures in Week 2 (and Sections 1.3-2.2 in the lecture notes). You can check your solutions to these questions yourself by entering them on Moodle.\nBefore starting the questions, please make sure to load the relevant R packages:\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nExercise 1 - Identifying visual cues\nAn analysis of the Australian weather data, considered in the lecture, produced the following plot:\n\n\n\n\n\n\n\n\nIdentify the visual cues used in the plot and submit your answers via the quiz on Moodle.\nExercise 2 - Temperatures for Rio de Janeiro and Sao Paulo\nThe files “Rio Temperature.csv” and “Sao Paulo Temperature.csv” contain monthly average temperature measurements for Rio de Janeiro and Sao Paulo in Brazil. Missing values in the data matrix are stored as \\(\\mathrm{\\texttt{NA}}\\).\n\nJoin the temperature data for Sao Paulo and Rio de Janeiro into a single data frame using full_join(). Each row in this new data frame should provide the year, month and the monthly average temperatures for Rio de Janeiro and Sao Paulo. Rename any variables that have uninformative names.\n\nUsing the data frame produced in part a), answer the following two questions and submit your answers via the quiz on Moodle:\n\nWhat was the monthly average temperature for Rio de Janeiro when Sao Paulo recorded its highest monthly average temperature?\nWhat was the largest difference in the monthly average temperatures for Sao Paulo and Rio de Janeiro?\n\n\nExercise 3 - Weather in New York City in 2019\nThe file “NYC Weather 2019.csv” provides daily weather data for 2019 for New York City. We are given the following variables:\n\nDate: Day of the recording\nTmin, Tmax: Minimum and maximum temperature in degree Fahrenheit\nPrecipitation: Amount of precipitation in inches\n\nConsider the following tasks. You can submit your answers to parts c) and e) via the quiz on Moodle.\n\nCreate a density plot to visualize the distribution of the maximum daily temperature.\nSimilar to Section 2.2.3. in the lecture notes, create a line plot of day of year against amount of precipitation on that day.\nIdentify the date for which the difference in recorded daily minimum and maximum temperatures was at its highest in 2019. Enter your answer in the Day/Month format via the quiz on Moodle.\nCreate a plot which per month provides a box plot for the maximum daily temperature. Hint: Use the function month() to extract the month from a date and then apply the function factor() to convert it to a categorical variable.\n\nAnswer the following two questions and submit your questions via the quiz on Moodle:\n\nWhich month had the highest median daily maximum temperature according to your plot in part d)?\nWe learned that box plots should usually be ordered based on a criteria, such as the median. Should this approach also be taken for the box plot you created in part d)?"
  },
  {
    "objectID": "practice/week_5/lab_5.html",
    "href": "practice/week_5/lab_5.html",
    "title": "MA22019 2025 - Computer Lab 5",
    "section": "",
    "text": "Overview\nTutorial Question 1 is similar to Exercises 1-3, while Tutorial Question 2 explores how well the type of sentiment analysis we introduced performs at assessing sentiment for product reviews. Should time permit, you may want to work on the Homework Question during the tutorial.\nYou may want to load the following packages before starting the exercise:\n\nlibrary( dplyr )\nlibrary( ggplot2 )\nlibrary( tidytext )\nlibrary( wordcloud )\nlibrary( stringr )\n\nWhen working on a University PC, you have to first install the tidytext and wordcloud packages. The installation of the textdata package is quite cumbersome, and thus I provide the AFINN and Bing sentiment lexicons as .csv files for your convenience. You can load them using\n\nAFINN &lt;- read.csv( \"data/AFINN Sentiment Lexicon.csv\" )\nBing &lt;- read.csv( \"data/Bing Sentiment Lexicon.csv\" )\n\n\nTutorial Question 1 - Word frequency analysis for Les Misérables\n\nWe want to analyse the book Les Misérables by Victor Hugo in terms of the words used therein. The file “LesMis.csv” contains the data as provided by Project Gutenberg, but with the metadata at the beginning and end already removed. Consider the following questions:\n\nExtract the individual words from the text, and remove any stop words and underscores.\nFind the 10 most common in Les Misérables (excluding stop words) and create a bar plot which visualizes their number of occurrences.\nCreate a word cloud for the 30 most frequently used words (excluding stop words).\nTutorial Question 2 - Amazon Reviews\nOne important application area for sentiment analysis concerns the analysis of customer reviews. To explore how good the AFINN lexicon performs at capturing the overall sentiment of a review, we consider 1,000 Amazon product reviews. The data is stored in the file “AmazonReviews.csv”.\n\nReviews_raw &lt;- read.csv(\"data/amazonreviews.csv\" )\n\nThe data provides the rating (“1 Star” to “5 Stars”) and the accompanying text for each review. We want to derive the sentiment for each review and then compare it to the given score to assess how well we perform at capturing the sentiment of the text.\n\nUsing the AFINN sentiment lexicon, for each review, calculate the average sentiment per word.\nPlot the average sentiments calculated in part a) against the star ratings. What do you conclude?"
  },
  {
    "objectID": "practice/week_5/quiz_5.html",
    "href": "practice/week_5/quiz_5.html",
    "title": "MA22019 2025 - Quiz 5",
    "section": "",
    "text": "Overview\nThis week’s problem sheet focuses on the text data analysis techniques covered in Sections 3.1-3.3.1 in the lecture notes. Exercises 1-3 are designed to help you with revising the content of the lecture from Week 5. You can check your solutions to these questions yourself by answering the Moodle quiz.\nYou may want to load the following packages before starting the exercise:\n\nlibrary( dplyr )\nlibrary( ggplot2 )\nlibrary( tidytext )\nlibrary( wordcloud )\nlibrary( stringr )\n\nWhen working on a University PC, you have to first install the tidytext and wordcloud packages. The installation of the textdata package is quite cumbersome, and thus I provide the AFINN and Bing sentiment lexicons as .csv files for your convenience. You can load them using\n\nAFINN &lt;- read.csv( \"data/AFINN Sentiment Lexicon.csv\" )\nBing &lt;- read.csv( \"data/Bing Sentiment Lexicon.csv\" )\n\nExercise 1 - Short stories by Edgar Allan Poe\nThe file “Poe.csv” contains the book The Works of Edgar Allan Poe - Volume 1 which includes eight short stories by the American author Edgar Allen Poe. Each row in the data set gives a line from the book and the name of the short story that line belongs to. To load the data, we use\n\nPoe_raw &lt;- read.csv(\"data/poe.csv\" )\n\nThe following exercises are designed to help you with revising the steps we used for extracting the most common words for Jane Eyre in the lecture.\n\nExtract the individual words from the text and remove any underscores.\nWhich are the five most common words (including stop words) across all stories?\nFor each short story, identify the word most commonly used within it (excluding stop words).\nExercise 2 - Baby names in the USA between 1880 and 2017\nThe file “Babynames.csv” provides a comprehensive record of the names given to newborns in the United States between 1880 and 2017 according to the Social Security Administration. For each name, year and sex, we are given the number of newborn babies given that name (as long as the name was used at least five times in that year). We are further provided with the proportion of newborn babies who were given that name amongst babies of the same sex.\n\nWhat was the most common baby name over the period 1880-2017?\nCreate a word cloud which visualizes the 30 most common baby names (in terms of total number) for a girl between 1880 and 2017.\nExplore how the popularity of the girl name “Mary” evolved over the period 1880-2017. When did its popularity peak?\n\nExercise 3 - Analysis of the book Frankenstein\n\nThe novel Frankenstein by Mary Shelley tells the story of a young scientist who creates a creature via an experiment and is subsequently horrified by what he has made. The book is provided in the file “Frankenstein.csv” and can be loaded using\n\nFrankenstein_raw &lt;- read.csv(\"data/frankenstein.csv\", fileEncoding = \"UTF-8\" )\n\nPerform an analysis that addresses the following questions for Frankenstein:\n\nWhich five words appear the most often (excluding stop words)?\nCalculate the AFINN sentiment score (sum of the AFINN score of all words) for each chapter. Which chapter has the highest/lowest sentiment score?"
  },
  {
    "objectID": "practice/week_2/homework_2.html",
    "href": "practice/week_2/homework_2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Overview\nYour answer to the Homework Question can be submitted on Moodle to your tutor for feedback. The submission deadline is 17:00 on Thursday 13 February 2025. You should submit a single Word, PDF or HTML file that provides your R code, any created R output and all your comments.\nBefore starting the exercises, make sure to load the dplyr and lubridate R packages:\n\nlibrary(dplyr)\nlibrary(lubridate)\n\nIf you are using your own laptop, you may have to first run the code in “InstallPackages.R” (available from Moodle) to install all the packages essential for this course.\nHomework Question - Salaries for data scientists\nThe file “Salary Data Scientists.csv” provides data for 603 employees working in Data Science / Machine Learning / AI. For each employee, we are given the following information\n\nwork_year - Year the data were collected for the employee\nexperience_level - Experience Level: Entry-level (EN), Junior (MI), Senior (SE) or Expert (EX)\nemployment_type - Full-time (FT), Part-time (PT) or Contingent Workers (CT)\nsalary, salary_currency - Nominal salary and currency in which it was paid\nsalary_in_usd - Nominal salary in U.S. Dollars\nemployee_residence - Country the employee is residing\nremote_ratio - Proportion of time the employee may work from home\ncompany_location - Location of the company’s headquarters\ncompany_size - Size of the company: Small (S), Medium (M) or Large (L)\n\nAn undergraduate is considering to take units in Data Science / Machine Learning / AI. However, they are unsure about the working conditions they are likely to experience if they decide to work in the sector. Specifically, the student plans to work full-time for an U.S. company, and they want to get some insight into the differences between small-/medium-sized and large companies. After a discussion with the student, the following three aspects have been identified as important for them:\n\nOpportunity to work remotely for at least 50% of the time.\nSalaries for Entry-Level positions.\nStructure of the workforce in terms of experience level.\n\nPerform an analysis which considers the three aspects above. Make sure to clearly describe your approach and conclusions."
  },
  {
    "objectID": "coursework.html",
    "href": "coursework.html",
    "title": "Coursework",
    "section": "",
    "text": "Major coursework assignments.\nCoursework materials will be released during the semester.",
    "crumbs": [
      "Coursework"
    ]
  },
  {
    "objectID": "computing_setup/ide_choice_rstudio.html",
    "href": "computing_setup/ide_choice_rstudio.html",
    "title": "IDE Choice for Intro to Data Science: RStudio vs. VS Code/Positron",
    "section": "",
    "text": "Recommendation: Use RStudio as the primary teaching environment for MA22019.\nWhile VS Code and Positron represent the modern, multi-language future of Data Science (especially in 2026), RStudio remains the superior pedagogical tool for an introductory R-based course, particularly given the constraints of university infrastructure and student consistency.\n\n\n\nYou noted that 10% of students rely on university computers, and the network lacks Positron.\n\nThe “Broken Laptop” Problem: If you teach Positron, you effectively exclude the 10% of students who cannot install software. If a student’s laptop breaks during the semester, they must fall back to university PCs. If the course material relies on Positron-specific workflows (or VS Code extensions not present on the network), these students are stranded.\nThe “Standard Operating Environment” (SOE): RStudio is likely installed and stable on the network. VS Code on university networks is often stripped of necessary extensions or restricted from installing language servers, making the R experience subpar compared to a personal laptop.\n\n\n\n\nStudents are already taking other courses that use RStudio.\n\nUniformity reduces friction: If a student learns statistics in the morning using RStudio and data science in the afternoon using VS Code/Positron, they burn mental energy re-mapping shortcuts (e.g., Ctrl+Shift+M for pipe) and pane layouts instead of learning concepts.\nThe “Just Works” Factor: RStudio is purpose-built for R. It handles plotting, help pages, environment variables, and especially Quarto rendering with zero configuration. VS Code requires configuring vscode-R, httpgd, and path variables—problems that distract from the actual syllabus.\n\n\n\n\nYou rightly noted that “R is the engine, the IDE is the dashboard.”\n\nFor Beginners: The dashboard is the car. If the dashboard is confusing (too many buttons, need to edit JSON settings files), they assume the engine is broken. RStudio’s dashboard is opinionated and optimized strictly for R data science, making the engine approachability higher.\nFor Professionals: We want a customizable dashboard (VS Code) to handle multiple engines (Python, SQL, R, Julia). Beginners aren’t there yet.\n\n\n\n\nPositron is an exciting development (by Posit/RStudio themselves) and is likely the future successor to RStudio.\n\nWhy wait? Since it is not yet on the university network, it is not a “safe” default. Intro courses should prioritize stability over bleeding-edge features.\n2026 Reality: While VS Code is dominant in industry, RStudio is still the lingua franca of academic R. By teaching RStudio, you aren’t teaching “obsolete” tech; you are teaching the standard academic toolset.\n\n\n\n\nDon’t ban modern tools, but don’t require them.\n\nInstruction in RStudio: All live demos, screenshots, and lab instructions should assume RStudio. This guarantees 100% of students (including those on uni PCs) can follow along.\nThe “Advanced Track”: Explicitly state: “You are free to use VS Code or Positron if you prefer, but course staff cannot debug installation issues on your personal machine.” This empowers the top 10% of tech-savvy students without burdening the TAs.\nThe “Bridge” Lecture: In the final week (or a “Future Career” seminar), demonstrate a workflow in VS Code or Positron. Show why a professional might switch (e.g., better Copilot integration, Python interoperability). This plants the seed for their future growth without overwhelming them now.\n\n\nVerdict: Stick to RStudio for consistency, equity, and ease of teaching. The transition to VS Code/Positron is a natural evolution they can make in their second year or first job."
  },
  {
    "objectID": "computing_setup/ide_choice_rstudio.html#executive-summary",
    "href": "computing_setup/ide_choice_rstudio.html#executive-summary",
    "title": "IDE Choice for Intro to Data Science: RStudio vs. VS Code/Positron",
    "section": "",
    "text": "Recommendation: Use RStudio as the primary teaching environment for MA22019.\nWhile VS Code and Positron represent the modern, multi-language future of Data Science (especially in 2026), RStudio remains the superior pedagogical tool for an introductory R-based course, particularly given the constraints of university infrastructure and student consistency."
  },
  {
    "objectID": "computing_setup/ide_choice_rstudio.html#the-equity-infrastructure-constraint",
    "href": "computing_setup/ide_choice_rstudio.html#the-equity-infrastructure-constraint",
    "title": "IDE Choice for Intro to Data Science: RStudio vs. VS Code/Positron",
    "section": "",
    "text": "You noted that 10% of students rely on university computers, and the network lacks Positron.\n\nThe “Broken Laptop” Problem: If you teach Positron, you effectively exclude the 10% of students who cannot install software. If a student’s laptop breaks during the semester, they must fall back to university PCs. If the course material relies on Positron-specific workflows (or VS Code extensions not present on the network), these students are stranded.\nThe “Standard Operating Environment” (SOE): RStudio is likely installed and stable on the network. VS Code on university networks is often stripped of necessary extensions or restricted from installing language servers, making the R experience subpar compared to a personal laptop."
  },
  {
    "objectID": "computing_setup/ide_choice_rstudio.html#cognitive-load-context-switching",
    "href": "computing_setup/ide_choice_rstudio.html#cognitive-load-context-switching",
    "title": "IDE Choice for Intro to Data Science: RStudio vs. VS Code/Positron",
    "section": "",
    "text": "Students are already taking other courses that use RStudio.\n\nUniformity reduces friction: If a student learns statistics in the morning using RStudio and data science in the afternoon using VS Code/Positron, they burn mental energy re-mapping shortcuts (e.g., Ctrl+Shift+M for pipe) and pane layouts instead of learning concepts.\nThe “Just Works” Factor: RStudio is purpose-built for R. It handles plotting, help pages, environment variables, and especially Quarto rendering with zero configuration. VS Code requires configuring vscode-R, httpgd, and path variables—problems that distract from the actual syllabus."
  },
  {
    "objectID": "computing_setup/ide_choice_rstudio.html#the-engine-vs.-dashboard-philosophy",
    "href": "computing_setup/ide_choice_rstudio.html#the-engine-vs.-dashboard-philosophy",
    "title": "IDE Choice for Intro to Data Science: RStudio vs. VS Code/Positron",
    "section": "",
    "text": "You rightly noted that “R is the engine, the IDE is the dashboard.”\n\nFor Beginners: The dashboard is the car. If the dashboard is confusing (too many buttons, need to edit JSON settings files), they assume the engine is broken. RStudio’s dashboard is opinionated and optimized strictly for R data science, making the engine approachability higher.\nFor Professionals: We want a customizable dashboard (VS Code) to handle multiple engines (Python, SQL, R, Julia). Beginners aren’t there yet."
  },
  {
    "objectID": "computing_setup/ide_choice_rstudio.html#the-positron-factor-2026-context",
    "href": "computing_setup/ide_choice_rstudio.html#the-positron-factor-2026-context",
    "title": "IDE Choice for Intro to Data Science: RStudio vs. VS Code/Positron",
    "section": "",
    "text": "Positron is an exciting development (by Posit/RStudio themselves) and is likely the future successor to RStudio.\n\nWhy wait? Since it is not yet on the university network, it is not a “safe” default. Intro courses should prioritize stability over bleeding-edge features.\n2026 Reality: While VS Code is dominant in industry, RStudio is still the lingua franca of academic R. By teaching RStudio, you aren’t teaching “obsolete” tech; you are teaching the standard academic toolset."
  },
  {
    "objectID": "computing_setup/ide_choice_rstudio.html#recommended-strategy-the-rstudio-first-approach",
    "href": "computing_setup/ide_choice_rstudio.html#recommended-strategy-the-rstudio-first-approach",
    "title": "IDE Choice for Intro to Data Science: RStudio vs. VS Code/Positron",
    "section": "",
    "text": "Don’t ban modern tools, but don’t require them.\n\nInstruction in RStudio: All live demos, screenshots, and lab instructions should assume RStudio. This guarantees 100% of students (including those on uni PCs) can follow along.\nThe “Advanced Track”: Explicitly state: “You are free to use VS Code or Positron if you prefer, but course staff cannot debug installation issues on your personal machine.” This empowers the top 10% of tech-savvy students without burdening the TAs.\nThe “Bridge” Lecture: In the final week (or a “Future Career” seminar), demonstrate a workflow in VS Code or Positron. Show why a professional might switch (e.g., better Copilot integration, Python interoperability). This plants the seed for their future growth without overwhelming them now.\n\n\nVerdict: Stick to RStudio for consistency, equity, and ease of teaching. The transition to VS Code/Positron is a natural evolution they can make in their second year or first job."
  },
  {
    "objectID": "computing_setup/why_teach_github.html",
    "href": "computing_setup/why_teach_github.html",
    "title": "Should Data Science Students Learn GitHub?",
    "section": "",
    "text": "For a modern Data Science curriculum (MA22019), teaching GitHub is highly recommended. While it introduces initial cognitive load, it addresses critical industry requirements for reproducibility, collaboration, and portfolio building. Since the course already utilizes Quarto (.qmd), the integration with GitHub for publishing and version control provides immediate practical benefits that outweigh the learning curve.\n\n\n\n\nIn 2026, the definition of a “complete” analysis includes not just the results, but the full history of how those results were achieved.\n\nThe Problem: Without version control, file naming conventions degrade into analysis_v1.R, analysis_final.R, analysis_final_REAL.R. This makes auditing data transformations impossible.\nThe GitHub Solution: Git provides an unalterable history of who changed what and when. It allows students to roll back to a working state if they break their code—a massive safety net for beginners.\nEmployability: Proficiency in Git is often a “gatekeeper” skill in technical interviews. Employers expect junior data scientists to know how to clone a repo and push code on Day 1.\n\n\n\n\nFor students entering the job market, a GitHub profile acts as a dynamic CV.\n\nProof of Skill: Listing “R and Quarto” on a resume is a claim. Linking to a GitHub repository containing a clean, documented investigation (like the topics covered in 01-DataWrangling.qmd) is proof.\nVisibility: It demonstrates they understand professional workflows, markdown documentation, and project organization—soft skills that coding tests often miss.\n\n\n\n\nYour course materials use Quarto, which was designed with Git in mind.\n\nFree Publishing: Quarto natively supports publishing to GitHub Pages. Students can turn their homework or final projects into live websites/blogs with a single terminal command: bash     quarto publish gh-pages\nRendered Output vs. Source: It teaches the valuable distinction between source code (.qmd) and production artifacts (.html/.pdf).\n\n\n\n\nData Science is inherently collaborative.\n\nConflict Resolution: Without Git, students collaborating on a project must take turns editing files or risk overwriting each other’s work via cloud storage syncs (Dropbox/OneDrive).\nCode Review: GitHub Pull Requests (PRs) introduce the concept of code review—commenting on specific lines of code before they are merged. This is excellent practice for peer-grading or feedback.\n\n\n\n\nYou do not need to teach the full command-line “plumbing” of Git to get 90% of the value.\nThe “Happy Path” Workflow: Focus on just three concepts using the RStudio or VS Code GUI:\n\nPull/Clone: Get the latest version.\nStage & Commit: Save your work with a message.\nPush: Sync it to the cloud.\n\nConcepts to Skip (for now):\n\nBranching/Rebasing (unless doing advanced group work).\nCommand line interface (GUI buttons are fine for beginners).\nComplex .gitignore rules (provide a default one for them)."
  },
  {
    "objectID": "computing_setup/why_teach_github.html#executive-summary",
    "href": "computing_setup/why_teach_github.html#executive-summary",
    "title": "Should Data Science Students Learn GitHub?",
    "section": "",
    "text": "For a modern Data Science curriculum (MA22019), teaching GitHub is highly recommended. While it introduces initial cognitive load, it addresses critical industry requirements for reproducibility, collaboration, and portfolio building. Since the course already utilizes Quarto (.qmd), the integration with GitHub for publishing and version control provides immediate practical benefits that outweigh the learning curve."
  },
  {
    "objectID": "computing_setup/why_teach_github.html#industry-standard-for-reproducibility",
    "href": "computing_setup/why_teach_github.html#industry-standard-for-reproducibility",
    "title": "Should Data Science Students Learn GitHub?",
    "section": "",
    "text": "In 2026, the definition of a “complete” analysis includes not just the results, but the full history of how those results were achieved.\n\nThe Problem: Without version control, file naming conventions degrade into analysis_v1.R, analysis_final.R, analysis_final_REAL.R. This makes auditing data transformations impossible.\nThe GitHub Solution: Git provides an unalterable history of who changed what and when. It allows students to roll back to a working state if they break their code—a massive safety net for beginners.\nEmployability: Proficiency in Git is often a “gatekeeper” skill in technical interviews. Employers expect junior data scientists to know how to clone a repo and push code on Day 1."
  },
  {
    "objectID": "computing_setup/why_teach_github.html#the-portfolio-factor",
    "href": "computing_setup/why_teach_github.html#the-portfolio-factor",
    "title": "Should Data Science Students Learn GitHub?",
    "section": "",
    "text": "For students entering the job market, a GitHub profile acts as a dynamic CV.\n\nProof of Skill: Listing “R and Quarto” on a resume is a claim. Linking to a GitHub repository containing a clean, documented investigation (like the topics covered in 01-DataWrangling.qmd) is proof.\nVisibility: It demonstrates they understand professional workflows, markdown documentation, and project organization—soft skills that coding tests often miss."
  },
  {
    "objectID": "computing_setup/why_teach_github.html#synergy-with-quarto-your-current-stack",
    "href": "computing_setup/why_teach_github.html#synergy-with-quarto-your-current-stack",
    "title": "Should Data Science Students Learn GitHub?",
    "section": "",
    "text": "Your course materials use Quarto, which was designed with Git in mind.\n\nFree Publishing: Quarto natively supports publishing to GitHub Pages. Students can turn their homework or final projects into live websites/blogs with a single terminal command: bash     quarto publish gh-pages\nRendered Output vs. Source: It teaches the valuable distinction between source code (.qmd) and production artifacts (.html/.pdf)."
  },
  {
    "objectID": "computing_setup/why_teach_github.html#collaboration-group-work",
    "href": "computing_setup/why_teach_github.html#collaboration-group-work",
    "title": "Should Data Science Students Learn GitHub?",
    "section": "",
    "text": "Data Science is inherently collaborative.\n\nConflict Resolution: Without Git, students collaborating on a project must take turns editing files or risk overwriting each other’s work via cloud storage syncs (Dropbox/OneDrive).\nCode Review: GitHub Pull Requests (PRs) introduce the concept of code review—commenting on specific lines of code before they are merged. This is excellent practice for peer-grading or feedback."
  },
  {
    "objectID": "computing_setup/why_teach_github.html#implementation-strategy-git-lite",
    "href": "computing_setup/why_teach_github.html#implementation-strategy-git-lite",
    "title": "Should Data Science Students Learn GitHub?",
    "section": "",
    "text": "You do not need to teach the full command-line “plumbing” of Git to get 90% of the value.\nThe “Happy Path” Workflow: Focus on just three concepts using the RStudio or VS Code GUI:\n\nPull/Clone: Get the latest version.\nStage & Commit: Save your work with a message.\nPush: Sync it to the cloud.\n\nConcepts to Skip (for now):\n\nBranching/Rebasing (unless doing advanced group work).\nCommand line interface (GUI buttons are fine for beginners).\nComplex .gitignore rules (provide a default one for them)."
  },
  {
    "objectID": "computing_setup/setup_university_drive.html",
    "href": "computing_setup/setup_university_drive.html",
    "title": "Setup: Using University Drives (H: Drive)",
    "section": "",
    "text": "When working on course assignments, where you save your files is as important as what you write.\n\n\n\n\n\n\nImportantCRITICAL: Avoid OneDrive for Git Projects\n\n\n\nDo not save your RStudio Projects or Git repositories in your OneDrive, Dropbox, or Google Drive folders.\nWhy? Cloud sync services (like OneDrive) and Git fight for control over your files.\n\nGit creates thousands of tiny hidden files in .git/.\nOneDrive tries to sync them instantly.\nResult: “Corrupt Index” errors, “Fatal” Git crashes, and potential loss of your project history.\n\nThe Solution: Use your H: Drive (University Network Drive).\n\n\n\n\nYour H: Drive (\\\\myfiles.campus.bath.ac.uk\\username) is a personal network storage space provided by the specific University.\n\n✅ Persistent: Accessible from any campus PC and your personal laptop.\n✅ Backed Up: University IT backs it up regularly.\n✅ Git-Safe: It behaves like a standard hard drive, avoiding OneDrive sync conflicts.\n\n\n\n\n\nWhen you log in to a University computer, your H: Drive should be mapped automatically.\n\nOpen File Explorer (Win+E).\nClick This PC in the left sidebar.\nLook for a drive labelled (H:) under “Network Locations”.\nAction: Create a folder here named MA22019 to store all your work.\n\n\n\n\n\nTo access your H: Drive from your own laptop (at home or in the library), you must be connected to the University network.\n\n\nYou generally cannot see university drives from the public internet. You must use the VPN (Virtual Private Network).\n\nAction: Download and install the GlobalProtect VPN client from the University of Bath VPN Guide.\nVerify: Ensure the VPN shows “Connected” before proceeding.\n\n\n\n\nSelect the instructions for your Operating System:\n\nWindowsmacOSLinux (Ubuntu/GNOME)\n\n\n\nEnsure VPN is connected.\nOpen File Explorer.\nRight-click on This PC (left sidebar) &gt; Map network drive…\nDrive: Select H: from the dropdown.\nFolder: Type \\\\myfiles.campus.bath.ac.uk\\username (replace username with your username, e.g., abc123).\nCheck Connect using different credentials.\nClick Finish.\nCredentials:\n\nUser: CAMPUS\\username (e.g., CAMPUS\\abc123)\nPassword: Your university password.\n\n\n\n\n\nEnsure VPN is connected.\nOpen Finder.\nIn the menu bar, click Go &gt; Connect to Server… (or press Cmd+K).\nServer Address: Type smb://myfiles.campus.bath.ac.uk/username (replace username with your username).\nClick Connect.\nCredentials:\n\nSelect “Registered User”.\nName: username (or CAMPUS\\username if that fails)\nPassword: Your university password.\n\n\n\n\n\nEnsure VPN is connected.\nOpen File Manager (Nautilus).\nClick Other Locations in the sidebar.\nConnect to Server: Type smb://myfiles.campus.bath.ac.uk/username (replace username with your username).\nClick Connect.\nCredentials:\n\nDomain: CAMPUS\nUsername: Your username\nPassword: Your university password.\n\n\n\n\n\n\n\n\n\n\nYou may discover files.bath.ac.uk (WebDrive).\n\n❌ Do NOT use this for your RStudio Project.\nWhy? RStudio cannot “see” files inside a web browser. You cannot run code, save plots, or use Git directly from this website.\nUse Case: Only use files.bath if you forgot your laptop and need to quickly download a single file to a public or shared computer.\n\n\n\n\n\n\nYou may also see a W: Drive (\\\\waltz.campus.bath.ac.uk\\wwwgroup).\n\n⚠️ DANGER: This is your Personal Web Space.\nPublic Risk: Files you save here may be visible to the entire world on the internet.\nCoursework:\n\nNEVER save your RStudio projects or homework solutions here.\nWHY? If your solution is google-able, you could be flagged for Academic Misconduct (collusion/sharing solutions).\nOnly use this drive if you are explicitly asked to host a final HTML webpage.\n\n\n\n\n\n\nWhen you follow “Step 2: Clone to Your Computer” in the Assignment Workflow:\n\nIn RStudio &gt; New Project &gt; Version Control &gt; Git…\nCreate project as subdirectory of: Click Browse…\nNavigate to your H: Drive.\n\nWindows: Select H:\\MA22019.\nMac: Select the mounted server drive from the sidebar.\n\nClick Create Project.\n\nYour code is now safe, backed up, and accessible from anywhere!",
    "crumbs": [
      "Computing",
      "University Drive Setup"
    ]
  },
  {
    "objectID": "computing_setup/setup_university_drive.html#the-h-drive-advantage",
    "href": "computing_setup/setup_university_drive.html#the-h-drive-advantage",
    "title": "Setup: Using University Drives (H: Drive)",
    "section": "",
    "text": "Your H: Drive (\\\\myfiles.campus.bath.ac.uk\\username) is a personal network storage space provided by the specific University.\n\n✅ Persistent: Accessible from any campus PC and your personal laptop.\n✅ Backed Up: University IT backs it up regularly.\n✅ Git-Safe: It behaves like a standard hard drive, avoiding OneDrive sync conflicts.",
    "crumbs": [
      "Computing",
      "University Drive Setup"
    ]
  },
  {
    "objectID": "computing_setup/setup_university_drive.html#on-campus-windows-lab-pcs",
    "href": "computing_setup/setup_university_drive.html#on-campus-windows-lab-pcs",
    "title": "Setup: Using University Drives (H: Drive)",
    "section": "",
    "text": "When you log in to a University computer, your H: Drive should be mapped automatically.\n\nOpen File Explorer (Win+E).\nClick This PC in the left sidebar.\nLook for a drive labelled (H:) under “Network Locations”.\nAction: Create a folder here named MA22019 to store all your work.",
    "crumbs": [
      "Computing",
      "University Drive Setup"
    ]
  },
  {
    "objectID": "computing_setup/setup_university_drive.html#remote-access-personal-laptop",
    "href": "computing_setup/setup_university_drive.html#remote-access-personal-laptop",
    "title": "Setup: Using University Drives (H: Drive)",
    "section": "",
    "text": "To access your H: Drive from your own laptop (at home or in the library), you must be connected to the University network.\n\n\nYou generally cannot see university drives from the public internet. You must use the VPN (Virtual Private Network).\n\nAction: Download and install the GlobalProtect VPN client from the University of Bath VPN Guide.\nVerify: Ensure the VPN shows “Connected” before proceeding.\n\n\n\n\nSelect the instructions for your Operating System:\n\nWindowsmacOSLinux (Ubuntu/GNOME)\n\n\n\nEnsure VPN is connected.\nOpen File Explorer.\nRight-click on This PC (left sidebar) &gt; Map network drive…\nDrive: Select H: from the dropdown.\nFolder: Type \\\\myfiles.campus.bath.ac.uk\\username (replace username with your username, e.g., abc123).\nCheck Connect using different credentials.\nClick Finish.\nCredentials:\n\nUser: CAMPUS\\username (e.g., CAMPUS\\abc123)\nPassword: Your university password.\n\n\n\n\n\nEnsure VPN is connected.\nOpen Finder.\nIn the menu bar, click Go &gt; Connect to Server… (or press Cmd+K).\nServer Address: Type smb://myfiles.campus.bath.ac.uk/username (replace username with your username).\nClick Connect.\nCredentials:\n\nSelect “Registered User”.\nName: username (or CAMPUS\\username if that fails)\nPassword: Your university password.\n\n\n\n\n\nEnsure VPN is connected.\nOpen File Manager (Nautilus).\nClick Other Locations in the sidebar.\nConnect to Server: Type smb://myfiles.campus.bath.ac.uk/username (replace username with your username).\nClick Connect.\nCredentials:\n\nDomain: CAMPUS\nUsername: Your username\nPassword: Your university password.",
    "crumbs": [
      "Computing",
      "University Drive Setup"
    ]
  },
  {
    "objectID": "computing_setup/setup_university_drive.html#web-access-files.bath---emergency-only",
    "href": "computing_setup/setup_university_drive.html#web-access-files.bath---emergency-only",
    "title": "Setup: Using University Drives (H: Drive)",
    "section": "",
    "text": "You may discover files.bath.ac.uk (WebDrive).\n\n❌ Do NOT use this for your RStudio Project.\nWhy? RStudio cannot “see” files inside a web browser. You cannot run code, save plots, or use Git directly from this website.\nUse Case: Only use files.bath if you forgot your laptop and need to quickly download a single file to a public or shared computer.",
    "crumbs": [
      "Computing",
      "University Drive Setup"
    ]
  },
  {
    "objectID": "computing_setup/setup_university_drive.html#what-about-the-w-drive-webdrive",
    "href": "computing_setup/setup_university_drive.html#what-about-the-w-drive-webdrive",
    "title": "Setup: Using University Drives (H: Drive)",
    "section": "",
    "text": "You may also see a W: Drive (\\\\waltz.campus.bath.ac.uk\\wwwgroup).\n\n⚠️ DANGER: This is your Personal Web Space.\nPublic Risk: Files you save here may be visible to the entire world on the internet.\nCoursework:\n\nNEVER save your RStudio projects or homework solutions here.\nWHY? If your solution is google-able, you could be flagged for Academic Misconduct (collusion/sharing solutions).\nOnly use this drive if you are explicitly asked to host a final HTML webpage.",
    "crumbs": [
      "Computing",
      "University Drive Setup"
    ]
  },
  {
    "objectID": "computing_setup/setup_university_drive.html#cloning-into-h-drive",
    "href": "computing_setup/setup_university_drive.html#cloning-into-h-drive",
    "title": "Setup: Using University Drives (H: Drive)",
    "section": "",
    "text": "When you follow “Step 2: Clone to Your Computer” in the Assignment Workflow:\n\nIn RStudio &gt; New Project &gt; Version Control &gt; Git…\nCreate project as subdirectory of: Click Browse…\nNavigate to your H: Drive.\n\nWindows: Select H:\\MA22019.\nMac: Select the mounted server drive from the sidebar.\n\nClick Create Project.\n\nYour code is now safe, backed up, and accessible from anywhere!",
    "crumbs": [
      "Computing",
      "University Drive Setup"
    ]
  },
  {
    "objectID": "slides/week_7/analysis_of_german_temperature_data.html",
    "href": "slides/week_7/analysis_of_german_temperature_data.html",
    "title": "Analysis of German Temperature Data",
    "section": "",
    "text": "library( dplyr )\nlibrary( ggplot2 )\nlibrary( sf )\nlibrary( ggspatial )\nlibrary( prettymapr )"
  },
  {
    "objectID": "slides/week_7/analysis_of_german_temperature_data.html#loading-the-data-and-a-first-plot",
    "href": "slides/week_7/analysis_of_german_temperature_data.html#loading-the-data-and-a-first-plot",
    "title": "Analysis of German Temperature Data",
    "section": "Loading the data and a first plot",
    "text": "Loading the data and a first plot\nThe data are stored in the file “Temperature Germany.csv”\n\nTemperature &lt;- read.csv(\"temperature_germany.csv\" )  \n\nThe data provides spatial locations and maximum daily temperature:\n\nglimpse( Temperature )\n\nRows: 75\nColumns: 5\n$ name      &lt;chr&gt; \"Leuchtturm Alte Weser\", \"Dresden-Klotzsche\", \"D\\xfcsseldorf…\n$ latitude  &lt;dbl&gt; 53.86330, 51.11991, 51.28658, 54.19130, 48.35505, 50.97729, …\n$ longitude &lt;dbl&gt; 8.127500, 13.763200, 6.768267, 6.560675, 11.808000, 10.97638…\n$ altitude  &lt;dbl&gt; 32.0, 206.7, 36.4, 0.8, 443.9, 288.9, 1489.7, 1213.0, 111.5,…\n$ max.temp  &lt;dbl&gt; 18.9, 23.8, 28.7, 16.6, 25.6, 23.9, 17.1, 17.0, 26.3, 23.4, …"
  },
  {
    "objectID": "slides/week_7/analysis_of_german_temperature_data.html#visualization-using-ggplot2-only",
    "href": "slides/week_7/analysis_of_german_temperature_data.html#visualization-using-ggplot2-only",
    "title": "Analysis of German Temperature Data",
    "section": "Visualization using ggplot2 only",
    "text": "Visualization using ggplot2 only\nUse colour as a visual cue for the temperature values and longitude and latitude for the x and y axis respectively:\n\nggplot( Temperature, aes(x=longitude, y=latitude) ) +\n  geom_point( aes(color=max.temp), size=1.2 ) + theme_bw() + \n  scale_color_distiller( palette=\"Reds\", trans=\"reverse\" ) +\n  labs( x=\"Longitude\", y=\"Latitude\", color=\"Temperature\")"
  },
  {
    "objectID": "slides/week_7/analysis_of_german_temperature_data.html#importing-and-plotting-shapefiles",
    "href": "slides/week_7/analysis_of_german_temperature_data.html#importing-and-plotting-shapefiles",
    "title": "Analysis of German Temperature Data",
    "section": "Importing and plotting shapefiles",
    "text": "Importing and plotting shapefiles\nWe load the shapefile with the read_sf() function\n\nGermany &lt;- read_sf( \"gadm41_DEU_1.shp\" )\n\nLet’s slightly reduce the complexity of the shape\n\nGermany &lt;- Germany %&gt;% st_simplify( dTolerance = 2000 )\n\nWe can plot the shape with\n\nggplot( Germany ) + geom_sf() + theme_bw() + \n  labs( x=\"Longitude\", y=\"Latitude\" )\n\n\n\n\n\n\n\nTime to combine the shapefile with the points\n\nggplot( Germany ) + geom_sf() + \n  geom_point( data=Temperature, aes(x=longitude, y=latitude,\n                                    color=max.temp), size=2 ) + \n  scale_color_distiller( palette=\"Reds\", trans=\"reverse\" ) +\n  theme_bw() + labs( x=\"Longitude\", y=\"Latitude\", color=\"Temperature\" )"
  },
  {
    "objectID": "slides/week_7/analysis_of_german_temperature_data.html#projections",
    "href": "slides/week_7/analysis_of_german_temperature_data.html#projections",
    "title": "Analysis of German Temperature Data",
    "section": "Projections",
    "text": "Projections\nLet’s apply a projection to the shapefile\n\nggplot( Germany ) + theme_bw() + geom_sf() + \n  coord_sf( crs=st_crs(3346) )\n\n\n\n\n\n\n\nWe can also store the projected shapefile\n\nGermany_projected &lt;- st_transform( Germany, crs = 3346 )\n\nWe also need to change the projection of the points if we wanted to plot on the projected surface. So we first convert them into a spatial object:\n\nTemperature_projected &lt;- st_as_sf( Temperature, coords=c(\"longitude\", \"latitude\"), crs=\"WGS84\" )\n\nNow we are ready to plot the points\n\nggplot( Germany_projected ) + theme_bw() + geom_sf() +  \n  geom_sf( data=Temperature_projected, aes(color=max.temp), size=2 ) + \n  scale_color_distiller( palette=\"Reds\", trans=\"reverse\" ) +\n  theme_bw() + labs( x=\"Longitude\", y=\"Latitude\", color=\"Temperature\" )\n\n\n\n\n\n\n\nYou will see another example in Tutorial Question 2 on Problem Sheet 6."
  },
  {
    "objectID": "slides/week_7/analysis_of_german_temperature_data.html#adding-maps",
    "href": "slides/week_7/analysis_of_german_temperature_data.html#adding-maps",
    "title": "Analysis of German Temperature Data",
    "section": "Adding maps",
    "text": "Adding maps\nAn alternative to shapefiles is to use maps. We will use ggspatial for this:\n\nggplot( Temperature, aes(x=longitude, y=latitude) ) +\n  annotation_map_tile() +\n  geom_spatial_point( aes(color=max.temp), size=2 ) + \n  scale_color_distiller( palette=\"Reds\", trans=\"reverse\" ) +\n  theme_bw() + labs( x=\"Longitude\", y=\"Latitude\", color=\"Temperature\" )"
  },
  {
    "objectID": "slides/week_7/analysis_of_german_temperature_data.html#function-to-perform-inverse-distance-weighting",
    "href": "slides/week_7/analysis_of_german_temperature_data.html#function-to-perform-inverse-distance-weighting",
    "title": "Analysis of German Temperature Data",
    "section": "Function to perform inverse distance weighting",
    "text": "Function to perform inverse distance weighting\n\nIDW &lt;- function( X, S, s_star, p){\n  d &lt;- sqrt( (S[,1]-s_star[1])^2 + (S[,2]-s_star[2])^2 )\n  w &lt;- d^(-p)\n  if( min(d) &gt; 0 )\n    return( sum( X * w ) / sum( w ) )\n  else \n    return( X[d==0] )\n}"
  },
  {
    "objectID": "slides/week_7/analysis_of_german_temperature_data.html#combine-observations-to-define-matrix-for-s",
    "href": "slides/week_7/analysis_of_german_temperature_data.html#combine-observations-to-define-matrix-for-s",
    "title": "Analysis of German Temperature Data",
    "section": "Combine observations to define matrix for S",
    "text": "Combine observations to define matrix for S\n\ncoord &lt;- cbind( Temperature$longitude, Temperature$latitude )\nIDW( X=Temperature$max.temp, S=coord, s_star= c( 13.1, 51.0 ), p=5 )\n\n[1] 19.9847"
  },
  {
    "objectID": "slides/week_7/analysis_of_german_temperature_data.html#estimate-values-over-a-grid",
    "href": "slides/week_7/analysis_of_german_temperature_data.html#estimate-values-over-a-grid",
    "title": "Analysis of German Temperature Data",
    "section": "Estimate values over a grid",
    "text": "Estimate values over a grid\nDefine the grid\n\npoints_lat &lt;- seq( 47.1, 55.1, by=0.05 )\npoints_lon &lt;- seq( 5.8, 15.6, by=0.05 )\npixels &lt;- as.matrix( expand.grid( points_lon, points_lat ) )\n\nCalculate values for difference choices of the power parameter\n\np05 &lt;- p2 &lt;- p20 &lt;- c()\nfor( j in 1:length(pixels[,1]) ){\n  p05[j] &lt;- IDW( X=Temperature$max.temp, S=coord, s_star=pixels[j,], p=0.5 )\n  p2[j]  &lt;- IDW( X=Temperature$max.temp, S=coord, s_star=pixels[j,], p=2.0 )\n  p20[j] &lt;- IDW( X=Temperature$max.temp, S=coord, s_star=pixels[j,], p=20  )\n}"
  },
  {
    "objectID": "slides/week_7/analysis_of_german_temperature_data.html#visualize-grid-of-predicted-values",
    "href": "slides/week_7/analysis_of_german_temperature_data.html#visualize-grid-of-predicted-values",
    "title": "Analysis of German Temperature Data",
    "section": "Visualize grid of predicted values",
    "text": "Visualize grid of predicted values\nSet up data frame\n\nlibrary(tidyr)\nPredict &lt;- data.frame( \"Lon\"=pixels[,1], \"Lat\"=pixels[,2], \n                       \"p0.5\"=p05, \"p2\"=p2, \"p20\"=p20 )\nPredict &lt;- Predict %&gt;% pivot_longer( cols=p0.5:p20, names_to = \"Power\" )\nPredict &lt;- Predict %&gt;% \n  mutate( Power = case_when( Power == \"p0.5\" ~ \"p=0.5\", Power == \"p2\" ~ \"p=2.0\",\n                             .default = \"p=20\" ) )\n\nVisualize predictions together with a map\n\nggplot( data=Predict ) + theme_bw() +\n  geom_raster( aes( x=Lon, y=Lat, fill=value ) ) + \n  facet_wrap( ~Power ) + \n  scale_fill_distiller( palette=\"Reds\", trans=\"reverse\" ) +\n  geom_sf( data=Germany, alpha=0.0, color=\"white\" ) +\n  geom_point( data=Temperature, aes(x=longitude, y=latitude) ) + \n  labs( fill=\"°C\", x=\"Longitude\", y=\"Latitude\" )"
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "",
    "text": "We finish the remaining content in the lecture notes:\n\nComments on semi-variogram and PCA\nVisualization of point pattern data (Sections 4.5)\nQuadrat counting and kernel smoothed intensity function (Section 4.6)\nVisualizing lattice/areal data (Section 4.7)\n\nThere will be a revision class in Week 10 (after the Easter break). Look out for a message about a poll on Moodle."
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#plan-for-today",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#plan-for-today",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "",
    "text": "We finish the remaining content in the lecture notes:\n\nComments on semi-variogram and PCA\nVisualization of point pattern data (Sections 4.5)\nQuadrat counting and kernel smoothed intensity function (Section 4.6)\nVisualizing lattice/areal data (Section 4.7)\n\nThere will be a revision class in Week 10 (after the Easter break). Look out for a message about a poll on Moodle."
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#semi-variogram---width-and-cutoff",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#semi-variogram---width-and-cutoff",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "Semi-variogram - width and cutoff",
    "text": "Semi-variogram - width and cutoff\nThe width in the variogram() function determines the number of points used to estimate \\(\\hat\\gamma(h)\\):\n\nA too small value means that estimates are highly uncertain\nA too large value means that important aspects may be missed\n\nThe cutoff determines which range of distances we consider:\n\nA too small value may imply that we miss important features\nA too large value increases the risk of spurious relations affecting our conclusions"
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#what-to-look-out-for",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#what-to-look-out-for",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "What to look out for?",
    "text": "What to look out for?\nIt may be useful to consider the number of points per estimate:\n\ngamma_hat$np\n\nIf the number of points is too small, it may be worth changing the width or cutoff.\nCertain patterns in the semi-variogram may, for instance, be due to the constant mean assumption not being satisfied.\nHowever, such features may also be caused by randomness while all assumptions hold."
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#pca---when-does-it-work-well",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#pca---when-does-it-work-well",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "PCA - when does it work well?",
    "text": "PCA - when does it work well?\nIdeally we want the data to lie in a low-dimensional linear subspace:\n\nOnly a few eigenvectors need to be considered for exploration\nGood potential for dimension reduction\n\nWe have not really covered the topic in enough depth to introduce methods for exploring whether the data lies in a linear subspace in our examples.\nInstead, we only explore whether a linear relation between pairs of variables is realistic - we should pick spatially close sites."
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#illustration",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#illustration",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "Illustration",
    "text": "Illustration\n\n\n\n\n\n\n\n\nWe find quite a strong correlation and thus there is potential."
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#visualization",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#visualization",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "Visualization",
    "text": "Visualization\nWe can use the same techniques as for point-referenced data:\n\nPlacing points onto a shapefile\nCreating maps with points on top\n\nLet’s look at an example-&gt; R Markdown file WildFires.Rmd\nBut: We may not always see the structure clearly-&gt; R Markdown file Tornadoes.Rmd\nThis leads us to the concept of intensity of a point process."
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#intensity-of-a-point-process",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#intensity-of-a-point-process",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "Intensity of a point process",
    "text": "Intensity of a point process\nRemember that the locations and their number are random when working with point pattern data.\nLet \\(N(\\mathcal{B})\\) be the random variable representing the number of points located in \\(\\mathcal{B}\\subseteq\\mathcal{S}\\).\nThe intensity then describes \\(\\mathbb{E}\\left[N(\\mathcal{B})\\right]\\) for any \\(\\mathcal{B}\\subseteq\\mathcal{S}\\) with [ ({}) = = _{} () . ] We refer to \\(\\lambda:\\mathcal{S}\\to\\mathbb{R}_+=\\{x\\in\\mathbb{R}:x\\geq0\\}\\) as the intensity function."
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#analysis-of-the-intensity-function",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#analysis-of-the-intensity-function",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "Analysis of the intensity function",
    "text": "Analysis of the intensity function\nWe say that that the point process is\n\nhomogeneous when \\(\\lambda(\\cdot)\\) is constant\nnon-homogeneous when \\(\\lambda(\\cdot)\\) is not constant\n\nIn what follows, we descrive two techniques for visualizing the intensity function:\n\nQuadrat counting\nKernel smoothed intensity function"
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#overview",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#overview",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "Overview",
    "text": "Overview\nGiven observed locations \\(\\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\), we perform three steps:\n\nSplit \\(\\mathcal{S}\\) into disjoint areas \\(\\mathcal{B}_1,\\ldots,\\mathcal{B}_m\\) which are set as rectangles (quadrats).\nCount the number of points in \\(\\mathcal{B}_j\\) as an estimate for \\(\\mathbb{E}\\left[N(\\mathcal{B}_j)\\right]\\), [ = _{i=1}^n {_i _j}. ]\nThe intensity function is then estimated as [ ^{(Q)}() = ,_j, ] where \\(|\\mathcal{B}_j|\\) denotes the area of \\(\\mathcal{B}_j\\)."
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#details",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#details",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "Details",
    "text": "Details\nThe quadratcount() function in the spatstat R package performs steps 1) and 2).\nWe have to balance two aspects:\n\n\\(\\mathcal{B}_j\\) should be small enough such that \\(\\lambda(\\mathbf{s})\\) is approximately constant for all \\(\\mathbf{s}\\in\\mathcal{B}_j\\).\n\\(\\widehat{\\mu(\\mathcal{B}_j)}\\) should be a reliable estimate for \\(\\mu(\\mathcal{B}_j)\\).\n\n-&gt; R Markdown file WildFires.Rmd"
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#overview-1",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#overview-1",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "Overview",
    "text": "Overview\nSuppose we have a probability density (kernel) \\(K(\\cdot)\\) and, similar to a density plot, we define [ ^{(K)}() = _{i=1}^{n} K(||-_i||_2), ] where \\(||\\mathbf{s}-\\mathbf{s}_i||_2\\) is the Euclidian distance.\nHowever, this approach assigns positive probability to areas outside \\(\\mathcal{S}\\) and thus [ _{} ^{(K)}() n. ]"
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#edge-correction",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#edge-correction",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "Edge correction",
    "text": "Edge correction\nThe uniformly corrected smoothed kernel intensity function is defined as [ ^{(C)}() = _{i=1}^{n} K(||-_i||2), ] where [ g() = {} K(-) . ]\nThis correction is available in the spatstat R package.-&gt; R Markdown file WildFires.Rmd"
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#analysis-of-tornadoes",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#analysis-of-tornadoes",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "Analysis of Tornadoes",
    "text": "Analysis of Tornadoes\nLet’s consider a second example-&gt; R Markdown file Tornadoes.Rmd"
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#overview-2",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#overview-2",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "Overview",
    "text": "Overview\nWe will consider two examples:\n\nPopulation density across London boroughs\nNumber of tornadoes per US state\n\nIn both of these cases, we will exploit an aspect of shapefiles we have not considered so far.\nSpecifically, we make use of the shapefile coming with a data frame, which permits data wrangling."
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#example-1---boroughs-in-london",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#example-1---boroughs-in-london",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "Example 1 - Boroughs in London",
    "text": "Example 1 - Boroughs in London\nLet’s load a shapefile for London\n\nLondon &lt;- read_sf(\"London.shp\")\nclass( London )\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWe see that London includes a data frame.\nAll entries except for the “geometry” can be manipulated.\nLet’s visualize population density for the different boroughs.-&gt; R Markdown file"
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#example-2---tornadoes-in-the-us",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#example-2---tornadoes-in-the-us",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "Example 2 - Tornadoes in the US",
    "text": "Example 2 - Tornadoes in the US\nThe data in “Tornadoes.csv” provides observations on tornadoes for 1950-1921 for the whole US.\nLet’s visualize the number of tornadoes recorded for each state. -&gt;R Markdown file Tornadoes.Rmd"
  },
  {
    "objectID": "slides/week_9/spatial_data_analyis_(part_3).html#summary",
    "href": "slides/week_9/spatial_data_analyis_(part_3).html#summary",
    "title": "MA22019 - Spatial Data Analysis (Part 3)",
    "section": "Summary",
    "text": "Summary\n\n\nWe covered all the relevant material:\n\nVisualization of point pattern and lattice data\nExploration of the intensity of a point process\n\n\nProblem Sheet 8 provides some practice and will be considered in the tutorial in Week 10.\n\nRevision lecture in Week 10:\n\nWe will revisit two topics of your choice\nSelection via a feedback form on Moodle (the two most popular choices win)"
  },
  {
    "objectID": "slides/week_9/London.html",
    "href": "slides/week_9/London.html",
    "title": "Visualizing population density across London boroughs",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\n\nLondon &lt;- read_sf(\"London.shp\")\n\n\nnames(London)\n\n[1] \"NAME\"       \"GSS_CODE\"   \"HECTARES\"   \"NONLD_AREA\" \"ONS_INNER\" \n[6] \"SUB_2009\"   \"SUB_2006\"   \"geometry\""
  },
  {
    "objectID": "slides/week_9/London.html#load-the-data-and-shapefile",
    "href": "slides/week_9/London.html#load-the-data-and-shapefile",
    "title": "Visualizing population density across London boroughs",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\n\nLondon &lt;- read_sf(\"London.shp\")\n\n\nnames(London)\n\n[1] \"NAME\"       \"GSS_CODE\"   \"HECTARES\"   \"NONLD_AREA\" \"ONS_INNER\" \n[6] \"SUB_2009\"   \"SUB_2006\"   \"geometry\""
  },
  {
    "objectID": "slides/week_9/London.html#explroe-the-data-structure",
    "href": "slides/week_9/London.html#explroe-the-data-structure",
    "title": "Visualizing population density across London boroughs",
    "section": "Explroe the data structure",
    "text": "Explroe the data structure\nThe following illustrates that the shapefile comes with a data frame\n\nclass( London )\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\""
  },
  {
    "objectID": "slides/week_9/London.html#load-and-clean-the-population-data",
    "href": "slides/week_9/London.html#load-and-clean-the-population-data",
    "title": "Visualizing population density across London boroughs",
    "section": "Load and clean the population data",
    "text": "Load and clean the population data\n\nLondon_population &lt;- read.csv(\"london.csv\" )\nLondon_population &lt;- London_population %&gt;% \n  mutate( Population = as.numeric( gsub(\"\\\\,\",\"\",Population) ) )"
  },
  {
    "objectID": "slides/week_9/London.html#merge-data-sets-and-calculate-density",
    "href": "slides/week_9/London.html#merge-data-sets-and-calculate-density",
    "title": "Visualizing population density across London boroughs",
    "section": "Merge data sets and calculate density",
    "text": "Merge data sets and calculate density\n\nLondon &lt;- inner_join( x=London, y=London_population, by=c(\"NAME\"=\"Borough\") )\nLondon &lt;- London %&gt;% mutate( Density = Population / HECTARES )"
  },
  {
    "objectID": "slides/week_9/London.html#visualzation-1-colour-as-visual-cue",
    "href": "slides/week_9/London.html#visualzation-1-colour-as-visual-cue",
    "title": "Visualizing population density across London boroughs",
    "section": "Visualzation 1: Colour as visual cue",
    "text": "Visualzation 1: Colour as visual cue\n\nggplot( data=London, aes(fill=Density) ) + geom_sf() + theme_bw() +\n  scale_fill_distiller( palette=\"Reds\", trans=\"reverse\" ) + \n  theme( axis.title=element_text(size=15), axis.text=element_text(size=15) ) + \n  labs( x=\"Longitude\", y=\"Latitude\" )"
  },
  {
    "objectID": "slides/week_9/London.html#visualzation-2-size-of-points-as-visual-cue",
    "href": "slides/week_9/London.html#visualzation-2-size-of-points-as-visual-cue",
    "title": "Visualizing population density across London boroughs",
    "section": "Visualzation 2: Size of points as visual cue",
    "text": "Visualzation 2: Size of points as visual cue\n\nggplot( data=London ) + geom_sf() + theme_bw() + \n  geom_point( data=st_centroid(London), aes( size = Density, geometry = geometry), \n              stat = \"sf_coordinates\" ) +\n  labs( x=\"Longitude\", y=\"Latitude\" ) + \n  theme( axis.title=element_text(size=15), axis.text=element_text(size=15) )\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data"
  },
  {
    "objectID": "slides/week_8/analysis_of_sea_surface_temperature_anomalies.html",
    "href": "slides/week_8/analysis_of_sea_surface_temperature_anomalies.html",
    "title": "Analysis of Sea Surface Temperature Anomalies",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(sf)\nlibrary(sp)\nlibrary(gstat)\n\nSSTA &lt;- read.csv(\"sea_surface_temperature_anomalies.csv\" )"
  },
  {
    "objectID": "slides/week_8/analysis_of_sea_surface_temperature_anomalies.html#load-the-data-and-r-packages",
    "href": "slides/week_8/analysis_of_sea_surface_temperature_anomalies.html#load-the-data-and-r-packages",
    "title": "Analysis of Sea Surface Temperature Anomalies",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(sf)\nlibrary(sp)\nlibrary(gstat)\n\nSSTA &lt;- read.csv(\"sea_surface_temperature_anomalies.csv\" )"
  },
  {
    "objectID": "slides/week_8/analysis_of_sea_surface_temperature_anomalies.html#visualize-data-similar-to-problem-class-5",
    "href": "slides/week_8/analysis_of_sea_surface_temperature_anomalies.html#visualize-data-similar-to-problem-class-5",
    "title": "Analysis of Sea Surface Temperature Anomalies",
    "section": "Visualize Data (similar to Problem Class 5)",
    "text": "Visualize Data (similar to Problem Class 5)\nConvert longitude and latitude to spatial coordinates\n\nSSTA_sf &lt;- st_as_sf( SSTA, coords=c(\"lon\", \"lat\") ) %&gt;% st_set_crs( 4326 )\n\nWe can now visulaize the data\n\nggplot( data=SSTA_sf ) + theme_bw() + geom_sf( aes(color=Anomaly) ) + \n  labs( x=\"Longitude\", y=\"Latitude\", color=\"SST Anomaly\" )"
  },
  {
    "objectID": "slides/week_8/analysis_of_sea_surface_temperature_anomalies.html#estimation-of-the-variogram",
    "href": "slides/week_8/analysis_of_sea_surface_temperature_anomalies.html#estimation-of-the-variogram",
    "title": "Analysis of Sea Surface Temperature Anomalies",
    "section": "Estimation of the variogram",
    "text": "Estimation of the variogram\nWe first drop any missing values\n\nSSTA_gamma &lt;- drop_na( SSTA, Anomaly )\n\nLongitude and latitude are converted to spatial data points (we need the sp package for this):\n\ncoordinates( SSTA_gamma ) &lt;- ~lon+lat\n\nUse the variogram() function to estimate the semi-variogram and visualize it:\n\ngamma_hat &lt;- variogram( Anomaly~1, SSTA_gamma, width=0.1, cutoff=2  )\nggplot( gamma_hat, aes( x=dist, y=gamma/2 ) ) + geom_point( size=2 ) + \n  theme_bw() + labs( x=\"Distance\", y=\"Semi-variogram\" )"
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html",
    "href": "slides/week_1/data_cleaning_and_wrangling.html",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "",
    "text": "What do you think is part of data cleaning?\n\n\nTwo key reasons are:\n\nData are not of the correct type and thus incompatible with available R functions\nUninformative variable names\n\nWe now explore how the dplyr and lubridate R packages can help us.\n\nLet’s have a look at the data in “Bathford River Flow.csv”.\n\nWhat do we notice?\n\n\nThe provided variables represent the date and mean river flow\nData is not in a nice format\nNo variable names\n\nWe use the read.csv() function to load the data:\n\nBathford &lt;- read.csv(\"bathford_river_flow.csv\", skip=20, header=FALSE,\n                      colClasses = c(\"character\",\"numeric\",\"NULL\") )\n\nWe use glimpse() to check the variable names and data types:\n\nlibrary( dplyr )\nglimpse( Bathford )\n\nRows: 19,697\nColumns: 2\n$ V1 &lt;chr&gt; \"1969-10-27\", \"1969-10-28\", \"1969-10-29\", \"1969-10-30\", \"1969-10-31…\n$ V2 &lt;dbl&gt; 3.998, 3.958, 4.210, 4.480, 4.205, 3.830, 3.723, 3.986, 4.353, 4.52…\n\n\n\nThe loaded data may not have the correct type:\n\nNumerical data were read as a character:\n\n\nExample1 &lt;- read.csv(\"datacleaningexample1.csv\" )\nglimpse( Example1 )\n\nRows: 8\nColumns: 1\n$ Value &lt;chr&gt; \"1.02\", \"0.98\", \"0.79\", \"M\", \"2.1\", \"15.1\", \"M\", \"4.2\"\n\nmean( Example1 )\n\n[1] NA\n\n\n\nWe use the function as.numeric() to convert the type:\n\nExample1$Value &lt;- as.numeric( Example1$Value )\nglimpse( Example1 )\n\nRows: 8\nColumns: 1\n$ Value &lt;dbl&gt; 1.02, 0.98, 0.79, NA, 2.10, 15.10, NA, 4.20\n\nmean( Example1$Value, na.rm=TRUE )\n\n[1] 4.031667\n\n\n\n\nBinary answers were recorded as “yes” or “no”:\n\n\nresponses &lt;- c( \"Y\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"Y\" )\nresponses &lt;- case_when( responses == \"Y\" ~ 1, responses == \"N\" ~ 0 )\nresponses\n\n [1] 1 1 0 1 0 1 0 0 0 1\n\n\n\nmean( responses )\n\n[1] 0.5\n\n\n\n\nConvert from character to date\n\n\n\nlibrary(lubridate)\ndate_observed  &lt;- c( \"01/10/2022\", \"15/10/2023\" )\ndate_converted &lt;- as_date( date_observed, format=\"%d/%m/%Y\" )\ndate_converted\n\n[1] \"2022-10-01\" \"2023-10-15\"\n\n\n\n\nExtraction of month and year from a Date object using month() and year() respectively:\n\n\nyear( date_converted )\n\n[1] 2022 2023\n\n\n\nCalculate differences between dates\n\n\nas_date( \"20/04/2025\", format=\"%d/%m/%Y\" ) - as_date( \"2025-02-05\" )\n\nTime difference of 74 days\n\n\n\nCreate plot with time being one of the axis.\n\n\nWe see that the first variable should be of type date\n\n\n\nBathford$V1 &lt;- as_date( Bathford$V1, format=\"%Y-%m-%d\" )\nglimpse( Bathford )\n\nRows: 19,697\nColumns: 2\n$ V1 &lt;date&gt; 1969-10-27, 1969-10-28, 1969-10-29, 1969-10-30, 1969-10-31, 1969-1…\n$ V2 &lt;dbl&gt; 3.998, 3.958, 4.210, 4.480, 4.205, 3.830, 3.723, 3.986, 4.353, 4.52…\n\n\n\nUninformative variable names can cause problems!\n\n\n\n\n\n\n\n\n\nThe rename() function in the dplyr package can be used to change variable names.\n\nBathford &lt;- rename( Bathford, Date=V1, RiverFlow=V2 )\nglimpse( Bathford )\n\nRows: 19,697\nColumns: 2\n$ Date      &lt;date&gt; 1969-10-27, 1969-10-28, 1969-10-29, 1969-10-30, 1969-10-31,…\n$ RiverFlow &lt;dbl&gt; 3.998, 3.958, 4.210, 4.480, 4.205, 3.830, 3.723, 3.986, 4.35…\n\n\n\n\nplot( Bathford$Date, Bathford$RiverFlow, type='l', \n      xlab=\"Date\", ylab=\"River Flow\" )\n\n\n\n\n\n\n\n\nWhich conclusions should we report when asked to comment on the frequency of river flow levels above 100m\\(^3\\)/s and the magnitude of river flow levels?\n\nRecorded river flow levels were as high as approximately 250m\\(^3\\)/s.\nThe data exhibits seasonality, with river flow levels being higher in winter than in summer.\nThe data covers the years 1969 to 2023.\nThere is at least one day with river flow levels exceeding 100m\\(^3\\)/s for most years."
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#converting-data-into-a-useable-format",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#converting-data-into-a-useable-format",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "",
    "text": "What do you think is part of data cleaning?"
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#why-do-we-perform-data-cleaning",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#why-do-we-perform-data-cleaning",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "",
    "text": "Two key reasons are:\n\nData are not of the correct type and thus incompatible with available R functions\nUninformative variable names\n\nWe now explore how the dplyr and lubridate R packages can help us."
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#example---river-flow-data",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#example---river-flow-data",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "",
    "text": "Let’s have a look at the data in “Bathford River Flow.csv”.\n\nWhat do we notice?\n\n\nThe provided variables represent the date and mean river flow\nData is not in a nice format\nNo variable names"
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#river-flow-data---loading-the-data",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#river-flow-data---loading-the-data",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "",
    "text": "We use the read.csv() function to load the data:\n\nBathford &lt;- read.csv(\"bathford_river_flow.csv\", skip=20, header=FALSE,\n                      colClasses = c(\"character\",\"numeric\",\"NULL\") )\n\nWe use glimpse() to check the variable names and data types:\n\nlibrary( dplyr )\nglimpse( Bathford )\n\nRows: 19,697\nColumns: 2\n$ V1 &lt;chr&gt; \"1969-10-27\", \"1969-10-28\", \"1969-10-29\", \"1969-10-30\", \"1969-10-31…\n$ V2 &lt;dbl&gt; 3.998, 3.958, 4.210, 4.480, 4.205, 3.830, 3.723, 3.986, 4.353, 4.52…"
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#converting-variables-i",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#converting-variables-i",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "",
    "text": "The loaded data may not have the correct type:\n\nNumerical data were read as a character:\n\n\nExample1 &lt;- read.csv(\"datacleaningexample1.csv\" )\nglimpse( Example1 )\n\nRows: 8\nColumns: 1\n$ Value &lt;chr&gt; \"1.02\", \"0.98\", \"0.79\", \"M\", \"2.1\", \"15.1\", \"M\", \"4.2\"\n\nmean( Example1 )\n\n[1] NA"
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#converting-variables-ii",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#converting-variables-ii",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "",
    "text": "We use the function as.numeric() to convert the type:\n\nExample1$Value &lt;- as.numeric( Example1$Value )\nglimpse( Example1 )\n\nRows: 8\nColumns: 1\n$ Value &lt;dbl&gt; 1.02, 0.98, 0.79, NA, 2.10, 15.10, NA, 4.20\n\nmean( Example1$Value, na.rm=TRUE )\n\n[1] 4.031667"
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#converting-variables-iii",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#converting-variables-iii",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "",
    "text": "Binary answers were recorded as “yes” or “no”:\n\n\nresponses &lt;- c( \"Y\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"Y\" )\nresponses &lt;- case_when( responses == \"Y\" ~ 1, responses == \"N\" ~ 0 )\nresponses\n\n [1] 1 1 0 1 0 1 0 0 0 1\n\n\n\nmean( responses )\n\n[1] 0.5"
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#converting-variables-iv",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#converting-variables-iv",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "",
    "text": "Convert from character to date\n\n\n\nlibrary(lubridate)\ndate_observed  &lt;- c( \"01/10/2022\", \"15/10/2023\" )\ndate_converted &lt;- as_date( date_observed, format=\"%d/%m/%Y\" )\ndate_converted\n\n[1] \"2022-10-01\" \"2023-10-15\""
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#why-do-we-convert-to-a-date-object",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#why-do-we-convert-to-a-date-object",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "",
    "text": "Extraction of month and year from a Date object using month() and year() respectively:\n\n\nyear( date_converted )\n\n[1] 2022 2023\n\n\n\nCalculate differences between dates\n\n\nas_date( \"20/04/2025\", format=\"%d/%m/%Y\" ) - as_date( \"2025-02-05\" )\n\nTime difference of 74 days\n\n\n\nCreate plot with time being one of the axis."
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#river-flow-data---converting-variables",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#river-flow-data---converting-variables",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "",
    "text": "We see that the first variable should be of type date\n\n\n\nBathford$V1 &lt;- as_date( Bathford$V1, format=\"%Y-%m-%d\" )\nglimpse( Bathford )\n\nRows: 19,697\nColumns: 2\n$ V1 &lt;date&gt; 1969-10-27, 1969-10-28, 1969-10-29, 1969-10-30, 1969-10-31, 1969-1…\n$ V2 &lt;dbl&gt; 3.998, 3.958, 4.210, 4.480, 4.205, 3.830, 3.723, 3.986, 4.353, 4.52…"
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#renaming-variables",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#renaming-variables",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "",
    "text": "Uninformative variable names can cause problems!"
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#river-flow-data---renaming-variables",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#river-flow-data---renaming-variables",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "",
    "text": "The rename() function in the dplyr package can be used to change variable names.\n\nBathford &lt;- rename( Bathford, Date=V1, RiverFlow=V2 )\nglimpse( Bathford )\n\nRows: 19,697\nColumns: 2\n$ Date      &lt;date&gt; 1969-10-27, 1969-10-28, 1969-10-29, 1969-10-30, 1969-10-31,…\n$ RiverFlow &lt;dbl&gt; 3.998, 3.958, 4.210, 4.480, 4.205, 3.830, 3.723, 3.986, 4.35…"
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#river-flow-data---plotting-the-data",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#river-flow-data---plotting-the-data",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "",
    "text": "plot( Bathford$Date, Bathford$RiverFlow, type='l', \n      xlab=\"Date\", ylab=\"River Flow\" )"
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#exercise",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#exercise",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "",
    "text": "Which conclusions should we report when asked to comment on the frequency of river flow levels above 100m\\(^3\\)/s and the magnitude of river flow levels?\n\nRecorded river flow levels were as high as approximately 250m\\(^3\\)/s.\nThe data exhibits seasonality, with river flow levels being higher in winter than in summer.\nThe data covers the years 1969 to 2023.\nThere is at least one day with river flow levels exceeding 100m\\(^3\\)/s for most years."
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#motivation",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#motivation",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "Motivation",
    "text": "Motivation"
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#why-is-data-wrangling-useful",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#why-is-data-wrangling-useful",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "Why is data wrangling useful?",
    "text": "Why is data wrangling useful?\nWe want to use the available data to address a research question.\nWhen working with large data sets, we usually have to:\n\nIdentify and extract the relevant data points / variables\nCreate new variables from the collected data\nSummarize / sort the data\n\nData wrangling refers to restructuring the raw data to aid with addressing specific research questions."
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#the-dplyr-r-package",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#the-dplyr-r-package",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "The dplyr R package",
    "text": "The dplyr R package\nWe will now introduce several functions provided by the dplyr package for data wrangling:\n\nfilter() and select()\nmutate()\nsummarize() and group_by()\narrange()\nthe pipe %&gt;%"
  },
  {
    "objectID": "slides/week_1/data_cleaning_and_wrangling.html#summary",
    "href": "slides/week_1/data_cleaning_and_wrangling.html#summary",
    "title": "MA22019 - Data Cleaning and Wrangling",
    "section": "Summary",
    "text": "Summary\nWe covered some general aims of the data cleaning process:\n\nConverting expressions to a useful format using as.numeric(), case_when() and as_date()\nGive variables informative names using rename()\n\nWe also explored how to use the dplyr package to:\n\nExtract the relevant observations and variables\nAdd new variables to a data frame\nSummarize and sort the data"
  },
  {
    "objectID": "slides/week_1/analysis_tuscany_data.html",
    "href": "slides/week_1/analysis_tuscany_data.html",
    "title": "Analysis of Tuscany Data Set",
    "section": "",
    "text": "library(dplyr)\nTuscany_raw &lt;- read.csv(\"tuscany.csv\" )"
  },
  {
    "objectID": "slides/week_1/analysis_tuscany_data.html#task-1-compare-populations-for-the-different-provinces-and-towns",
    "href": "slides/week_1/analysis_tuscany_data.html#task-1-compare-populations-for-the-different-provinces-and-towns",
    "title": "Analysis of Tuscany Data Set",
    "section": "Task 1: Compare populations for the different provinces and towns",
    "text": "Task 1: Compare populations for the different provinces and towns\nExplore the data and remove unncessary variables\nPrint first five data entries:\n\nslice_head( Tuscany_raw, n=5 )\n\n  Year Postal_Code  Town Province Age Men Women\n1 2020       45001 Aulla       MS   0  32    30\n2 2020       45001 Aulla       MS   1  30    34\n3 2020       45001 Aulla       MS   2  43    39\n4 2020       45001 Aulla       MS   3  50    35\n5 2020       45001 Aulla       MS   4  38    29\n\n\nMethod 1 - keep relevant variables:\n\nTuscany &lt;- select( Tuscany_raw, Town:Women )\n\nMethod 2 - remove variables which are not important for the analysis:\n\nTuscany &lt;- select( Tuscany_raw, -Year, -Postal_Code )\n\nCreate a variable which represents population\n\nTuscany &lt;- mutate( Tuscany, Population = Men + Women )\nslice_head( Tuscany, n=5 )\n\n   Town Province Age Men Women Population\n1 Aulla       MS   0  32    30         62\n2 Aulla       MS   1  30    34         64\n3 Aulla       MS   2  43    39         82\n4 Aulla       MS   3  50    35         85\n5 Aulla       MS   4  38    29         67\n\n\nThe pipe %&gt;%\nWe can combine the previous commands into a single piece of code using %&gt;%:\n\nTuscany &lt;- Tuscany_raw %&gt;%\n  mutate( Population = Men + Women ) %&gt;%\n  select( -Year, -Postal_Code )\nslice_head( Tuscany, n=5 )\n\n   Town Province Age Men Women Population\n1 Aulla       MS   0  32    30         62\n2 Aulla       MS   1  30    34         64\n3 Aulla       MS   2  43    39         82\n4 Aulla       MS   3  50    35         85\n5 Aulla       MS   4  38    29         67\n\n\nSummarizing the data\nWe use summarize() to extract summaries for three variables\n\nTuscany %&gt;%\n  summarize( \"Population_Tuscany_2020\" = sum( Population ),\n             \"Men_Tuscany_2020\" = sum( Men ),\n             \"Women_Tuscany_2020\" = sum( Women ) )\n\n  Population_Tuscany_2020 Men_Tuscany_2020 Women_Tuscany_2020\n1                 3691409          1787649            1903760\n\n\nDerive the population for each province\nWe combine summarize() and group_by():\n\nTuscany_Province &lt;- Tuscany %&gt;%\n  group_by( Province ) %&gt;%\n  summarize( Total = sum(Population) )\nTuscany_Province\n\n# A tibble: 10 × 2\n   Province  Total\n   &lt;chr&gt;     &lt;int&gt;\n 1 AR       336450\n 2 FI       997940\n 3 GR       217803\n 4 LI       328855\n 5 LU       383688\n 6 MS       189786\n 7 PI       417799\n 8 PO       265153\n 9 PT       290177\n10 SI       263758\n\n\nSort provinces based on population\nWe use the arrange() function:\nTo sort in ascending order:\n\narrange( Tuscany_Province, Total )\n\n# A tibble: 10 × 2\n   Province  Total\n   &lt;chr&gt;     &lt;int&gt;\n 1 MS       189786\n 2 GR       217803\n 3 SI       263758\n 4 PO       265153\n 5 PT       290177\n 6 LI       328855\n 7 AR       336450\n 8 LU       383688\n 9 PI       417799\n10 FI       997940\n\n\nTo sort in descending order:\n\nTuscany_Province %&gt;% arrange( desc(Total) )\n\n# A tibble: 10 × 2\n   Province  Total\n   &lt;chr&gt;     &lt;int&gt;\n 1 FI       997940\n 2 PI       417799\n 3 LU       383688\n 4 AR       336450\n 5 LI       328855\n 6 PT       290177\n 7 PO       265153\n 8 SI       263758\n 9 GR       217803\n10 MS       189786"
  },
  {
    "objectID": "slides/week_1/analysis_tuscany_data.html#task-2-explore-age-profile-of-the-population",
    "href": "slides/week_1/analysis_tuscany_data.html#task-2-explore-age-profile-of-the-population",
    "title": "Analysis of Tuscany Data Set",
    "section": "Task 2: Explore age profile of the population",
    "text": "Task 2: Explore age profile of the population\nExtract proportion of women at various ages:\n\nTuscany_Women_Age &lt;- Tuscany %&gt;%\n  group_by( Age ) %&gt;%\n  summarize( Number = sum(Women) ) %&gt;%\n  mutate( Proportion = Number / sum(Number) )\n\nVisualize the proportions:\n\nbarplot( Proportion~Age, data=Tuscany_Women_Age )"
  },
  {
    "objectID": "slides/week_1/analysis_nrfa_data_for_bathford.html",
    "href": "slides/week_1/analysis_nrfa_data_for_bathford.html",
    "title": "Analysis of NRFA Data",
    "section": "",
    "text": "library( lubridate )\nlibrary( dplyr )\n\n\nBathford_RF &lt;- read.csv(\"bathford_river_flow.csv\", skip=20, header=FALSE,\n                         colClasses = c(\"character\",\"numeric\",\"NULL\") ) \n\nWarning in read.table(file = file, header = header, sep = sep, quote = quote, :\ncols = 2 != length(data) = 3"
  },
  {
    "objectID": "slides/week_1/analysis_nrfa_data_for_bathford.html#loading-the-data-and-r-packages",
    "href": "slides/week_1/analysis_nrfa_data_for_bathford.html#loading-the-data-and-r-packages",
    "title": "Analysis of NRFA Data",
    "section": "",
    "text": "library( lubridate )\nlibrary( dplyr )\n\n\nBathford_RF &lt;- read.csv(\"bathford_river_flow.csv\", skip=20, header=FALSE,\n                         colClasses = c(\"character\",\"numeric\",\"NULL\") ) \n\nWarning in read.table(file = file, header = header, sep = sep, quote = quote, :\ncols = 2 != length(data) = 3"
  },
  {
    "objectID": "slides/week_1/analysis_nrfa_data_for_bathford.html#data-cleaning",
    "href": "slides/week_1/analysis_nrfa_data_for_bathford.html#data-cleaning",
    "title": "Analysis of NRFA Data",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nCheck which variables need to converted and renamed\n\nglimpse( Bathford_RF )\n\nRows: 19,697\nColumns: 2\n$ V1 &lt;chr&gt; \"1969-10-27\", \"1969-10-28\", \"1969-10-29\", \"1969-10-30\", \"1969-10-31…\n$ V2 &lt;dbl&gt; 3.998, 3.958, 4.210, 4.480, 4.205, 3.830, 3.723, 3.986, 4.353, 4.52…\n\n\nChange variables names\n\nBathford_RF &lt;- rename( Bathford_RF, Date = V1, RiverFlow = V2 )\n\nConvert to correct type\n\nBathford_RF$Date &lt;- as_date( Bathford_RF$Date, format=\"%Y-%m-%d\" )"
  },
  {
    "objectID": "slides/week_1/analysis_nrfa_data_for_bathford.html#data-wrangling-and-visualization",
    "href": "slides/week_1/analysis_nrfa_data_for_bathford.html#data-wrangling-and-visualization",
    "title": "Analysis of NRFA Data",
    "section": "Data Wrangling and visualization",
    "text": "Data Wrangling and visualization\nProportion of missing data\n\nmean( is.na( Bathford_RF$RiverFlow ) )\n\n[1] 0.006244606\n\n\nPlot river flow over time\n\nplot( Bathford_RF$Date, Bathford_RF$RiverFlow, type='l',\n      xlab=\"Date\", ylab=\"River Flow\", cex.lab = 1.5 )\n\n\n\n\n\n\n\nFilter observations based on date and observed river flow\n\nBathford_RF_High &lt;- filter( Bathford_RF, RiverFlow &gt; 100 )\nslice_head( Bathford_RF_High, n=5 )\n\n        Date RiverFlow\n1 1970-11-19     104.3\n2 1971-01-21     128.4\n3 1971-01-22     114.8\n4 1971-01-23     115.2\n5 1971-01-24     133.1\n\n\n\nBathford_RF_High &lt;- filter( Bathford_RF, RiverFlow &gt; 100, year(Date) &gt; 1990 )\nslice_head( Bathford_RF_High, n=5 )\n\n        Date RiverFlow\n1 1991-01-10     112.1\n2 1992-11-26     106.9\n3 1992-11-27     100.9\n4 1992-11-28     100.6\n5 1992-11-29     131.6"
  },
  {
    "objectID": "slides/week_6/analysis_of_nyt_articles.html",
    "href": "slides/week_6/analysis_of_nyt_articles.html",
    "title": "Analysis of NYT Articles",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(tidytext)\nlibrary(tidyr)\nlibrary(topicmodels)\n\nWe work with 1289 articles published by the New York Times in their “Europe” subsection between 01/01/2023 and 01/11/2024:\n\nNYT &lt;- read.csv(\"nyt.csv\" )\n\nSet identifier for each article:\n\nNYT &lt;- NYT %&gt;% mutate( ID=1:nrow(NYT) )"
  },
  {
    "objectID": "slides/week_6/analysis_of_nyt_articles.html#loading-the-packages-and-data",
    "href": "slides/week_6/analysis_of_nyt_articles.html#loading-the-packages-and-data",
    "title": "Analysis of NYT Articles",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(tidytext)\nlibrary(tidyr)\nlibrary(topicmodels)\n\nWe work with 1289 articles published by the New York Times in their “Europe” subsection between 01/01/2023 and 01/11/2024:\n\nNYT &lt;- read.csv(\"nyt.csv\" )\n\nSet identifier for each article:\n\nNYT &lt;- NYT %&gt;% mutate( ID=1:nrow(NYT) )"
  },
  {
    "objectID": "slides/week_6/analysis_of_nyt_articles.html#constructing-the-document-term-matrix",
    "href": "slides/week_6/analysis_of_nyt_articles.html#constructing-the-document-term-matrix",
    "title": "Analysis of NYT Articles",
    "section": "Constructing the Document Term Matrix",
    "text": "Constructing the Document Term Matrix\n\nNYT_count &lt;- NYT %&gt;%\n  unnest_tokens( word, lead_paragraph ) %&gt;%\n  anti_join( stop_words, by=\"word\" ) %&gt;%\n  count( ID, word, sort = TRUE )\n\n\nNYT_dtm &lt;- NYT_count %&gt;% cast_dtm( ID, word, n )"
  },
  {
    "objectID": "slides/week_6/analysis_of_nyt_articles.html#estimaet-the-latent-dirichlet-allocation-model",
    "href": "slides/week_6/analysis_of_nyt_articles.html#estimaet-the-latent-dirichlet-allocation-model",
    "title": "Analysis of NYT Articles",
    "section": "Estimaet the Latent Dirichlet Allocation Model",
    "text": "Estimaet the Latent Dirichlet Allocation Model\n\nNYT_LDA &lt;- LDA( NYT_dtm, k = 2, method=\"Gibbs\", control = list(seed=2024) )"
  },
  {
    "objectID": "slides/week_6/analysis_of_nyt_articles.html#analysis-of-the-results",
    "href": "slides/week_6/analysis_of_nyt_articles.html#analysis-of-the-results",
    "title": "Analysis of NYT Articles",
    "section": "Analysis of the results",
    "text": "Analysis of the results\nMake up of articles\n\ntidy( NYT_LDA , matrix = \"gamma\" ) %&gt;%\n  filter( topic==1 ) %&gt;%\n  ggplot( aes( y=gamma ) ) + theme_bw() + \n  geom_boxplot() + labs( y=\"Proportion\" )\n\n\n\n\n\n\n\nMake up of topics\n\ntidy( NYT_LDA , matrix = \"beta\" ) %&gt;%\n  mutate( topic = case_when( topic==1 ~ \"Topic1\", topic==2 ~ \"Topic2\") ) %&gt;%\n  pivot_wider( names_from = topic, values_from = beta, values_fill = 0 ) %&gt;%\n  ggplot( aes(x=Topic1, y=Topic2) ) +  coord_trans( x=\"sqrt\", y=\"sqrt\" ) + \n  geom_point() + theme_bw() +\n  geom_text( aes(label=term), check_overlap = TRUE, vjust=1 ) + \n  labs( x=\"Term Frequency in Topic 1\", y=\"Term Frequency in Topic 2\" )\n\nWarning: `coord_trans()` was deprecated in ggplot2 4.0.0.\nℹ Please use `coord_transform()` instead."
  },
  {
    "objectID": "slides/week_3/analysis_of_wind_speed_and_direction_for_bela_vista.html",
    "href": "slides/week_3/analysis_of_wind_speed_and_direction_for_bela_vista.html",
    "title": "Analysis of wind in Bela Vista",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)"
  },
  {
    "objectID": "slides/week_3/analysis_of_wind_speed_and_direction_for_bela_vista.html#loading-the-data",
    "href": "slides/week_3/analysis_of_wind_speed_and_direction_for_bela_vista.html#loading-the-data",
    "title": "Analysis of wind in Bela Vista",
    "section": "Loading the data",
    "text": "Loading the data\n\nwind_BV &lt;- read.csv(\"wind_bela_vista.csv\")\n\n\nglimpse(wind_BV)\n\nRows: 16,142\nColumns: 5\n$ Date           &lt;chr&gt; \"01/01/2017\", \"01/01/2017\", \"01/01/2017\", \"01/01/2017\",…\n$ Hour           &lt;chr&gt; \"00:00\", \"01:00\", \"02:00\", \"03:00\", \"04:00\", \"05:00\", \"…\n$ Wind.Direction &lt;int&gt; 21, 22, 31, 28, 20, 27, 29, 29, 21, 28, 42, 21, 18, 356…\n$ Gust.Speed     &lt;dbl&gt; 13.9, 8.2, 8.2, 7.7, 7.9, 7.9, 6.6, 6.1, 5.8, 5.2, 4.7,…\n$ Average.Speed  &lt;dbl&gt; 3.8, 3.9, 4.3, 3.6, 3.7, 3.4, 3.2, 2.9, 2.3, 2.3, 2.2, …"
  },
  {
    "objectID": "slides/week_3/analysis_of_wind_speed_and_direction_for_bela_vista.html#histogram-for-wind-direction",
    "href": "slides/week_3/analysis_of_wind_speed_and_direction_for_bela_vista.html#histogram-for-wind-direction",
    "title": "Analysis of wind in Bela Vista",
    "section": "Histogram for wind direction",
    "text": "Histogram for wind direction\n\nPlot_Cartesian &lt;- ggplot(wind_BV, aes(x = Wind.Direction)) +\n    geom_histogram(bins = 120) +\n    labs(x = \"Wind Direction\")\n\nPlot_Polar &lt;- Plot_Cartesian + coord_polar()\n\nPlot_Cartesian + Plot_Polar\n\n\n\n\n\n\n\n\nPlot_Cartesian"
  },
  {
    "objectID": "slides/week_3/analysis_of_wind_speed_and_direction_for_bela_vista.html#relation-between-wind-speed-and-direction",
    "href": "slides/week_3/analysis_of_wind_speed_and_direction_for_bela_vista.html#relation-between-wind-speed-and-direction",
    "title": "Analysis of wind in Bela Vista",
    "section": "Relation between wind speed and direction",
    "text": "Relation between wind speed and direction\n\nPlot_Cartesian &lt;-\n    ggplot(wind_BV, aes(x = Wind.Direction, y = Gust.Speed)) +\n    geom_point() +\n    geom_smooth() +\n    labs(x = \"Wind Direction\", y = \"Wind gust speed in m/s\")\n\nPlot_Polar &lt;- Plot_Cartesian + coord_polar(theta = \"x\")\n\nPlot_Cartesian + Plot_Polar\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'"
  },
  {
    "objectID": "slides/week_3/analysis_of_wind_speed_and_direction_for_bela_vista.html#producing-pie-charts-in-r",
    "href": "slides/week_3/analysis_of_wind_speed_and_direction_for_bela_vista.html#producing-pie-charts-in-r",
    "title": "Analysis of wind in Bela Vista",
    "section": "Producing pie charts in R",
    "text": "Producing pie charts in R\n\ndf &lt;- data.frame(\"prob\" = c(0.3, 0.4, 0.3), \"group\" = c(\"A\", \"B\", \"C\"))\n\nBarplot &lt;- ggplot(df, aes(x = \"\", y = prob, fill = group)) +\n    geom_col() +\n    labs(x = \"\", y = \"\")\n\nPieChart &lt;- Barplot + coord_polar(theta = \"y\")\n\nBarplot + PieChart"
  },
  {
    "objectID": "slides/week_3/change_in_scale___examples.html",
    "href": "slides/week_3/change_in_scale___examples.html",
    "title": "Changing scales - Example",
    "section": "",
    "text": "library(ggplot2)\nlibrary(patchwork)"
  },
  {
    "objectID": "slides/week_3/change_in_scale___examples.html#example-1---relation-between-body-and-brain-weight-for-mammals",
    "href": "slides/week_3/change_in_scale___examples.html#example-1---relation-between-body-and-brain-weight-for-mammals",
    "title": "Changing scales - Example",
    "section": "Example 1 - Relation between body and brain weight for mammals",
    "text": "Example 1 - Relation between body and brain weight for mammals\nLoad the data:\n\nMammals &lt;- read.csv(\"mammals.csv\" )\n\nLet’s compare linear scale (left) and logarithmic scale (right):\n\ng1 &lt;- ggplot( Mammals, aes( x=body, y=brain ) ) + geom_point() + \n  labs( title=\"Brain vs body weight on linear scale\",\n        x=\"Body Weight in kg\", y=\"Brain weight in g\" )\n\ng2 &lt;- g1 + coord_trans( x=\"log10\", y=\"log10\" )\n\nWarning: `coord_trans()` was deprecated in ggplot2 4.0.0.\nℹ Please use `coord_transform()` instead.\n\ng1 + g2"
  },
  {
    "objectID": "slides/week_3/change_in_scale___examples.html#example-2---analysis-of-facebook-data",
    "href": "slides/week_3/change_in_scale___examples.html#example-2---analysis-of-facebook-data",
    "title": "Changing scales - Example",
    "section": "Example 2 - Analysis of Facebook Data",
    "text": "Example 2 - Analysis of Facebook Data\n\nFacebook &lt;- read.csv(\"facebook.csv\", header=TRUE )\n\ng1  &lt;- ggplot( Facebook, aes( x=follow, y=postlikes ) ) + \n  geom_point() +\n  labs( x=\"Number of Followers\", y=\"Number of likes\" )\ng2 &lt;- g1 + coord_trans( x=\"log\", y=\"log\" )\ng1 + g2"
  },
  {
    "objectID": "slides/week_3/change_in_scale___examples.html#change-of-font-size-and-theme",
    "href": "slides/week_3/change_in_scale___examples.html#change-of-font-size-and-theme",
    "title": "Changing scales - Example",
    "section": "Change of font size and theme",
    "text": "Change of font size and theme\n\ng2 &lt;- g2 + theme_bw() +  \n  theme( axis.title=element_text(size=16),\n         axis.text=element_text(size=14) )\ng1 + g2"
  },
  {
    "objectID": "slides/week_4/notes_week_4.html",
    "href": "slides/week_4/notes_week_4.html",
    "title": "MA22019 - Coursework 1 Q&A",
    "section": "",
    "text": "Week 4 Questionnaire - Please take 5 minutes to complete it\nNo tutorials next week (Week 5)\n\nCoursework 2:\n\nRelease on Friday 25 April (end of Week 10)\nDue on Tuesday 6 May (beginning of Revision Week)\n\n\nI will try to post questions and answers on the Padlet board\nWe will finish at 11:05."
  },
  {
    "objectID": "slides/week_4/notes_week_4.html#some-important-remarks",
    "href": "slides/week_4/notes_week_4.html#some-important-remarks",
    "title": "MA22019 - Coursework 1 Q&A",
    "section": "",
    "text": "Week 4 Questionnaire - Please take 5 minutes to complete it\nNo tutorials next week (Week 5)\n\nCoursework 2:\n\nRelease on Friday 25 April (end of Week 10)\nDue on Tuesday 6 May (beginning of Revision Week)\n\n\nI will try to post questions and answers on the Padlet board\nWe will finish at 11:05."
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html",
    "href": "slides/week_5/text_data_analysis_(part_1).html",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "",
    "text": "We start Chapter 3 on Text Data Analysis:\n\nWhat is text data and why is it important?\nAnalysis of word frequency (Section 3.1)\nSentiment Analysis (Section 3.2)\nComparing term frequencies (Section 3.3.1)"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#plan-for-today",
    "href": "slides/week_5/text_data_analysis_(part_1).html#plan-for-today",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "",
    "text": "We start Chapter 3 on Text Data Analysis:\n\nWhat is text data and why is it important?\nAnalysis of word frequency (Section 3.1)\nSentiment Analysis (Section 3.2)\nComparing term frequencies (Section 3.3.1)"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#summary-of-week-4-feedback",
    "href": "slides/week_5/text_data_analysis_(part_1).html#summary-of-week-4-feedback",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "Summary of Week 4 Feedback",
    "text": "Summary of Week 4 Feedback\n\n19 submissions (out of 165)\nMajority of students seems happy with how the course is run\n\nConcerns that were raised by more than one student:\n\nVagueness and lack of context in the coursework\nLack of complex examples and a fully written report\nAmount of questions on the problem sheets"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#what-is-text-data",
    "href": "slides/week_5/text_data_analysis_(part_1).html#what-is-text-data",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "What is text data?",
    "text": "What is text data?"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#applications",
    "href": "slides/week_5/text_data_analysis_(part_1).html#applications",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "Applications",
    "text": "Applications\nText data is collected widely, for instance, for\n\nOptimizing search engines\nCrime prevention\nCustomer service\n\nIn all these applications, we are interested in\n\nThe words used within the text\nThe intention of the text"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#loading-text-data-from-a-.txt-file",
    "href": "slides/week_5/text_data_analysis_(part_1).html#loading-text-data-from-a-.txt-file",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "Loading text data from a .txt file",
    "text": "Loading text data from a .txt file\nOne way to store text data is as .txt file.\nThe file “Hesse quote.txt” contains a short quote that we load with readLines()\n\ntext_Hesse &lt;- readLines( \"hesse_quote.txt\" )\ntext_Hesse\n\n[1] \"Some of us think holding on makes us strong,\"\n[2] \"but sometimes it is letting go.\"             \n\n\nIs this data in a useful format?"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#transforming-text-data",
    "href": "slides/week_5/text_data_analysis_(part_1).html#transforming-text-data",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "Transforming text data",
    "text": "Transforming text data\nLet’s define a data frame\n\nquote_Hesse &lt;- data.frame( line=1:2, text=text_Hesse )\nquote_Hesse\n\n  line                                         text\n1    1 Some of us think holding on makes us strong,\n2    2              but sometimes it is letting go.\n\n\nDoes this help?"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#the-tidy-text-format-i",
    "href": "slides/week_5/text_data_analysis_(part_1).html#the-tidy-text-format-i",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "The tidy text format I",
    "text": "The tidy text format I\nWe focus on analysing the individual words in a text.\nAs such,\n\neach word is an individual observation\nwe do not care about punctuation\n\nWe use the unnest_tokens() function in the tidytext package to extract the individual words\n\nlibrary(tidytext)\nHesse_tidy &lt;- quote_Hesse %&gt;% unnest_tokens( word, text )"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#the-tidy-text-format-ii",
    "href": "slides/week_5/text_data_analysis_(part_1).html#the-tidy-text-format-ii",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "The tidy text format II",
    "text": "The tidy text format II\n\nhead( Hesse_tidy )\n\n  line    word\n1    1    some\n2    1      of\n3    1      us\n4    1   think\n5    1 holding\n6    1      on\n\n\nThe tidy text format propagates:\n\nEach variable is a column: our variable of interest is word\nEach observation is a row: we have one word per row"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#charlotte-brontë",
    "href": "slides/week_5/text_data_analysis_(part_1).html#charlotte-brontë",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "Charlotte Brontë",
    "text": "Charlotte Brontë\n\n\n\n\n\n\n\n\nJane Eyre is a novel that was published in 1847."
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#loading-the-data",
    "href": "slides/week_5/text_data_analysis_(part_1).html#loading-the-data",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "Loading the data",
    "text": "Loading the data\nWe download the full text from Project Gutenberg.\n-&gt; R Markdown file\nThe book has about 21,000 lines of text, and we separate them into individual words:\n\nJaneEyre &lt;- JaneEyre_raw %&gt;% unnest_tokens( word, text )\n\nThere is some data cleaning to be done! \n-&gt; R Markdown file"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#calcualting-term-frequency",
    "href": "slides/week_5/text_data_analysis_(part_1).html#calcualting-term-frequency",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "Calcualting term frequency",
    "text": "Calcualting term frequency\nExploring the frequency of the different words in a text is a common approach in text data analysis.\nWe use functions in the dplyr R package for this\n\nlibrary(dplyr)\nJaneEyre_Count &lt;- JaneEyre %&gt;% \n  count( word, sort=TRUE ) %&gt;%\n  mutate( 'term frequency'=n/sum(n), rank=row_number() )\nslice_head( JaneEyre_Count, n=10 )\n\n-&gt; R Markdown file"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#illustration",
    "href": "slides/week_5/text_data_analysis_(part_1).html#illustration",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "Illustration",
    "text": "Illustration\nLet’s create a plot of term frequency versus rank on log scale\n\n\n\n\n\n\n\n\nWhat do we notice?"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#zipfs-law",
    "href": "slides/week_5/text_data_analysis_(part_1).html#zipfs-law",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "Zipf’s Law",
    "text": "Zipf’s Law\n\nRelationship between rank and term frequency on logarithmic scale is close to linear\nThe non-linear shape in the top-left is driven by only six words.\nZipf’s Law states that empirically a word’s frequency is inversely proportional to its rank."
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#stop-words",
    "href": "slides/week_5/text_data_analysis_(part_1).html#stop-words",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "Stop words",
    "text": "Stop words\nThe words “the”, “I” and “and” appear most often in Jane Eyre.\nIs this information useful?\nIt is quite common in text data analysis to specify stop words which are ignored in the analysis.\nThe tidytext package comes with its own list of stop words:\n\nlibrary( tidytext )\ndata( stop_words )\n\n-&gt; R Markdown file"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#visualizing-word-frequency",
    "href": "slides/week_5/text_data_analysis_(part_1).html#visualizing-word-frequency",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "Visualizing word frequency",
    "text": "Visualizing word frequency\nThere are two options for visualizing the most frequent words:\n\nBar plots - good to display the actual counts\nWord clouds - good for many words\n\nLet’s produce these plots for Jane Eyre\n-&gt; R Markdown file"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#what-do-we-mean-by-sentiment",
    "href": "slides/week_5/text_data_analysis_(part_1).html#what-do-we-mean-by-sentiment",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "What do we mean by sentiment?",
    "text": "What do we mean by sentiment?\nThe emotional intent of the text:\n\nIs a text more positive or negative?\nIs a review positive or negative?\nHow does the story evolve throughout the book?"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#measuring-sentiment",
    "href": "slides/week_5/text_data_analysis_(part_1).html#measuring-sentiment",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "Measuring sentiment",
    "text": "Measuring sentiment\nWe start by assigning a sentiment to each individual word using a sentiment lexicon.\nThe following two lexicons are provided by tidytext:\n\nAFINN: Sentiment score between -5 and +5.\nBing: Words are categorized as “positive” or “negative”.\n\nThe sentiment of a text is then the sum (or mean) of the sentiment of the words.\nIs this a weak, moderate or strong assumption?\nFor which pieces of text may our approach perform poorly?"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#sentiment-analysis-for-jane-eyre",
    "href": "slides/week_5/text_data_analysis_(part_1).html#sentiment-analysis-for-jane-eyre",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "Sentiment analysis for Jane Eyre\n",
    "text": "Sentiment analysis for Jane Eyre\n\nWe already analysed word frequency for the book.\nLet’s see what we can say about the sentiment.\n-&gt; R Markdown file"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#what-to-remember",
    "href": "slides/week_5/text_data_analysis_(part_1).html#what-to-remember",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "What to remember",
    "text": "What to remember\nSentiment analysis assesses whether a text has a positive or negative emotional intent.\nWe measure sentiment via the individual words.  We make an assumption here!\nImportant: Stop words are not removed, because they may be important.\nRemark: The relative change in sentiment is often more accurate than the actual values."
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#motivation",
    "href": "slides/week_5/text_data_analysis_(part_1).html#motivation",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "Motivation",
    "text": "Motivation\nIn many applications we are interested in comparing two (or more) pieces of text.\nCan you think of examples?\nUsing the methods we introduced, we may explore two aspects:\n\nWord frequency\nSentiment"
  },
  {
    "objectID": "slides/week_5/text_data_analysis_(part_1).html#comparison-using-term-frequency",
    "href": "slides/week_5/text_data_analysis_(part_1).html#comparison-using-term-frequency",
    "title": "MA22019 - Text Data Analysis (Part 1)",
    "section": "Comparison using term frequency",
    "text": "Comparison using term frequency\nInterest in exploring two pieces of text in terms of term frequency of the individual words.\nWhy should we not compare the actual counts?\nLet’s compare the novels Jane Eyre and Wuthering Heights\n-&gt; R Markdown file"
  },
  {
    "objectID": "slides/week_2/data_visualization_(part_1).html",
    "href": "slides/week_2/data_visualization_(part_1).html",
    "title": "MA22019 - Data Visualization (Part 1)",
    "section": "",
    "text": "Data exploration requires generating and analyzing plots of the data.\nThis raises the questions:\n\nWhat are the general guidelines for data visualization?\nHow can we create effective plots in R?\nHow do we decide which plot to produce?\n\n\nPoor graphics will prevent us from clearly communicating our findings."
  },
  {
    "objectID": "slides/week_2/data_visualization_(part_1).html#motivation",
    "href": "slides/week_2/data_visualization_(part_1).html#motivation",
    "title": "MA22019 - Data Visualization (Part 1)",
    "section": "",
    "text": "Data exploration requires generating and analyzing plots of the data.\nThis raises the questions:\n\nWhat are the general guidelines for data visualization?\nHow can we create effective plots in R?\nHow do we decide which plot to produce?\n\n\nPoor graphics will prevent us from clearly communicating our findings."
  },
  {
    "objectID": "slides/week_2/data_visualization_(part_1).html#important",
    "href": "slides/week_2/data_visualization_(part_1).html#important",
    "title": "MA22019 - Data Visualization (Part 1)",
    "section": "Important",
    "text": "Important\nWe are not aiming to impress others with beautiful pictures:"
  },
  {
    "objectID": "slides/week_2/data_visualization_(part_1).html#the-four-elements-of-a-plot",
    "href": "slides/week_2/data_visualization_(part_1).html#the-four-elements-of-a-plot",
    "title": "MA22019 - Data Visualization (Part 1)",
    "section": "The four elements of a plot",
    "text": "The four elements of a plot\nWe will describe and produce data graphics in terms of four elements:\n\nVisual cues\nCoordinate system\nScales\nContext\n\nToday we focus on visual cues."
  },
  {
    "objectID": "slides/week_2/data_visualization_(part_1).html#visual-cues",
    "href": "slides/week_2/data_visualization_(part_1).html#visual-cues",
    "title": "MA22019 - Data Visualization (Part 1)",
    "section": "Visual cues",
    "text": "Visual cues\nWe consider nine distinct visual cues\n\nPosition (quantity)\nLength (quantity)\nAngle (quantity)\nDirection (quantity)\nShape (category)\nArea and Volume (quantity)\nShade and Colour (quantity or category)"
  },
  {
    "objectID": "slides/week_2/data_visualization_(part_1).html#example",
    "href": "slides/week_2/data_visualization_(part_1).html#example",
    "title": "MA22019 - Data Visualization (Part 1)",
    "section": "Example",
    "text": "Example\n\nWhich visual cues are used in the following plot?\n\n\n\n\n\n\n\n\n\n\nWhich conclusions can we draw?"
  },
  {
    "objectID": "slides/week_2/data_visualization_(part_1).html#data",
    "href": "slides/week_2/data_visualization_(part_1).html#data",
    "title": "MA22019 - Data Visualization (Part 1)",
    "section": "Data",
    "text": "Data\nDaily weather data for five Australian cities:"
  },
  {
    "objectID": "slides/week_2/data_visualization_(part_1).html#the-plots-we-consider",
    "href": "slides/week_2/data_visualization_(part_1).html#the-plots-we-consider",
    "title": "MA22019 - Data Visualization (Part 1)",
    "section": "The plots we consider",
    "text": "The plots we consider\nWe will today consider the following plots and produce them using ggplot2:\n\nScatter plot\nLine plot\nHistogram and density plot\nBox plot and violin plot\n\n\nWhich of these have you heard of?"
  },
  {
    "objectID": "slides/week_2/data_visualization_(part_1).html#density-plot",
    "href": "slides/week_2/data_visualization_(part_1).html#density-plot",
    "title": "MA22019 - Data Visualization (Part 1)",
    "section": "Density Plot",
    "text": "Density Plot\nGiven data \\(x_1,\\ldots,x_n\\) for a random variable \\(X\\), we estimate the probability density function of \\(X\\) as [ (x) = _{i=1}^{n} K(), ]\n\n\\(K\\) is called the kernel and it is a probability density function\n\\(h\\) is the bandwidth"
  },
  {
    "objectID": "slides/week_2/data_visualization_(part_1).html#importance-of-bandwidth",
    "href": "slides/week_2/data_visualization_(part_1).html#importance-of-bandwidth",
    "title": "MA22019 - Data Visualization (Part 1)",
    "section": "Importance of bandwidth",
    "text": "Importance of bandwidth\n\n\n\n\n\n\n\n\n\nA too small \\(h\\) leads to undersmoothing (left)\nA too large \\(h\\) leads to oversmoothing (right)"
  },
  {
    "objectID": "slides/week_2/data_visualization_(part_1).html#histogram-vs-density-plot",
    "href": "slides/week_2/data_visualization_(part_1).html#histogram-vs-density-plot",
    "title": "MA22019 - Data Visualization (Part 1)",
    "section": "Histogram vs Density plot",
    "text": "Histogram vs Density plot"
  },
  {
    "objectID": "slides/week_2/data_visualization_(part_1).html#which-plot-should-i-choose",
    "href": "slides/week_2/data_visualization_(part_1).html#which-plot-should-i-choose",
    "title": "MA22019 - Data Visualization (Part 1)",
    "section": "Which plot should I choose?",
    "text": "Which plot should I choose?\nWhile the following guidance may not always give the best choice, it often works fine in practice:\nScatter plots: Useful to compare the relation between two variables.\nLine plots: Useful to explore how a variable evolves over time.\nHistogram/Density plots: Good for exploring the distribution of a variable.\nBox plots/Violin plots: Good to compare the distributions of multiple variables."
  },
  {
    "objectID": "slides/week_2/data_visualization_(part_1).html#exercises",
    "href": "slides/week_2/data_visualization_(part_1).html#exercises",
    "title": "MA22019 - Data Visualization (Part 1)",
    "section": "Exercises",
    "text": "Exercises\nDecide which plot(s) to use in the following situations:\n\nAssociation between amount of exercise and sleep.\nDistribution of annual maximum temperature for Bath.\nImpact of insulation on energy consumption.\n\nIs length or position better in terms of our ability to perceive differences in magnitude?"
  },
  {
    "objectID": "slides/week_2/data_visualization_(part_1).html#summary",
    "href": "slides/week_2/data_visualization_(part_1).html#summary",
    "title": "MA22019 - Data Visualization (Part 1)",
    "section": "Summary",
    "text": "Summary\n\nData graphics can be described in terms of visual cues\nVisual cues differ in how easy it is to perceive differences in magnitude\nWe should not rely too much on colour as a visual cue\nVarious types of plots can be produced in ggplot2\n\n\nWhat next?\n\n\nOn Friday we apply these techniques to another data set.\nThe remaining visual cues are covered next week."
  },
  {
    "objectID": "slides/week_2/combining_data_frames.html",
    "href": "slides/week_2/combining_data_frames.html",
    "title": "MA22019 - Combining data frames",
    "section": "",
    "text": "So far we have analyzed a single complex data frame using the dplyr R package.\nToday we explore\n\nhow we can combine multiple data frames (Section 1.3)\nthe elements of a plot (Sections 2.1.1 and 2.1.2)\nvisualizing data using ggplot2 (Section 2.2)"
  },
  {
    "objectID": "slides/week_2/combining_data_frames.html#plan-for-today",
    "href": "slides/week_2/combining_data_frames.html#plan-for-today",
    "title": "MA22019 - Combining data frames",
    "section": "",
    "text": "So far we have analyzed a single complex data frame using the dplyr R package.\nToday we explore\n\nhow we can combine multiple data frames (Section 1.3)\nthe elements of a plot (Sections 2.1.1 and 2.1.2)\nvisualizing data using ggplot2 (Section 2.2)"
  },
  {
    "objectID": "slides/week_2/combining_data_frames.html#motivation",
    "href": "slides/week_2/combining_data_frames.html#motivation",
    "title": "MA22019 - Combining data frames",
    "section": "Motivation",
    "text": "Motivation\nLet’s revisit the river flow data for Bathford\n\nlibrary(dplyr)\nlibrary(lubridate)\nBathford &lt;- read.csv(\"bathford_river_flow.csv\", skip=20, header=FALSE,\n                      colClasses = c(\"character\",\"numeric\",\"NULL\") )  %&gt;%\n  rename( Date = V1, RiverFlow = V2 ) %&gt;%\n  mutate( Date = as_date( Date, format=\"%Y-%m-%d\" ) )\nglimpse( Bathford )\n\nRows: 19,697\nColumns: 2\n$ Date      &lt;date&gt; 1969-10-27, 1969-10-28, 1969-10-29, 1969-10-30, 1969-10-31,…\n$ RiverFlow &lt;dbl&gt; 3.998, 3.958, 4.210, 4.480, 4.205, 3.830, 3.723, 3.986, 4.35…"
  },
  {
    "objectID": "slides/week_2/combining_data_frames.html#reporting-missing-data",
    "href": "slides/week_2/combining_data_frames.html#reporting-missing-data",
    "title": "MA22019 - Combining data frames",
    "section": "Reporting Missing Data",
    "text": "Reporting Missing Data\nWhen we work with real-world data, we have to check whether data are missing.\nIf there is missing data, you should report the amount of missing data!\n\nBathford %&gt;% \n  summarize( \"Proportion Missing\" = mean( is.na(RiverFlow) ) )\n\n  Proportion Missing\n1        0.006244606\n\n\nWe see that river flow is missing for about 0.6% of days."
  },
  {
    "objectID": "slides/week_2/combining_data_frames.html#combining-data-sets-with-dplyr",
    "href": "slides/week_2/combining_data_frames.html#combining-data-sets-with-dplyr",
    "title": "MA22019 - Combining data frames",
    "section": "Combining data sets with dplyr",
    "text": "Combining data sets with dplyr\nHow can we add data from other gauges to the data frame?\nThe dplyr package offers four functions to combine data frames based on matching values, which we call the “key(s)”.\n\ninner_join()\nfull_join()\nleft_join()\nright_join()\n\nThe functions differ in terms of observations kept."
  },
  {
    "objectID": "slides/week_2/combining_data_frames.html#example---loading-the-data",
    "href": "slides/week_2/combining_data_frames.html#example---loading-the-data",
    "title": "MA22019 - Combining data frames",
    "section": "Example - Loading the data",
    "text": "Example - Loading the data\nLet’s load the river flow data for another gauge:\n\nBath &lt;- read.csv(\"bath_river_flow.csv\", skip=19, header=FALSE,\n                  colClasses = c(\"character\",\"numeric\",\"NULL\") )\nBath &lt;- Bath %&gt;% rename( Date = V1, RiverFlow = V2 ) %&gt;%\n  mutate( Date = as_date( Date, format=\"%Y-%m-%d\" ) )\nglimpse( Bath )\n\nRows: 17,196\nColumns: 2\n$ Date      &lt;date&gt; 1976-09-01, 1976-09-02, 1976-09-03, 1976-09-04, 1976-09-05,…\n$ RiverFlow &lt;dbl&gt; 3.39, 2.83, 2.97, 2.81, 2.90, 2.81, 2.59, 3.11, 2.78, 2.46, …"
  },
  {
    "objectID": "slides/week_2/combining_data_frames.html#example---comparing-the-data-sets",
    "href": "slides/week_2/combining_data_frames.html#example---comparing-the-data-sets",
    "title": "MA22019 - Combining data frames",
    "section": "Example - Comparing the data sets",
    "text": "Example - Comparing the data sets\nLet’s check the first elements of each data set\n\nslice_head( Bath, n=2 )\n\n        Date RiverFlow\n1 1976-09-01      3.39\n2 1976-09-02      2.83\n\nslice_head( Bathford, n=2 )\n\n        Date RiverFlow\n1 1969-10-27     3.998\n2 1969-10-28     3.958\n\n\nWhat do we notice?"
  },
  {
    "objectID": "slides/week_2/combining_data_frames.html#example---merging-the-data-sets",
    "href": "slides/week_2/combining_data_frames.html#example---merging-the-data-sets",
    "title": "MA22019 - Combining data frames",
    "section": "Example - Merging the data sets",
    "text": "Example - Merging the data sets\nLet’s merge observations based on Date\n\nRF &lt;- full_join( Bathford, Bath, by=c(\"Date\" = \"Date\") )\nslice_head(RF, n=1)\n\n        Date RiverFlow.x RiverFlow.y\n1 1969-10-27       3.998          NA\n\n\nWe see that the variable names need updating:\n\nRF &lt;- rename( RF, Bathford = RiverFlow.x, Bath = RiverFlow.y )"
  },
  {
    "objectID": "slides/week_2/combining_data_frames.html#example---data-analysis",
    "href": "slides/week_2/combining_data_frames.html#example---data-analysis",
    "title": "MA22019 - Combining data frames",
    "section": "Example - Data analysis",
    "text": "Example - Data analysis\nWe explore dependence of the river flow levels"
  },
  {
    "objectID": "slides/week_2/combining_data_frames.html#comparison-with-inner_join",
    "href": "slides/week_2/combining_data_frames.html#comparison-with-inner_join",
    "title": "MA22019 - Combining data frames",
    "section": "Comparison with inner_join()",
    "text": "Comparison with inner_join()\nWhile full_join() keeps all observation, inner_join() only keeps the dates listed in both data sets\n\nRF &lt;- inner_join( Bathford, Bath, by=c(\"Date\" = \"Date\") )\nslice_head(RF, n=3)\n\n        Date RiverFlow.x RiverFlow.y\n1 1976-09-01       2.811        3.39\n2 1976-09-02       2.560        2.83\n3 1976-09-03       2.337        2.97"
  },
  {
    "objectID": "slides/week_2/combining_data_frames.html#remarks",
    "href": "slides/week_2/combining_data_frames.html#remarks",
    "title": "MA22019 - Combining data frames",
    "section": "Remarks",
    "text": "Remarks\n\nleft_join() and right_join()\nCombining a large number of data sets  -&gt; for loop (Section 1.3.2)\nOrder of elements after merging\n\n\nRF &lt;- full_join( Bath, Bathford, by=c(\"Date\" = \"Date\") )\nslice_head( RF, n=1 )\n\n        Date RiverFlow.x RiverFlow.y\n1 1976-09-01        3.39       2.811"
  },
  {
    "objectID": "slides/week_2/combining_data_frames.html#key-messages",
    "href": "slides/week_2/combining_data_frames.html#key-messages",
    "title": "MA22019 - Combining data frames",
    "section": "Key Messages",
    "text": "Key Messages\nWe have now finished the chapter on Data Wrangling. The take-home messages are\n\nEnsure that variables are of the correct type and use informative variable names\nUse the dplyr package to structure and manipulate your data\nThe restructuring process should be determined by the research question\nCheck for missing data and report the proportion if data are missing"
  },
  {
    "objectID": "slides/week_10/revision_class.html",
    "href": "slides/week_10/revision_class.html",
    "title": "MA22019 - Revision Class",
    "section": "",
    "text": "Coursework 2 will be released on Friday 25 April at 9:00am, and the deadline is Tuesday 6 May at 12:00pm (noon).\nThis is our final lecture - there will be a 10:00-12:30 office hour instead of a lecture next week.\nTutorials in Week 11 will be there for you to work on the coursework and to ask questions.\nQuestions can be posted to the Padlet board - please don’t expect answers to questions posted after 17:00 on Friday 2 May.\nNo 9:00-11:00 office hour on Monday 28 April."
  },
  {
    "objectID": "slides/week_10/revision_class.html#important-information",
    "href": "slides/week_10/revision_class.html#important-information",
    "title": "MA22019 - Revision Class",
    "section": "",
    "text": "Coursework 2 will be released on Friday 25 April at 9:00am, and the deadline is Tuesday 6 May at 12:00pm (noon).\nThis is our final lecture - there will be a 10:00-12:30 office hour instead of a lecture next week.\nTutorials in Week 11 will be there for you to work on the coursework and to ask questions.\nQuestions can be posted to the Padlet board - please don’t expect answers to questions posted after 17:00 on Friday 2 May.\nNo 9:00-11:00 office hour on Monday 28 April."
  },
  {
    "objectID": "slides/week_10/revision_class.html#plan-for-today",
    "href": "slides/week_10/revision_class.html#plan-for-today",
    "title": "MA22019 - Revision Class",
    "section": "Plan for Today",
    "text": "Plan for Today\nThe most requested topics (based on 3 responses) were\n\nTopic modelling / LDA\nSemi-variogram\n\nWe will spend about 20 minutes on each of the two topics."
  },
  {
    "objectID": "slides/week_10/revision_class.html#setup",
    "href": "slides/week_10/revision_class.html#setup",
    "title": "MA22019 - Revision Class",
    "section": "Setup",
    "text": "Setup\nWe have \\(N\\) documents and \\(M\\) unique words.\nOur aim is to structure the text data into \\(K\\) topics with:\n\nEach document can feature multiple topics (with varying proportions)\nEach word can feature in multiple topics (with varying proportions)\n\nWe introduced LDA to estimate:\n\nThe topics;\nThe proportions with which topics feature in each document."
  },
  {
    "objectID": "slides/week_10/revision_class.html#lda-as-a-generative-model",
    "href": "slides/week_10/revision_class.html#lda-as-a-generative-model",
    "title": "MA22019 - Revision Class",
    "section": "LDA as a generative model",
    "text": "LDA as a generative model\nLet’s consider from a different angle.\nGeneration of new documents:\nSTEP 1: Sample the proportion with which each topic features.\nSTEP 2: Sample the words."
  },
  {
    "objectID": "slides/week_10/revision_class.html#what-to-remember",
    "href": "slides/week_10/revision_class.html#what-to-remember",
    "title": "MA22019 - Revision Class",
    "section": "What to remember",
    "text": "What to remember\n\nTopic modelling can be a powerful tool to analyse, structure and classify documents.\nThe topics produced by LDA may not be what we expect  -&gt; hard to interpret our findings\nStop words should be removed\n\nLet’s have a look at the example in Section 3.4.4, which we did not consider in Week 6."
  },
  {
    "objectID": "slides/week_10/revision_class.html#definition-and-assumption-1",
    "href": "slides/week_10/revision_class.html#definition-and-assumption-1",
    "title": "MA22019 - Revision Class",
    "section": "Definition and Assumption 1",
    "text": "Definition and Assumption 1\nWe explore the dependence between sites \\(\\mathbf{s}\\) and \\(\\tilde{\\mathbf{s}}\\) using the semi-variogram [ (,) = ] under the assumption that \\(X(\\mathbf{s})\\) and \\(X(\\tilde{\\mathbf{s}})\\) have the same mean (constant mean).\nHow do we check the constant mean assumption?\n-&gt; We have to discuss the assumption in the context of the data and application."
  },
  {
    "objectID": "slides/week_10/revision_class.html#estimation-and-assumption-2",
    "href": "slides/week_10/revision_class.html#estimation-and-assumption-2",
    "title": "MA22019 - Revision Class",
    "section": "Estimation and Assumption 2",
    "text": "Estimation and Assumption 2\nWe assume that \\(\\gamma(\\mathbf{s},\\tilde{\\mathbf{s}})\\) is fully defined by the distance between \\(\\mathbf{s}\\) and \\(\\tilde{\\mathbf{s}}\\).\nHow do we check this assumption?\n-&gt; Differences in levels should be similar for all areas and directions.\nSo we can define a function \\(\\tilde{\\gamma}:\\mathbb{R}_+\\to\\mathbb{R}_+\\) with [ ( , ) = (||-||), ]\nThe function \\(\\tilde{\\gamma}()\\) is estimated from the data."
  },
  {
    "objectID": "slides/week_10/revision_class.html#what-to-look-out-for",
    "href": "slides/week_10/revision_class.html#what-to-look-out-for",
    "title": "MA22019 - Revision Class",
    "section": "What to look out for?",
    "text": "What to look out for?\n\nAre the assumptions sensible? -&gt; Visualize the data and, if possible, discuss the assumptions in context.\nDoes the estimate increase with distance? -&gt; spatial dependence weakens with increasing distance\nIs there a point at which the estimate is close to flat? -&gt; Marks the distance at which outcomes are close to independent\nDid we set suitable values for the width and cutoff? -&gt; Check number of points per estimate.\n\nLet’s look at one more example: air pollution across Manchester."
  },
  {
    "objectID": "slides/week_10/analysis_of_nyt_articles.html",
    "href": "slides/week_10/analysis_of_nyt_articles.html",
    "title": "Analysis of NYT Articles",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(tidytext)\nlibrary(tidyr)\nlibrary(topicmodels)\n\nWe work with 1289 articles published by the New York Times in their “Europe” subsection between 01/01/2023 and 01/11/2024:\n\nNYT &lt;- read.csv(\"nyt.csv\" )\n\nSet identifier for each article:\n\nNYT &lt;- NYT %&gt;% mutate( ID=1:nrow(NYT) )"
  },
  {
    "objectID": "slides/week_10/analysis_of_nyt_articles.html#loading-the-packages-and-data",
    "href": "slides/week_10/analysis_of_nyt_articles.html#loading-the-packages-and-data",
    "title": "Analysis of NYT Articles",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(tidytext)\nlibrary(tidyr)\nlibrary(topicmodels)\n\nWe work with 1289 articles published by the New York Times in their “Europe” subsection between 01/01/2023 and 01/11/2024:\n\nNYT &lt;- read.csv(\"nyt.csv\" )\n\nSet identifier for each article:\n\nNYT &lt;- NYT %&gt;% mutate( ID=1:nrow(NYT) )"
  },
  {
    "objectID": "slides/week_10/analysis_of_nyt_articles.html#constructing-the-document-term-matrix",
    "href": "slides/week_10/analysis_of_nyt_articles.html#constructing-the-document-term-matrix",
    "title": "Analysis of NYT Articles",
    "section": "Constructing the Document Term Matrix",
    "text": "Constructing the Document Term Matrix\n\nNYT_count &lt;- NYT %&gt;%\n  unnest_tokens( word, lead_paragraph ) %&gt;%\n  anti_join( stop_words, by=\"word\" ) %&gt;%\n  count( ID, word, sort = TRUE )\n\n\nNYT_dtm &lt;- NYT_count %&gt;% cast_dtm( ID, word, n )"
  },
  {
    "objectID": "slides/week_10/analysis_of_nyt_articles.html#estimate-the-latent-dirichlet-allocation-model",
    "href": "slides/week_10/analysis_of_nyt_articles.html#estimate-the-latent-dirichlet-allocation-model",
    "title": "Analysis of NYT Articles",
    "section": "Estimate the Latent Dirichlet Allocation Model",
    "text": "Estimate the Latent Dirichlet Allocation Model\n\nNYT_LDA &lt;- LDA( NYT_dtm, k = 2, method=\"Gibbs\", control = list(seed=2024) )"
  },
  {
    "objectID": "slides/week_10/analysis_of_nyt_articles.html#analysis-of-the-results",
    "href": "slides/week_10/analysis_of_nyt_articles.html#analysis-of-the-results",
    "title": "Analysis of NYT Articles",
    "section": "Analysis of the results",
    "text": "Analysis of the results\nMake up of articles\n\ntidy( NYT_LDA , matrix = \"gamma\" ) %&gt;%\n  filter( topic==1 ) %&gt;%\n  ggplot( aes( y=gamma ) ) + theme_bw() + \n  geom_boxplot() + labs( y=\"Proportion\" )\n\n\n\n\n\n\n\nMake up of topics\n\ntidy( NYT_LDA , matrix = \"beta\" ) %&gt;%\n  mutate( topic = case_when( topic==1 ~ \"Topic1\", topic==2 ~ \"Topic2\") ) %&gt;%\n  pivot_wider( names_from = topic, values_from = beta, values_fill = 0 ) %&gt;%\n  ggplot( aes(x=Topic1, y=Topic2) ) +  coord_trans( x=\"sqrt\", y=\"sqrt\" ) + \n  geom_point() + theme_bw() +\n  geom_text( aes(label=term), check_overlap = TRUE, vjust=1 ) + \n  labs( x=\"Term Frequency in Topic 1\", y=\"Term Frequency in Topic 2\" )\n\nWarning: `coord_trans()` was deprecated in ggplot2 4.0.0.\nℹ Please use `coord_transform()` instead."
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "Labs",
    "section": "",
    "text": "Weekly practice labs.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nSubtitle\n\n\n\n\n\n\n\n\nMA22019 2025 - Computer Lab 1\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Computer Lab 5\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Computer Lab 6\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Computer Lab 7\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Computer Lab 8\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Computer Lab 9\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 -Computer Lab 2\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 -Computer Lab 3\n\n\nMise en place\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "lecture_notes/01-DataWrangling.html",
    "href": "lecture_notes/01-DataWrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "In practice, raw data (measurements, etc.) may be provided in a format that hampers its analysis. Common problems include:\n\nData types are incompatible with available R functions - data may, for instance, be imported as a set of characters, but we require numerical values for most R functions;\nVariable names are uninformative - we do not want to always go back and look up what the individual variables represent;\nWe are only interested in a subset of the data - a study may have produced a lot of data, but we want to focus on a specific aspect.\n\nData wrangling is concerned with restructuring the raw data into a format more useful for the analysis. We also want to extract key information from the data, i.e, performing some data exploration.\nIn this chapter we will explore how we can perform such tasks in R. Most of the functions we will use are provided by the dplyr R package (and other R packages where appropriate). Let’s load the dplyr package,\n\nlibrary( dplyr )\n\nWhen we import data into R, the data will usually be stored as a data frame, which corresponds to a matrix, where each column has a name. The dplyr package is particularly effective at working with this data format. To access the individual columns for the different variables in the data frame, we can use the $ sign, followed by the name of the column (you will see this syntax at multiple points).\n\nAfter loading the data, the first step is to check that variables have the correct data type and decide whether the variable names are suitable/informative. If the data type is incorrect, we should convert it, in particular if we want to apply R functions to the data. The most common conversion we will have to make is from the type character to the type numeric or date. In this section, we highlight some of the available R functions for converting and renaming variables, and illustrate their application using publicly available river flow data.\n\nWhen loading the data for a numerical variable into R, the individual values may be stored as strings/words. Possible reasons include, for instance, that the value 2345.34 is stored as “2,345.34” in the data file (see Problem Class 1) or that missing values are represented via a letter - once R fails to convert a single entry to a numerical value, the whole column (variable) is converted to type character. However, this data type is often not useful because only a few functions can work with it.\nWe will introduce the functions as.numeric() and case_when() that may be used to convert a character to a numerical value.\nExample 1: Suppose we work with a data set where the letter “M” is used to indicate that an observation is missing. A toy example is provided in the data file “DataCleaningExample1.csv”. We load this data and use the function glimpse() in the dplyr package to print the data type and values:\n\nExample1 &lt;- read.csv(\"data/datacleaningexample1.csv\" )\nglimpse( Example1 )\n\nRows: 8\nColumns: 1\n$ Value &lt;chr&gt; \"1.02\", \"0.98\", \"0.79\", \"M\", \"2.1\", \"15.1\", \"M\", \"4.2\"\n\n\nWe find that the values are stored as characters/words, as indicated by the data type being \\(\\mathrm{\\texttt{&lt;chr&gt;}}\\). In such a situation, we cannot use the mean() function to derive the average value - there is no average of a set of words:\n\nmean( Example1$Value )\n\nWarning in mean.default(Example1$Value): argument is not numeric or logical:\nreturning NA\n\n\n[1] NA\n\n\nInstead, we have to first use the function as.numeric() to convert the words to numerical values\n\nExample1$Value &lt;- as.numeric( Example1$Value )\n\nWarning: NAs introduced by coercion\n\nglimpse( Example1 )\n\nRows: 8\nColumns: 1\n$ Value &lt;dbl&gt; 1.02, 0.98, 0.79, NA, 2.10, 15.10, NA, 4.20\n\n\nWe see that the data type has changed to \\(\\mathrm{\\texttt{&lt;dbl&gt;}}\\), which is one data type for numerical values. Further, the R output shows that all entries with the letter “M” were converted to \\(\\mathrm{\\texttt{NA}}\\) (not available) - this is R’s way to tell us that the conversion did not work or that a value is missing (which is exactly what we want here).\nWith the values converted to the correct type, we can now calculate their mean:\n\nmean( Example1$Value, na.rm=TRUE ) # na.rm=TRUE to ignore the entries with NA\n\n[1] 4.031667\n\n\nExample 2: Suppose responses to a survey question were encoded as “Y” (“Yes”) or “N” (“No”) and we received the following data vector:\n\nresponses &lt;- c(\"Y\",\"Y\",\"N\",\"Y\",\"N\",\"Y\",\"N\",\"N\",\"N\",\"Y\")\n\nAgain, we cannot use \\(\\mathrm{\\texttt{mean(responses)}}\\) to derive the proportion of participants who answered with “Yes”, because the mean() function requires numerical or logical values.\nOne common approach to derive the proportion in practice is to encode the outcomes as numerical values to which the mean() function is then applied. The dplyr package provides the function case_when() which allows us to replace “Y” and “N” by 1 and 0 respectively:\n\nresponses &lt;- case_when( responses == \"Y\" ~ 1, responses == \"N\" ~ 0 )\nmean( responses )\n\n[1] 0.5\n\n\nWe find that 50% of the participants answered the question with “Yes”. One strength of case_when() is that we can define as many cases as we need, there is no limit. The function can also be used to convert other types of data, and it is not limited to converting a character into a numerical value.\nRemark 1: If you forget to specify a case in case_when(), the converted value for any unspecified case will be \\(\\mathrm{\\texttt{NA}}\\) by default. The default option can be changed and for our our example we could have also used\n\nresponses &lt;- case_when( responses == \"Y\" ~ 1, .default = 0 )\n\nRemark 2: Be aware that case_when() considers the expressions sequentially (just as when you are using if, else if and else statements). The following pieces of code show an example where the result depends on the order of the conditions:\n\nx &lt;- c( 10, 20, 40 )\ncase_when( x %% 10 == 0 ~ 1, x %% 20 == 0 ~ 2 )\n\n[1] 1 1 1\n\n\nWe see that all converted values are all equal to 1. Let’s see what happens when we change the order of the conditions:\n\ncase_when( x %% 20 == 0 ~ 2, x %% 10 == 0 ~ 1 )\n\n[1] 1 2 2\n\n\nThis second result seems more intuitive. Consequently, we should proceed from the most specific to the most general condition when using case_when().\n\nIn many studies we are provided with the time the data were observed. This information is often important in applications and we cannot simply ignore it. When loading variables representing dates into R, their values are often stored as strings, such as “01/10/2022”.\nThe R package lubridate provides a range of nice functions to convert data of type character into the data type date or date-time. For instance, to convert the character expressions “01/10/2022” and “15/10/2023”, we use the as_date() function,\n\nlibrary(lubridate)\ndate_observed  &lt;- c( \"01/10/2022\", \"15/10/2023\" )\ndate_converted &lt;- as_date( date_observed, format=\"%d/%m/%Y\" )\nglimpse( date_converted )\n\n Date[1:2], format: \"2022-10-01\" \"2023-10-15\"\n\n\nWe see that the default output format for dates is year-month-day.\nRemark 1: After converting values to date, we can extract the year and month using the functions year() and month() respectively. Let’s extract the year from the dates in date_converted:\n\nyear( date_converted )\n\n[1] 2022 2023\n\n\nRemark 2: We can also calculate the difference between dates. For instance, if we consider the two entries in the vector of converted dates, we find\n\ndate_converted[2] - date_converted[1]\n\nTime difference of 379 days\n\n\nSo you can now use R to quickly calculate how many days there are left until the Easter break.\n\nWe should avoid using uninformative (or very long) variable names. Let’s generate a data frame with two columns, where each column contains five samples from a standard normal distribution\n\nset.seed( 2025 )\nobs &lt;- data.frame( \"x\"=rnorm(5, mean=0, sd=1), \"y\"=rnorm(5, mean=0, sd=1) )\nobs\n\n          x           y\n1 0.6207567 -0.16285434\n2 0.0356414  0.39711189\n3 0.7731545 -0.07998932\n4 1.2724891 -0.34496518\n5 0.3709754  0.70215136\n\n\nWe may argue that the variable names x and y are uninformative and should be changed to Sample1 and Sample2 respectively. The function rename() in the dplyr R package allows us to do this:\n\nobs &lt;- rename( obs, \"Sample1\"=x, \"Sample2\"=y )\nobs\n\n    Sample1     Sample2\n1 0.6207567 -0.16285434\n2 0.0356414  0.39711189\n3 0.7731545 -0.07998932\n4 1.2724891 -0.34496518\n5 0.3709754  0.70215136\n\n\n\nThe National River Flow Archive (www.nrfa.ceh.ac.uk) provides data for hundreds of sites (gauges) across the UK. We want to analyze daily river flow data for the River Avon at Bathford. The data are available in the file “Bathford River Flow.csv”.\nWhen looking at the data file, we identify two aspects that need to be taken into account when loading the data into R\n\nThe first 20 lines are data descriptors (so called meta data), while the remaining lines contain the actual data: dates and river flow measurements.\nThe letter “M” appears in the third column whenever the river flow measurement is missing in later years.\n\nTo ignore the first 20 lines and avoid importing the data file in a wrong format, we have to use three of the options provided by the read.csv() function:\n\nBathford_RF &lt;- read.csv(\"data/bathford_river_flow.csv\", skip=20, header=FALSE,\n                         colClasses = c(\"character\",\"numeric\",\"NULL\") ) \n\nWarning in read.table(file = file, header = header, sep = sep, quote = quote, :\ncols = 2 != length(data) = 3\n\n\nThe option \\(\\mathrm{\\texttt{skip=20}}\\) means we ignore the first 20 lines, while \\(\\mathrm{\\texttt{colClasses= c(\"character\",\"numeric\",\"NULL\")}}\\) leads to the third column being ignored when loading the data (you can ignore the warning message in this case). Finally, we set \\(\\mathrm{\\texttt{header=FALSE}}\\), because the file does not provide variable names.\nTip: Have a look at the data set in the data file before trying to load it. R (in particular recent versions) may load the data into an incorrect format instead of giving an error. As an example, remove the option \\(\\mathrm{\\texttt{colClasses=..}}\\). You will find that the number of observations increases, but some of the dates are now listed as “M”.\nLet’s have a look at the imported data using the glimpse() function,\n\nglimpse( Bathford_RF )\n\nRows: 19,697\nColumns: 2\n$ V1 &lt;chr&gt; \"1969-10-27\", \"1969-10-28\", \"1969-10-29\", \"1969-10-30\", \"1969-10-31~\n$ V2 &lt;dbl&gt; 3.998, 3.958, 4.210, 4.480, 4.205, 3.830, 3.723, 3.986, 4.353, 4.52~\n\n\nWe see that there are 19,697 measurements in the data set. However, the variable names have to be changed - V1 and V2 are just not sensible. Further, we have to convert the dates into the date format. The river flow measurements are already stored as numeric values, so no conversion is required.\nLet’s start by changing the variable names to Date and RiverFlow\n\nBathford_RF &lt;- rename( Bathford_RF, Date = V1, RiverFlow = V2 )\n\nbefore converting the variable Date to the correct type,\n\nBathford_RF$Date &lt;- as_date( Bathford_RF$Date, format=\"%Y-%m-%d\" )\n\nWith the data having been cleaned, i.e., they have the correct type and informative names, we can start the analysis. As a first step, it is good practice to report the proportion of missing data of a variable. We can extract the proportion of missing river flow measurements using the functions mean() and is.na()\n\nmean( is.na( Bathford_RF$RiverFlow ) )\n\n[1] 0.006244606\n\n\nWe find that river flow measurements are missing on about 0.62% of dates and this should be reported.\nRemark: Missing data is important when building models. In this course, you are only expected to state the proportion of missing data. The handling of missing data will be considered in more detail in the Year 3 unit MA32022 Statistical Modelling and Data Analytics 3A.\nNow that the data frame is in a much better format, we can plot it using the function plot() covered in Year 1 Probability & Statistics:\n\nplot( Bathford_RF$Date, Bathford_RF$RiverFlow, type='l',\n      xlab=\"Date\", ylab=\"River Flow\", cex.lab = 1.5 )\n\n\n\nRiver flow measurements at Bathford for 27 October 1969 to 30 September 2023.\n\n\n\nWhich of the following conclusions should we report when asked to comment on the frequency of river flow levels above 100m\\(^3\\)/s and the magnitude of river flow levels?\n\nRecorded river flow levels were as high as approximately 250m\\(^3\\)/s.\nThe data exhibits seasonality, with river flow levels being higher in winter than in summer.\nThe data covers the years 1969 to 2023.\nThere is at least one day with river flow levels exceeding 100m\\(^3\\)/s for most years.\n\nWe now study a range of aspects that frequently come up during the data wrangling process:\n\nSelecting subsets of observations and variables:\n\nWhen working with a large data set, not all variables and observations may be relevant. So we may want to reduce the size of our data set and only keep the observations required for our analysis. This process can happen before or after the data cleaning.\n\nDeriving new variables from existing data:\n\nIt may be useful to create new variables which we believe to be interesting to explore in our analysis. These new variables should be stored in the same data frame as the other variables.\n\nSummarizing the data:\n\nAs one of the first steps in the data exploration, we should derive summaries of the different variables to gain a better understanding of the data. For instance, as highlighted in Section 1.1.4, the proportion of missing data is one useful summary.\n\nSorting the data:\n\nIn applications, interest may lie in extracting the smallest/largest observations and/or providing a ranking. As such, we need to be able to sort observations based on one, or more, criteria.\n\n\nIn this section we explore how to perform these operations using the dplyr package. In Problem Class 1 we will use the considered techniques to analyze a relatively large data set from Brazil. Other aspects of data exploration will be discussed in the next chapters.\nTip: All steps of the data wrangling / exploration process should be placed in an R (or R Markdown) script, so that we can make modifications quickly if something needs to be changed. It’s also good practice to keep the raw data available in your R Workspace. In the following examples we never replace the raw data.\n\nIn an analysis we may only want to focus on a subset of the data. For instance, when modelling the risk of flooding, we are mostly interested in the extremely high river flow measurements.\nThe function filter() is useful in such cases. Suppose we classified a river flow exceeding 100 m\\(^3\\)/s at the gauge of Bathford in Section 1.1.4 as extremely high. We can then extract the subset of observations exceeding 100m\\(^3\\)/s using filter(), and we use slice_head() to print the first five observations:\n\nBathford_RF_High &lt;- filter( Bathford_RF, RiverFlow &gt; 100 )\nslice_head( Bathford_RF_High, n=5 )\n\n        Date RiverFlow\n1 1970-11-19     104.3\n2 1971-01-21     128.4\n3 1971-01-22     114.8\n4 1971-01-23     115.2\n5 1971-01-24     133.1\n\n\nThe function filter() can also handle multiple conditions. For instance, we can extract the days across the period 1991-2023 when the river flow exceeded 100m\\(^3\\)/s using\n\nBathford_RF_High &lt;- filter( Bathford_RF, RiverFlow &gt; 100, year(Date) &gt; 1990 )\nslice_head( Bathford_RF_High, n=5 )\n\n        Date RiverFlow\n1 1991-01-10     112.1\n2 1992-11-26     106.9\n3 1992-11-27     100.9\n4 1992-11-28     100.6\n5 1992-11-29     131.6\n\n\n\nNot all variables in a data set may be of interest to us. For instance, meteorological data sets often provide measurements for multiple weather variables, but we may only need to analyze precipitation and temperature.\nExample: Let’s consider the data set “Tuscany.csv” which provides information on the population in Tuscany, Italy, for 2020:\n\nTuscany_raw &lt;- read.csv(\"data/tuscany.csv\" )\nslice_head( Tuscany_raw, n=5 )\n\n  Year Postal_Code  Town Province Age Men Women\n1 2020       45001 Aulla       MS   0  32    30\n2 2020       45001 Aulla       MS   1  30    34\n3 2020       45001 Aulla       MS   2  43    39\n4 2020       45001 Aulla       MS   3  50    35\n5 2020       45001 Aulla       MS   4  38    29\n\n\nSuppose we only want to compare the population data for the different provinces and towns. As such, we don’t need the variables Year, since all data are from 2020, and Postal_Code. Let’s look at two possible options to achieve this using the select() function in dplyr.\nThe first option is to specify the variables we want to keep\n\nTuscany &lt;- select( Tuscany_raw, Town:Women )\nslice_head( Tuscany, n=5 )\n\n   Town Province Age Men Women\n1 Aulla       MS   0  32    30\n2 Aulla       MS   1  30    34\n3 Aulla       MS   2  43    39\n4 Aulla       MS   3  50    35\n5 Aulla       MS   4  38    29\n\n\nHere, the colon sign indicates that we want to keep all columns from Town to Women.\nThe second option is to specify the variables to be excluded using the minus sign,\n\nTuscany &lt;- select( Tuscany_raw, -Year, -Postal_Code )\n\nWhether we specify the variables to be kept, or the variables to be removed, really depends on the number of variables to be included (or excluded) - we want to write as little code as possible.\n\nWhen analyzing real-world data, it may be useful to create new variables which we believe to be interesting to explore. For instance, for the population from Tuscany, we may want to calculate the total population for each age group and town, and attach this information as a new variable to the data frame.\nThe mutate() function in the dplyr package is really useful in such situations, as we can produce and directly attach a new variable Population using\n\nTuscany &lt;- mutate( Tuscany, Population = Men + Women )\nslice_head( Tuscany, n=5 )\n\n   Town Province Age Men Women Population\n1 Aulla       MS   0  32    30         62\n2 Aulla       MS   1  30    34         64\n3 Aulla       MS   2  43    39         82\n4 Aulla       MS   3  50    35         85\n5 Aulla       MS   4  38    29         67\n\n\nWe see that mutate() requires us to provide a new variable name and to define how the values of this new variable are to be derived. Note, the function mutate() can also be used to attach values stored in another R object to the data frame.\nImportant: If you use a variable name that already exists within the data frame, mutate() will overwrite this column with the new values - so we can also use mutate() to modify the columns in your data frame.\n\nWe have already introduced quite a few useful functions for data cleaning and wrangling. Let’s now consider the case that we want to combine these functions. For instance, we may want to derive the population per age group and town, and then remove the variables Year and Postal_Code from the original data frame.\nHow can we do this?\nThe first option is to manipulate the data step by step and to always store the R object after finishing one operation (similar to what we have done so far). This would be implemented as\n\nTuscany &lt;- mutate( Tuscany_raw, Population = Men + Women )\nTuscany &lt;- select( Tuscany, -Year, -Postal_Code )\n\nThis is quite a bit of code, because we have to type \\(\\mathrm{\\texttt{Tuscany}}\\) in each line.\nCan we do better?\nWell, we could place all the operations into a single line\n\nTuscany &lt;- select( mutate( Tuscany_raw, Population = Men + Women ), Town:Population )\n\nHowever, such an approach may quickly lead to a large number of brackets, which increases the risk of frustrating syntax errors - remember this may only be the start of our analysis.\nLuckily, we can avoid both these two options by using the pipe command %&gt;% in the dplyr R package. The same commands as above would be implemented as\n\nTuscany &lt;- Tuscany_raw %&gt;%\n  mutate( Population = Men + Women ) %&gt;%\n  select( -Year, -Postal_Code )\nslice_head( Tuscany, n=5 )\n\n   Town Province Age Men Women Population\n1 Aulla       MS   0  32    30         62\n2 Aulla       MS   1  30    34         64\n3 Aulla       MS   2  43    39         82\n4 Aulla       MS   3  50    35         85\n5 Aulla       MS   4  38    29         67\n\n\nThe operations are executed from top to bottom: We take the data frame Tuscany_raw, then apply the mutate() function to create the column Population, and conclude by removing the columns Year and Postal_Code from the created data frame using the select() function.\nTip: Combining multiple R commands can be tricky at first. If you are unsure, try to outline the way you want to manipulate the data before starting to implement it in R.\n\nFor large data sets, we usually want to provide data summaries. For instance, one important summary for the Tuscany data set may be the total number of people within the data. In such situations, we can apply functions such as sum() directly\n\nsum( Tuscany$Population )\n\n[1] 3691409\n\n\nIf we want to extract several such summaries, we can either derive each summary individually, or use the summarize() function in the dplyr R package. Let’s also extract the proportion of men and women\n\nTuscany %&gt;%\n  summarize( \"Population_Tuscany_2020\" = sum( Population ),\n             \"Men_Tuscany_2020\" = sum( Men ),\n             \"Women_Tuscany_2020\" = sum( Women ) )\n\n  Population_Tuscany_2020 Men_Tuscany_2020 Women_Tuscany_2020\n1                 3691409          1787649            1903760\n\n\nThe summarize() function really starts to shine when we combine it with the group_by() function.\nSuppose we wanted the population numbers for each of the provinces, which requires us to sum up the numbers across towns and age groups while accounting for the variable Province. We can do this using group_by() and summarize():\n\nTuscany_Province &lt;- Tuscany %&gt;%\n  group_by( Province ) %&gt;%\n  summarize( Total = sum(Population) )\nTuscany_Province\n\n# A tibble: 10 x 2\n   Province  Total\n   &lt;chr&gt;     &lt;int&gt;\n 1 AR       336450\n 2 FI       997940\n 3 GR       217803\n 4 LI       328855\n 5 LU       383688\n 6 MS       189786\n 7 PI       417799\n 8 PO       265153\n 9 PT       290177\n10 SI       263758\n\n\nThe group_by() function splits the data subject to the specified variable (Province in this case) and, for each subset, the summarize() function then derives the population total.\nRemark: We can specify multiple variables in group_by() to define the subgroups based on several criteria.\nLet’s consider a slightly more complicated task. Suppose we were asked to study the age profile of women within the population. To extract the proportion of women of a certain age, we need to group women by Age, but also keep track of the total number of women within the population. One possible way to extract the proportions is as follows:\n\nTuscany_Women_Age &lt;- Tuscany %&gt;%\n  group_by( Age ) %&gt;%\n  summarize( Number = sum(Women) ) %&gt;%\n  mutate( Proportion = Number / sum(Number) )\n\nNote, we used the fact that the summarize() function returns a data frame, and thus we can perform further operations. Finally, let’s illustrate the calculated proportions using a bar plot:\n\nbarplot( Proportion~Age, data=Tuscany_Women_Age )\n\n\n\nAge profile of living women in Tuscany for the year 2020.\n\n\n\nWe see that the highest proportions are observed for ages 40-70. The lower proportions for younger ages reflect the decrease in birth rates recorded for many countries over the past years. The decreasing proportion beyond 70 is presumably due to an increased rate of mortality for these age groups.\n\nYou may have already seen the sort() command, which allows you to order the values within a vector. When we consider a data frame, we may want to sort its rows subject to the values in one of the columns. For instance, we may want to sort provinces based on their population.\nThe function arrange() in the dplyr R package does exactly this job,\n\narrange( Tuscany_Province, Total )\n\n# A tibble: 10 x 2\n   Province  Total\n   &lt;chr&gt;     &lt;int&gt;\n 1 MS       189786\n 2 GR       217803\n 3 SI       263758\n 4 PO       265153\n 5 PT       290177\n 6 LI       328855\n 7 AR       336450\n 8 LU       383688\n 9 PI       417799\n10 FI       997940\n\n\nWe see that “FI” (Firenze) has the highest population among the provinces in Tuscany. Further, the output demonstrates that the default setting for arrange() is to sort the values in ascending order. Should we want to sort values in descending order, we have to use the additional command desc():\n\nTuscany_Province %&gt;% arrange( desc(Total) )\n\nRemark: If two observations have the same value, they are listed in their original order, regardless of whether we sort in ascending or descending order. If we want to change this (which we sometimes want), we can specify a second variable in arrange(), just as for group_by().\n\nSo far we have focused on analyzing a single data file. In many applications, however, data is stored across multiple data files. For instance, we may have one data file containing weather data and another data file providing insurance data related to weather-related damages. In these cases, we want to combine the different data files into a single data frame for our analysis.\nThe dplyr R package provides the functions inner_join(), left_joint(), right_join() and full_join() to combine data frames based on a “key”. All these functions combine two data frames and their application is illustrated via an example in Section 1.3.1.\nWhen working with multiple data sets, we may also want to automate the process. Imagine you had weather measurements for over 100 sites - you do not really want to spend hours just to merge the data frames. This aspect is considered in Section 1.3.2.\n\nIn Section 1.1.4, we focused on the river flow data collected at Bathford. The National River Flow Archive provides data for another gauge located to the west of Bath city centre; you can find the data file “Bath River Flow.csv” on Moodle. Our aim is to combine the river flow measurements into a single data frame.\nWe start by again loading the data for Bathford and renaming the variables,\n\nBathford_RF &lt;- read.csv(\"data/bathford_river_flow.csv\", skip=20, header=FALSE,\n                         colClasses = c(\"character\",\"numeric\",\"NULL\") )\nBathford_RF &lt;- rename( Bathford_RF, Date = V1, RiverFlow = V2 )\n\nA closer look at data file for the Bath gauge suggests that the data format is similar to that for Bathford. The only difference is that we now have to ignore the first 19 instead of the first 20 lines:\n\nBath_RF &lt;- read.csv(\"data/bath_river_flow.csv\", skip=19, header=FALSE,\n                     colClasses = c(\"character\",\"numeric\",\"NULL\") )\nBath_RF &lt;- rename( Bath_RF, Date = V1, RiverFlow = V2 )\n\nLet’s investigate the first element in each data frame:\n\nBath_RF %&gt;% slice_head( n=1 )\n\n        Date RiverFlow\n1 1976-09-01      3.39\n\nBathford_RF %&gt;% slice_head( n=1 )\n\n        Date RiverFlow\n1 1969-10-27     3.998\n\n\nWe see that the two gauges started operating in different years - Bath in 1976 and Bathford in 1969. So the number of rows in the two data frames is different.\nWhen combining the two data frames, we want to match observations based on the variable Date, this is our “key”. Here we use the function full_join(), which ensures that all observations for Bath and Bathford are contained in the combined data set, and we specify that observations should be matched based on the variable Date,\n\nRF &lt;- Bathford_RF %&gt;% full_join( Bath_RF, by=c(\"Date\" = \"Date\") )\nglimpse( RF )\n\nRows: 19,697\nColumns: 3\n$ Date        &lt;chr&gt; \"1969-10-27\", \"1969-10-28\", \"1969-10-29\", \"1969-10-30\", \"1~\n$ RiverFlow.x &lt;dbl&gt; 3.998, 3.958, 4.210, 4.480, 4.205, 3.830, 3.723, 3.986, 4.~\n$ RiverFlow.y &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA~\n\n\nWe see that the values for the first dates are correctly identified as being missing for Bath - the gauge was not in operation at the time. We are left with changing the variable names and converting the data type of Date\n\nRF$Date &lt;- as_date( RF$Date, format=\"%Y-%m-%d\" )\nRF &lt;- rename( RF, Bathford = RiverFlow.x, Bath = RiverFlow.y )\n\nLet’s plot the observations for Bath and Bathford against each other,\n\nplot( RF$Bath, RF$Bathford, cex.lab = 1.5, pch=19,\n      xlab=\"River Flow at Bath\", ylab=\"River Flow at Bathford\" )\n\n\n\nComparison of river flow for Bath and Bathford for 1 September 1976 - 30 October 2023.\n\n\n\nWhat can we conclude from this plot?\nRemark: If we want the first element in the combined data frame to be 01/09/1976 (the date when the gauge at Bath started operations), we would use the function inner_join(),\n\nRF_1976_2020 &lt;- Bathford_RF %&gt;% inner_join( Bath_RF, by=c(\"Date\" = \"Date\") )\n\nRemark: The function inner_join() does not remove the dates after 1976 for which the observations for Bath (or Bathford) are missing, but only the days which are not listed in both files.\n\nIn practice we may work with \\(N\\) data sets of the same (or a very similar) format. For instance, we may have 20 data sets, and each data set contains the river flow measurements for a gauge in Somerset. Then, we do not want to implement a lot of code of the form in Section 1.3.1 just to combine all these data sets into a single data frame. Instead, we will use the for() loop in R.\nExample: Suppose that, in addition to the river flow measurements for Bath and Bathford, we also need to consider the observations for Compton Dando, a small village to the west of Bath not located at the River Avon. For our analysis, it may be good to combine all three data sets into a single data frame, and the following piece of code is one way to create it.\nWe start by defining the file names and the number of lines that we have to ignore when loading the data files\n\ngauges &lt;- c( \"Bath\", \"Bathford\", \"Compton Dando\" )\nlines_to_ignore &lt;- c( 19, 20, 20)\n\nThe next step is to load the data from the different files, store the data frames in a list we call RF_individual, and update the variable names.\n\n# setwd(\"Data/\") &lt;--- REMOVED: Bad practice in RMarkdown/Quarto\nRF_individual &lt;- list()\nfor( k in 1:length(gauges) ){\n  \n  ## Load the data from the .csv file\n  # Construct path relative to project root (or file location)\n  file_name &lt;- paste0( \"data/\", tolower(gsub(\" \", \"_\", gauges[k])), \"_river_flow.csv\" ) \n  RF_individual[[k]] &lt;- read.csv( file_name, skip=lines_to_ignore[k], header=FALSE,\n                                  colClasses = c(\"character\",\"numeric\",\"NULL\") )\n  \n  ## Change the variable names\n  names( RF_individual[[k]] ) &lt;- c( \"Date\", gauges[k] )\n  \n}\n\nThe code above includes two functions you may not have used so far and so we briefly describe them:\n\npaste() is used to append “River Flow.csv” to the name of the gauge to get the file name.\nnames() is used to rename the variable names. In this case, this function was easier to use than rename(); the latter does not like to be given names from a vector.\n\nNow we are ready to merge the different data frames by repeatedly using the function full_join():\n\nRF &lt;- RF_individual[[1]]\nfor( k in 2:length(gauges) )\n  RF &lt;- RF %&gt;% full_join( RF_individual[[k]], by=c(\"Date\"=\"Date\") )\nglimpse( RF )\n\nRows: 23,955\nColumns: 4\n$ Date            &lt;chr&gt; \"1976-09-01\", \"1976-09-02\", \"1976-09-03\", \"1976-09-04\"~\n$ Bath            &lt;dbl&gt; 3.39, 2.83, 2.97, 2.81, 2.90, 2.81, 2.59, 3.11, 2.78, ~\n$ Bathford        &lt;dbl&gt; 2.811, 2.560, 2.337, 2.385, 2.146, 2.359, 2.367, 2.416~\n$ `Compton Dando` &lt;dbl&gt; 0.188, 0.173, 0.170, 0.172, 0.174, 0.174, 0.176, 0.195~\n\n\nThe final step is to convert the type of the variable Date and to sort observations by date; we know that Bathford started collecting data in 1969 but the first entry is for 1976. So we obtain the final data frame using\n\nRF &lt;- RF %&gt;% \n  mutate( Date = as_date( Date, format=\"%Y-%m-%d\" ) ) %&gt;%\n  arrange( Date )\nslice_head( RF, n=5 )\n\n        Date Bath Bathford Compton Dando\n1 1958-03-01   NA       NA          2.97\n2 1958-03-02   NA       NA          2.32\n3 1958-03-03   NA       NA          1.98\n4 1958-03-04   NA       NA          1.70\n5 1958-03-05   NA       NA          1.42\n\n\nWe can now start our analysis, for instance, by plotting the different river flows against each other:\n\npar( mfrow=c(1,3), cex.lab = 1.5 )\nplot( RF$Bath, RF$Bathford, pch=19, xlab=\"Bath\", ylab=\"Bathford\" )\nplot( RF$Bath, RF$`Compton Dando`, pch=19, xlab=\"Bath\", ylab=\"Compton Dando\" )\nplot( RF$Bathford, RF$`Compton Dando`, pch=19, xlab=\"Bathford\", ylab=\"Compton Dando\" )\n\n\n\nScatter plots of river flow measurements for each pair of gauges in the combined data set\n\n\n\nWhat do you conclude from these plots?\nRemark: If we wanted to add more gauges to the data, we only need to update the first two lines of R code in this example; the rest of the code can be left unchanged.\n\nWe have covered some of the key concepts regarding data cleaning and wrangling:\n\nEnsure that variables have the correct type and are given informative names\nUse the dplyr R package when working with a single data frame. The package allows you to create subsets, sort the data, etc.\nIn many real-world applications we have to combine multiple data sets. The dplyr R package also provides functions to achieve this.\n\nImportant: Hardly any real-world data set is “standard” - we had to use some additional functions/options for the river flow data, as well as the Airbnb data analyzed in Problem Class 1. While we introduced useful functions to perform data wrangling, we still usually have to investigate the data file “by hand” before loading the data into R. In this course we cannot possibly cover all scenarios that may occur when working with real-world data, but you can usually find a satisfying solution using Google (or other search engines).",
    "crumbs": [
      "Lecture Notes",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "lecture_notes/01-DataWrangling.html#data-cleaning",
    "href": "lecture_notes/01-DataWrangling.html#data-cleaning",
    "title": "Data Wrangling",
    "section": "",
    "text": "After loading the data, the first step is to check that variables have the correct data type and decide whether the variable names are suitable/informative. If the data type is incorrect, we should convert it, in particular if we want to apply R functions to the data. The most common conversion we will have to make is from the type character to the type numeric or date. In this section, we highlight some of the available R functions for converting and renaming variables, and illustrate their application using publicly available river flow data.\n\nWhen loading the data for a numerical variable into R, the individual values may be stored as strings/words. Possible reasons include, for instance, that the value 2345.34 is stored as “2,345.34” in the data file (see Problem Class 1) or that missing values are represented via a letter - once R fails to convert a single entry to a numerical value, the whole column (variable) is converted to type character. However, this data type is often not useful because only a few functions can work with it.\nWe will introduce the functions as.numeric() and case_when() that may be used to convert a character to a numerical value.\nExample 1: Suppose we work with a data set where the letter “M” is used to indicate that an observation is missing. A toy example is provided in the data file “DataCleaningExample1.csv”. We load this data and use the function glimpse() in the dplyr package to print the data type and values:\n\nExample1 &lt;- read.csv(\"data/datacleaningexample1.csv\" )\nglimpse( Example1 )\n\nRows: 8\nColumns: 1\n$ Value &lt;chr&gt; \"1.02\", \"0.98\", \"0.79\", \"M\", \"2.1\", \"15.1\", \"M\", \"4.2\"\n\n\nWe find that the values are stored as characters/words, as indicated by the data type being \\(\\mathrm{\\texttt{&lt;chr&gt;}}\\). In such a situation, we cannot use the mean() function to derive the average value - there is no average of a set of words:\n\nmean( Example1$Value )\n\nWarning in mean.default(Example1$Value): argument is not numeric or logical:\nreturning NA\n\n\n[1] NA\n\n\nInstead, we have to first use the function as.numeric() to convert the words to numerical values\n\nExample1$Value &lt;- as.numeric( Example1$Value )\n\nWarning: NAs introduced by coercion\n\nglimpse( Example1 )\n\nRows: 8\nColumns: 1\n$ Value &lt;dbl&gt; 1.02, 0.98, 0.79, NA, 2.10, 15.10, NA, 4.20\n\n\nWe see that the data type has changed to \\(\\mathrm{\\texttt{&lt;dbl&gt;}}\\), which is one data type for numerical values. Further, the R output shows that all entries with the letter “M” were converted to \\(\\mathrm{\\texttt{NA}}\\) (not available) - this is R’s way to tell us that the conversion did not work or that a value is missing (which is exactly what we want here).\nWith the values converted to the correct type, we can now calculate their mean:\n\nmean( Example1$Value, na.rm=TRUE ) # na.rm=TRUE to ignore the entries with NA\n\n[1] 4.031667\n\n\nExample 2: Suppose responses to a survey question were encoded as “Y” (“Yes”) or “N” (“No”) and we received the following data vector:\n\nresponses &lt;- c(\"Y\",\"Y\",\"N\",\"Y\",\"N\",\"Y\",\"N\",\"N\",\"N\",\"Y\")\n\nAgain, we cannot use \\(\\mathrm{\\texttt{mean(responses)}}\\) to derive the proportion of participants who answered with “Yes”, because the mean() function requires numerical or logical values.\nOne common approach to derive the proportion in practice is to encode the outcomes as numerical values to which the mean() function is then applied. The dplyr package provides the function case_when() which allows us to replace “Y” and “N” by 1 and 0 respectively:\n\nresponses &lt;- case_when( responses == \"Y\" ~ 1, responses == \"N\" ~ 0 )\nmean( responses )\n\n[1] 0.5\n\n\nWe find that 50% of the participants answered the question with “Yes”. One strength of case_when() is that we can define as many cases as we need, there is no limit. The function can also be used to convert other types of data, and it is not limited to converting a character into a numerical value.\nRemark 1: If you forget to specify a case in case_when(), the converted value for any unspecified case will be \\(\\mathrm{\\texttt{NA}}\\) by default. The default option can be changed and for our our example we could have also used\n\nresponses &lt;- case_when( responses == \"Y\" ~ 1, .default = 0 )\n\nRemark 2: Be aware that case_when() considers the expressions sequentially (just as when you are using if, else if and else statements). The following pieces of code show an example where the result depends on the order of the conditions:\n\nx &lt;- c( 10, 20, 40 )\ncase_when( x %% 10 == 0 ~ 1, x %% 20 == 0 ~ 2 )\n\n[1] 1 1 1\n\n\nWe see that all converted values are all equal to 1. Let’s see what happens when we change the order of the conditions:\n\ncase_when( x %% 20 == 0 ~ 2, x %% 10 == 0 ~ 1 )\n\n[1] 1 2 2\n\n\nThis second result seems more intuitive. Consequently, we should proceed from the most specific to the most general condition when using case_when().\n\nIn many studies we are provided with the time the data were observed. This information is often important in applications and we cannot simply ignore it. When loading variables representing dates into R, their values are often stored as strings, such as “01/10/2022”.\nThe R package lubridate provides a range of nice functions to convert data of type character into the data type date or date-time. For instance, to convert the character expressions “01/10/2022” and “15/10/2023”, we use the as_date() function,\n\nlibrary(lubridate)\ndate_observed  &lt;- c( \"01/10/2022\", \"15/10/2023\" )\ndate_converted &lt;- as_date( date_observed, format=\"%d/%m/%Y\" )\nglimpse( date_converted )\n\n Date[1:2], format: \"2022-10-01\" \"2023-10-15\"\n\n\nWe see that the default output format for dates is year-month-day.\nRemark 1: After converting values to date, we can extract the year and month using the functions year() and month() respectively. Let’s extract the year from the dates in date_converted:\n\nyear( date_converted )\n\n[1] 2022 2023\n\n\nRemark 2: We can also calculate the difference between dates. For instance, if we consider the two entries in the vector of converted dates, we find\n\ndate_converted[2] - date_converted[1]\n\nTime difference of 379 days\n\n\nSo you can now use R to quickly calculate how many days there are left until the Easter break.\n\nWe should avoid using uninformative (or very long) variable names. Let’s generate a data frame with two columns, where each column contains five samples from a standard normal distribution\n\nset.seed( 2025 )\nobs &lt;- data.frame( \"x\"=rnorm(5, mean=0, sd=1), \"y\"=rnorm(5, mean=0, sd=1) )\nobs\n\n          x           y\n1 0.6207567 -0.16285434\n2 0.0356414  0.39711189\n3 0.7731545 -0.07998932\n4 1.2724891 -0.34496518\n5 0.3709754  0.70215136\n\n\nWe may argue that the variable names x and y are uninformative and should be changed to Sample1 and Sample2 respectively. The function rename() in the dplyr R package allows us to do this:\n\nobs &lt;- rename( obs, \"Sample1\"=x, \"Sample2\"=y )\nobs\n\n    Sample1     Sample2\n1 0.6207567 -0.16285434\n2 0.0356414  0.39711189\n3 0.7731545 -0.07998932\n4 1.2724891 -0.34496518\n5 0.3709754  0.70215136\n\n\n\nThe National River Flow Archive (www.nrfa.ceh.ac.uk) provides data for hundreds of sites (gauges) across the UK. We want to analyze daily river flow data for the River Avon at Bathford. The data are available in the file “Bathford River Flow.csv”.\nWhen looking at the data file, we identify two aspects that need to be taken into account when loading the data into R\n\nThe first 20 lines are data descriptors (so called meta data), while the remaining lines contain the actual data: dates and river flow measurements.\nThe letter “M” appears in the third column whenever the river flow measurement is missing in later years.\n\nTo ignore the first 20 lines and avoid importing the data file in a wrong format, we have to use three of the options provided by the read.csv() function:\n\nBathford_RF &lt;- read.csv(\"data/bathford_river_flow.csv\", skip=20, header=FALSE,\n                         colClasses = c(\"character\",\"numeric\",\"NULL\") ) \n\nWarning in read.table(file = file, header = header, sep = sep, quote = quote, :\ncols = 2 != length(data) = 3\n\n\nThe option \\(\\mathrm{\\texttt{skip=20}}\\) means we ignore the first 20 lines, while \\(\\mathrm{\\texttt{colClasses= c(\"character\",\"numeric\",\"NULL\")}}\\) leads to the third column being ignored when loading the data (you can ignore the warning message in this case). Finally, we set \\(\\mathrm{\\texttt{header=FALSE}}\\), because the file does not provide variable names.\nTip: Have a look at the data set in the data file before trying to load it. R (in particular recent versions) may load the data into an incorrect format instead of giving an error. As an example, remove the option \\(\\mathrm{\\texttt{colClasses=..}}\\). You will find that the number of observations increases, but some of the dates are now listed as “M”.\nLet’s have a look at the imported data using the glimpse() function,\n\nglimpse( Bathford_RF )\n\nRows: 19,697\nColumns: 2\n$ V1 &lt;chr&gt; \"1969-10-27\", \"1969-10-28\", \"1969-10-29\", \"1969-10-30\", \"1969-10-31~\n$ V2 &lt;dbl&gt; 3.998, 3.958, 4.210, 4.480, 4.205, 3.830, 3.723, 3.986, 4.353, 4.52~\n\n\nWe see that there are 19,697 measurements in the data set. However, the variable names have to be changed - V1 and V2 are just not sensible. Further, we have to convert the dates into the date format. The river flow measurements are already stored as numeric values, so no conversion is required.\nLet’s start by changing the variable names to Date and RiverFlow\n\nBathford_RF &lt;- rename( Bathford_RF, Date = V1, RiverFlow = V2 )\n\nbefore converting the variable Date to the correct type,\n\nBathford_RF$Date &lt;- as_date( Bathford_RF$Date, format=\"%Y-%m-%d\" )\n\nWith the data having been cleaned, i.e., they have the correct type and informative names, we can start the analysis. As a first step, it is good practice to report the proportion of missing data of a variable. We can extract the proportion of missing river flow measurements using the functions mean() and is.na()\n\nmean( is.na( Bathford_RF$RiverFlow ) )\n\n[1] 0.006244606\n\n\nWe find that river flow measurements are missing on about 0.62% of dates and this should be reported.\nRemark: Missing data is important when building models. In this course, you are only expected to state the proportion of missing data. The handling of missing data will be considered in more detail in the Year 3 unit MA32022 Statistical Modelling and Data Analytics 3A.\nNow that the data frame is in a much better format, we can plot it using the function plot() covered in Year 1 Probability & Statistics:\n\nplot( Bathford_RF$Date, Bathford_RF$RiverFlow, type='l',\n      xlab=\"Date\", ylab=\"River Flow\", cex.lab = 1.5 )\n\n\n\nRiver flow measurements at Bathford for 27 October 1969 to 30 September 2023.\n\n\n\nWhich of the following conclusions should we report when asked to comment on the frequency of river flow levels above 100m\\(^3\\)/s and the magnitude of river flow levels?\n\nRecorded river flow levels were as high as approximately 250m\\(^3\\)/s.\nThe data exhibits seasonality, with river flow levels being higher in winter than in summer.\nThe data covers the years 1969 to 2023.\nThere is at least one day with river flow levels exceeding 100m\\(^3\\)/s for most years.",
    "crumbs": [
      "Lecture Notes",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "lecture_notes/01-DataWrangling.html#working-with-a-single-data-frame",
    "href": "lecture_notes/01-DataWrangling.html#working-with-a-single-data-frame",
    "title": "Data Wrangling",
    "section": "",
    "text": "We now study a range of aspects that frequently come up during the data wrangling process:\n\nSelecting subsets of observations and variables:\n\nWhen working with a large data set, not all variables and observations may be relevant. So we may want to reduce the size of our data set and only keep the observations required for our analysis. This process can happen before or after the data cleaning.\n\nDeriving new variables from existing data:\n\nIt may be useful to create new variables which we believe to be interesting to explore in our analysis. These new variables should be stored in the same data frame as the other variables.\n\nSummarizing the data:\n\nAs one of the first steps in the data exploration, we should derive summaries of the different variables to gain a better understanding of the data. For instance, as highlighted in Section 1.1.4, the proportion of missing data is one useful summary.\n\nSorting the data:\n\nIn applications, interest may lie in extracting the smallest/largest observations and/or providing a ranking. As such, we need to be able to sort observations based on one, or more, criteria.\n\n\nIn this section we explore how to perform these operations using the dplyr package. In Problem Class 1 we will use the considered techniques to analyze a relatively large data set from Brazil. Other aspects of data exploration will be discussed in the next chapters.\nTip: All steps of the data wrangling / exploration process should be placed in an R (or R Markdown) script, so that we can make modifications quickly if something needs to be changed. It’s also good practice to keep the raw data available in your R Workspace. In the following examples we never replace the raw data.\n\nIn an analysis we may only want to focus on a subset of the data. For instance, when modelling the risk of flooding, we are mostly interested in the extremely high river flow measurements.\nThe function filter() is useful in such cases. Suppose we classified a river flow exceeding 100 m\\(^3\\)/s at the gauge of Bathford in Section 1.1.4 as extremely high. We can then extract the subset of observations exceeding 100m\\(^3\\)/s using filter(), and we use slice_head() to print the first five observations:\n\nBathford_RF_High &lt;- filter( Bathford_RF, RiverFlow &gt; 100 )\nslice_head( Bathford_RF_High, n=5 )\n\n        Date RiverFlow\n1 1970-11-19     104.3\n2 1971-01-21     128.4\n3 1971-01-22     114.8\n4 1971-01-23     115.2\n5 1971-01-24     133.1\n\n\nThe function filter() can also handle multiple conditions. For instance, we can extract the days across the period 1991-2023 when the river flow exceeded 100m\\(^3\\)/s using\n\nBathford_RF_High &lt;- filter( Bathford_RF, RiverFlow &gt; 100, year(Date) &gt; 1990 )\nslice_head( Bathford_RF_High, n=5 )\n\n        Date RiverFlow\n1 1991-01-10     112.1\n2 1992-11-26     106.9\n3 1992-11-27     100.9\n4 1992-11-28     100.6\n5 1992-11-29     131.6\n\n\n\nNot all variables in a data set may be of interest to us. For instance, meteorological data sets often provide measurements for multiple weather variables, but we may only need to analyze precipitation and temperature.\nExample: Let’s consider the data set “Tuscany.csv” which provides information on the population in Tuscany, Italy, for 2020:\n\nTuscany_raw &lt;- read.csv(\"data/tuscany.csv\" )\nslice_head( Tuscany_raw, n=5 )\n\n  Year Postal_Code  Town Province Age Men Women\n1 2020       45001 Aulla       MS   0  32    30\n2 2020       45001 Aulla       MS   1  30    34\n3 2020       45001 Aulla       MS   2  43    39\n4 2020       45001 Aulla       MS   3  50    35\n5 2020       45001 Aulla       MS   4  38    29\n\n\nSuppose we only want to compare the population data for the different provinces and towns. As such, we don’t need the variables Year, since all data are from 2020, and Postal_Code. Let’s look at two possible options to achieve this using the select() function in dplyr.\nThe first option is to specify the variables we want to keep\n\nTuscany &lt;- select( Tuscany_raw, Town:Women )\nslice_head( Tuscany, n=5 )\n\n   Town Province Age Men Women\n1 Aulla       MS   0  32    30\n2 Aulla       MS   1  30    34\n3 Aulla       MS   2  43    39\n4 Aulla       MS   3  50    35\n5 Aulla       MS   4  38    29\n\n\nHere, the colon sign indicates that we want to keep all columns from Town to Women.\nThe second option is to specify the variables to be excluded using the minus sign,\n\nTuscany &lt;- select( Tuscany_raw, -Year, -Postal_Code )\n\nWhether we specify the variables to be kept, or the variables to be removed, really depends on the number of variables to be included (or excluded) - we want to write as little code as possible.\n\nWhen analyzing real-world data, it may be useful to create new variables which we believe to be interesting to explore. For instance, for the population from Tuscany, we may want to calculate the total population for each age group and town, and attach this information as a new variable to the data frame.\nThe mutate() function in the dplyr package is really useful in such situations, as we can produce and directly attach a new variable Population using\n\nTuscany &lt;- mutate( Tuscany, Population = Men + Women )\nslice_head( Tuscany, n=5 )\n\n   Town Province Age Men Women Population\n1 Aulla       MS   0  32    30         62\n2 Aulla       MS   1  30    34         64\n3 Aulla       MS   2  43    39         82\n4 Aulla       MS   3  50    35         85\n5 Aulla       MS   4  38    29         67\n\n\nWe see that mutate() requires us to provide a new variable name and to define how the values of this new variable are to be derived. Note, the function mutate() can also be used to attach values stored in another R object to the data frame.\nImportant: If you use a variable name that already exists within the data frame, mutate() will overwrite this column with the new values - so we can also use mutate() to modify the columns in your data frame.\n\nWe have already introduced quite a few useful functions for data cleaning and wrangling. Let’s now consider the case that we want to combine these functions. For instance, we may want to derive the population per age group and town, and then remove the variables Year and Postal_Code from the original data frame.\nHow can we do this?\nThe first option is to manipulate the data step by step and to always store the R object after finishing one operation (similar to what we have done so far). This would be implemented as\n\nTuscany &lt;- mutate( Tuscany_raw, Population = Men + Women )\nTuscany &lt;- select( Tuscany, -Year, -Postal_Code )\n\nThis is quite a bit of code, because we have to type \\(\\mathrm{\\texttt{Tuscany}}\\) in each line.\nCan we do better?\nWell, we could place all the operations into a single line\n\nTuscany &lt;- select( mutate( Tuscany_raw, Population = Men + Women ), Town:Population )\n\nHowever, such an approach may quickly lead to a large number of brackets, which increases the risk of frustrating syntax errors - remember this may only be the start of our analysis.\nLuckily, we can avoid both these two options by using the pipe command %&gt;% in the dplyr R package. The same commands as above would be implemented as\n\nTuscany &lt;- Tuscany_raw %&gt;%\n  mutate( Population = Men + Women ) %&gt;%\n  select( -Year, -Postal_Code )\nslice_head( Tuscany, n=5 )\n\n   Town Province Age Men Women Population\n1 Aulla       MS   0  32    30         62\n2 Aulla       MS   1  30    34         64\n3 Aulla       MS   2  43    39         82\n4 Aulla       MS   3  50    35         85\n5 Aulla       MS   4  38    29         67\n\n\nThe operations are executed from top to bottom: We take the data frame Tuscany_raw, then apply the mutate() function to create the column Population, and conclude by removing the columns Year and Postal_Code from the created data frame using the select() function.\nTip: Combining multiple R commands can be tricky at first. If you are unsure, try to outline the way you want to manipulate the data before starting to implement it in R.\n\nFor large data sets, we usually want to provide data summaries. For instance, one important summary for the Tuscany data set may be the total number of people within the data. In such situations, we can apply functions such as sum() directly\n\nsum( Tuscany$Population )\n\n[1] 3691409\n\n\nIf we want to extract several such summaries, we can either derive each summary individually, or use the summarize() function in the dplyr R package. Let’s also extract the proportion of men and women\n\nTuscany %&gt;%\n  summarize( \"Population_Tuscany_2020\" = sum( Population ),\n             \"Men_Tuscany_2020\" = sum( Men ),\n             \"Women_Tuscany_2020\" = sum( Women ) )\n\n  Population_Tuscany_2020 Men_Tuscany_2020 Women_Tuscany_2020\n1                 3691409          1787649            1903760\n\n\nThe summarize() function really starts to shine when we combine it with the group_by() function.\nSuppose we wanted the population numbers for each of the provinces, which requires us to sum up the numbers across towns and age groups while accounting for the variable Province. We can do this using group_by() and summarize():\n\nTuscany_Province &lt;- Tuscany %&gt;%\n  group_by( Province ) %&gt;%\n  summarize( Total = sum(Population) )\nTuscany_Province\n\n# A tibble: 10 x 2\n   Province  Total\n   &lt;chr&gt;     &lt;int&gt;\n 1 AR       336450\n 2 FI       997940\n 3 GR       217803\n 4 LI       328855\n 5 LU       383688\n 6 MS       189786\n 7 PI       417799\n 8 PO       265153\n 9 PT       290177\n10 SI       263758\n\n\nThe group_by() function splits the data subject to the specified variable (Province in this case) and, for each subset, the summarize() function then derives the population total.\nRemark: We can specify multiple variables in group_by() to define the subgroups based on several criteria.\nLet’s consider a slightly more complicated task. Suppose we were asked to study the age profile of women within the population. To extract the proportion of women of a certain age, we need to group women by Age, but also keep track of the total number of women within the population. One possible way to extract the proportions is as follows:\n\nTuscany_Women_Age &lt;- Tuscany %&gt;%\n  group_by( Age ) %&gt;%\n  summarize( Number = sum(Women) ) %&gt;%\n  mutate( Proportion = Number / sum(Number) )\n\nNote, we used the fact that the summarize() function returns a data frame, and thus we can perform further operations. Finally, let’s illustrate the calculated proportions using a bar plot:\n\nbarplot( Proportion~Age, data=Tuscany_Women_Age )\n\n\n\nAge profile of living women in Tuscany for the year 2020.\n\n\n\nWe see that the highest proportions are observed for ages 40-70. The lower proportions for younger ages reflect the decrease in birth rates recorded for many countries over the past years. The decreasing proportion beyond 70 is presumably due to an increased rate of mortality for these age groups.\n\nYou may have already seen the sort() command, which allows you to order the values within a vector. When we consider a data frame, we may want to sort its rows subject to the values in one of the columns. For instance, we may want to sort provinces based on their population.\nThe function arrange() in the dplyr R package does exactly this job,\n\narrange( Tuscany_Province, Total )\n\n# A tibble: 10 x 2\n   Province  Total\n   &lt;chr&gt;     &lt;int&gt;\n 1 MS       189786\n 2 GR       217803\n 3 SI       263758\n 4 PO       265153\n 5 PT       290177\n 6 LI       328855\n 7 AR       336450\n 8 LU       383688\n 9 PI       417799\n10 FI       997940\n\n\nWe see that “FI” (Firenze) has the highest population among the provinces in Tuscany. Further, the output demonstrates that the default setting for arrange() is to sort the values in ascending order. Should we want to sort values in descending order, we have to use the additional command desc():\n\nTuscany_Province %&gt;% arrange( desc(Total) )\n\nRemark: If two observations have the same value, they are listed in their original order, regardless of whether we sort in ascending or descending order. If we want to change this (which we sometimes want), we can specify a second variable in arrange(), just as for group_by().",
    "crumbs": [
      "Lecture Notes",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "lecture_notes/01-DataWrangling.html#working-with-multiple-data-sets",
    "href": "lecture_notes/01-DataWrangling.html#working-with-multiple-data-sets",
    "title": "Data Wrangling",
    "section": "",
    "text": "So far we have focused on analyzing a single data file. In many applications, however, data is stored across multiple data files. For instance, we may have one data file containing weather data and another data file providing insurance data related to weather-related damages. In these cases, we want to combine the different data files into a single data frame for our analysis.\nThe dplyr R package provides the functions inner_join(), left_joint(), right_join() and full_join() to combine data frames based on a “key”. All these functions combine two data frames and their application is illustrated via an example in Section 1.3.1.\nWhen working with multiple data sets, we may also want to automate the process. Imagine you had weather measurements for over 100 sites - you do not really want to spend hours just to merge the data frames. This aspect is considered in Section 1.3.2.\n\nIn Section 1.1.4, we focused on the river flow data collected at Bathford. The National River Flow Archive provides data for another gauge located to the west of Bath city centre; you can find the data file “Bath River Flow.csv” on Moodle. Our aim is to combine the river flow measurements into a single data frame.\nWe start by again loading the data for Bathford and renaming the variables,\n\nBathford_RF &lt;- read.csv(\"data/bathford_river_flow.csv\", skip=20, header=FALSE,\n                         colClasses = c(\"character\",\"numeric\",\"NULL\") )\nBathford_RF &lt;- rename( Bathford_RF, Date = V1, RiverFlow = V2 )\n\nA closer look at data file for the Bath gauge suggests that the data format is similar to that for Bathford. The only difference is that we now have to ignore the first 19 instead of the first 20 lines:\n\nBath_RF &lt;- read.csv(\"data/bath_river_flow.csv\", skip=19, header=FALSE,\n                     colClasses = c(\"character\",\"numeric\",\"NULL\") )\nBath_RF &lt;- rename( Bath_RF, Date = V1, RiverFlow = V2 )\n\nLet’s investigate the first element in each data frame:\n\nBath_RF %&gt;% slice_head( n=1 )\n\n        Date RiverFlow\n1 1976-09-01      3.39\n\nBathford_RF %&gt;% slice_head( n=1 )\n\n        Date RiverFlow\n1 1969-10-27     3.998\n\n\nWe see that the two gauges started operating in different years - Bath in 1976 and Bathford in 1969. So the number of rows in the two data frames is different.\nWhen combining the two data frames, we want to match observations based on the variable Date, this is our “key”. Here we use the function full_join(), which ensures that all observations for Bath and Bathford are contained in the combined data set, and we specify that observations should be matched based on the variable Date,\n\nRF &lt;- Bathford_RF %&gt;% full_join( Bath_RF, by=c(\"Date\" = \"Date\") )\nglimpse( RF )\n\nRows: 19,697\nColumns: 3\n$ Date        &lt;chr&gt; \"1969-10-27\", \"1969-10-28\", \"1969-10-29\", \"1969-10-30\", \"1~\n$ RiverFlow.x &lt;dbl&gt; 3.998, 3.958, 4.210, 4.480, 4.205, 3.830, 3.723, 3.986, 4.~\n$ RiverFlow.y &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA~\n\n\nWe see that the values for the first dates are correctly identified as being missing for Bath - the gauge was not in operation at the time. We are left with changing the variable names and converting the data type of Date\n\nRF$Date &lt;- as_date( RF$Date, format=\"%Y-%m-%d\" )\nRF &lt;- rename( RF, Bathford = RiverFlow.x, Bath = RiverFlow.y )\n\nLet’s plot the observations for Bath and Bathford against each other,\n\nplot( RF$Bath, RF$Bathford, cex.lab = 1.5, pch=19,\n      xlab=\"River Flow at Bath\", ylab=\"River Flow at Bathford\" )\n\n\n\nComparison of river flow for Bath and Bathford for 1 September 1976 - 30 October 2023.\n\n\n\nWhat can we conclude from this plot?\nRemark: If we want the first element in the combined data frame to be 01/09/1976 (the date when the gauge at Bath started operations), we would use the function inner_join(),\n\nRF_1976_2020 &lt;- Bathford_RF %&gt;% inner_join( Bath_RF, by=c(\"Date\" = \"Date\") )\n\nRemark: The function inner_join() does not remove the dates after 1976 for which the observations for Bath (or Bathford) are missing, but only the days which are not listed in both files.\n\nIn practice we may work with \\(N\\) data sets of the same (or a very similar) format. For instance, we may have 20 data sets, and each data set contains the river flow measurements for a gauge in Somerset. Then, we do not want to implement a lot of code of the form in Section 1.3.1 just to combine all these data sets into a single data frame. Instead, we will use the for() loop in R.\nExample: Suppose that, in addition to the river flow measurements for Bath and Bathford, we also need to consider the observations for Compton Dando, a small village to the west of Bath not located at the River Avon. For our analysis, it may be good to combine all three data sets into a single data frame, and the following piece of code is one way to create it.\nWe start by defining the file names and the number of lines that we have to ignore when loading the data files\n\ngauges &lt;- c( \"Bath\", \"Bathford\", \"Compton Dando\" )\nlines_to_ignore &lt;- c( 19, 20, 20)\n\nThe next step is to load the data from the different files, store the data frames in a list we call RF_individual, and update the variable names.\n\n# setwd(\"Data/\") &lt;--- REMOVED: Bad practice in RMarkdown/Quarto\nRF_individual &lt;- list()\nfor( k in 1:length(gauges) ){\n  \n  ## Load the data from the .csv file\n  # Construct path relative to project root (or file location)\n  file_name &lt;- paste0( \"data/\", tolower(gsub(\" \", \"_\", gauges[k])), \"_river_flow.csv\" ) \n  RF_individual[[k]] &lt;- read.csv( file_name, skip=lines_to_ignore[k], header=FALSE,\n                                  colClasses = c(\"character\",\"numeric\",\"NULL\") )\n  \n  ## Change the variable names\n  names( RF_individual[[k]] ) &lt;- c( \"Date\", gauges[k] )\n  \n}\n\nThe code above includes two functions you may not have used so far and so we briefly describe them:\n\npaste() is used to append “River Flow.csv” to the name of the gauge to get the file name.\nnames() is used to rename the variable names. In this case, this function was easier to use than rename(); the latter does not like to be given names from a vector.\n\nNow we are ready to merge the different data frames by repeatedly using the function full_join():\n\nRF &lt;- RF_individual[[1]]\nfor( k in 2:length(gauges) )\n  RF &lt;- RF %&gt;% full_join( RF_individual[[k]], by=c(\"Date\"=\"Date\") )\nglimpse( RF )\n\nRows: 23,955\nColumns: 4\n$ Date            &lt;chr&gt; \"1976-09-01\", \"1976-09-02\", \"1976-09-03\", \"1976-09-04\"~\n$ Bath            &lt;dbl&gt; 3.39, 2.83, 2.97, 2.81, 2.90, 2.81, 2.59, 3.11, 2.78, ~\n$ Bathford        &lt;dbl&gt; 2.811, 2.560, 2.337, 2.385, 2.146, 2.359, 2.367, 2.416~\n$ `Compton Dando` &lt;dbl&gt; 0.188, 0.173, 0.170, 0.172, 0.174, 0.174, 0.176, 0.195~\n\n\nThe final step is to convert the type of the variable Date and to sort observations by date; we know that Bathford started collecting data in 1969 but the first entry is for 1976. So we obtain the final data frame using\n\nRF &lt;- RF %&gt;% \n  mutate( Date = as_date( Date, format=\"%Y-%m-%d\" ) ) %&gt;%\n  arrange( Date )\nslice_head( RF, n=5 )\n\n        Date Bath Bathford Compton Dando\n1 1958-03-01   NA       NA          2.97\n2 1958-03-02   NA       NA          2.32\n3 1958-03-03   NA       NA          1.98\n4 1958-03-04   NA       NA          1.70\n5 1958-03-05   NA       NA          1.42\n\n\nWe can now start our analysis, for instance, by plotting the different river flows against each other:\n\npar( mfrow=c(1,3), cex.lab = 1.5 )\nplot( RF$Bath, RF$Bathford, pch=19, xlab=\"Bath\", ylab=\"Bathford\" )\nplot( RF$Bath, RF$`Compton Dando`, pch=19, xlab=\"Bath\", ylab=\"Compton Dando\" )\nplot( RF$Bathford, RF$`Compton Dando`, pch=19, xlab=\"Bathford\", ylab=\"Compton Dando\" )\n\n\n\nScatter plots of river flow measurements for each pair of gauges in the combined data set\n\n\n\nWhat do you conclude from these plots?\nRemark: If we wanted to add more gauges to the data, we only need to update the first two lines of R code in this example; the rest of the code can be left unchanged.",
    "crumbs": [
      "Lecture Notes",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "lecture_notes/01-DataWrangling.html#summary",
    "href": "lecture_notes/01-DataWrangling.html#summary",
    "title": "Data Wrangling",
    "section": "",
    "text": "We have covered some of the key concepts regarding data cleaning and wrangling:\n\nEnsure that variables have the correct type and are given informative names\nUse the dplyr R package when working with a single data frame. The package allows you to create subsets, sort the data, etc.\nIn many real-world applications we have to combine multiple data sets. The dplyr R package also provides functions to achieve this.\n\nImportant: Hardly any real-world data set is “standard” - we had to use some additional functions/options for the river flow data, as well as the Airbnb data analyzed in Problem Class 1. While we introduced useful functions to perform data wrangling, we still usually have to investigate the data file “by hand” before loading the data into R. In this course we cannot possibly cover all scenarios that may occur when working with real-world data, but you can usually find a satisfying solution using Google (or other search engines).",
    "crumbs": [
      "Lecture Notes",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "lecture_notes/04-SpatialDataAnalysis.html",
    "href": "lecture_notes/04-SpatialDataAnalysis.html",
    "title": "Spatial Data Analysis",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union",
    "crumbs": [
      "Lecture Notes",
      "Spatial Data Analysis"
    ]
  },
  {
    "objectID": "lecture_notes/04-SpatialDataAnalysis.html#visualizing-point-referenced-data",
    "href": "lecture_notes/04-SpatialDataAnalysis.html#visualizing-point-referenced-data",
    "title": "Spatial Data Analysis",
    "section": "Visualizing point-referenced data",
    "text": "Visualizing point-referenced data\nMotivation\nThe file “Temperature Germany.csv” contains the maximum temperature recorded on 1 May 2020 for 75 weather stations across Germany. Let’s look at the provided variables\n\nTemperature &lt;- read.csv(\"data/temperature_germany.csv\" )\nnames( Temperature )\n\n[1] \"name\"      \"latitude\"  \"longitude\" \"altitude\"  \"max.temp\" \n\n\nOur variable of interest is max.temp, the recorded maximum temperature, and latitude, latitude and altitude specify the spatial locations of the 75 weather stations.\nWe can already create a first plot of maximum daily temperature at different locations using ggplot2:\n\nlibrary(ggplot2)\nggplot( Temperature, aes( x=longitude, y=latitude ) ) +\n  geom_point( aes(color=max.temp), size=2.5 ) + theme_bw() + \n  scale_color_distiller( palette=\"Reds\", trans=\"reverse\" ) +\n  labs( x=\"Longitude\", y=\"Latitude\", color=\"Temperature\" ) + \n  theme( axis.title=element_text(size=15), axis.text=element_text(size=15) )\n\n\n\nIllustration of daily maximum temperature recorded at 75 weather stations across Germany. We use colour as a visual cue to illustrate differences in maximum temperature.\n\n\n\nIs this really the best way to plot the data?\nOne may argue that the plot is not ideal since it does not provide any spatial context, that is, we cannot relate the illustrated values and locations to any known geographical features.\nWe now introduce two approaches for visualizing point-referenced data, both of which aim to provide additional spatial context to aid interpretation. The first approach uses shapefiles which allow us to add the outline of Germany, that is its borders, to the plot. The second approach will place the points upon a map.\nShapefiles\nAdministrative boundaries can be described as a polygon with a potentially very large number of edges. In some cases we may also deal with several polygons. For instance, when considering the UK, we would have one polygon per island. Such polygons are specified via a shapefile (.shp) and can be used in R.\nThe administrative boundaries of countries are freely available from  www.gadm.org  for educational and academic use. Let’s download the data for Germany and load the Level 1 data into R using the function read_sf() in the sf R package\n\nlibrary(sf)\nGER &lt;- read_sf( \"Data/gadm41_DEU_shp/gadm41_DEU_1.shp\" )\n\nImportant: You need all the downloaded files, not just the .shp file.\nRemark: The Level 0 shapefile only contains the borders, while higher levels (Level 1, etc.) also provide administrative boundaries within the country - this applies to all countries listed on  www.gadm.org .\nWe plot the boundaries using geom_sf() from the sf package in combination with ggplot():\n\nggplot( data=GER ) + theme_bw() + geom_sf()\n\n\n\nAdministrative boundaries of the 16 German states.\n\n\n\nIMPORTANT: The shapefiles provided by GADM are very large, and I noticed that some laptops may struggle to create the plot. Should you have this issue, you may want to use the function st_simplify() to reduce the complexity of your polygon - the function has a parameter dTolerance which specifies how much the “wiggles” in the polygon will be straightened. Let’s set dTolerance=2000 for the shapefile considered above\n\nGER_simple &lt;- st_simplify( GER, dTolerance = 2000, preserveTopology=TRUE )\nggplot( data=GER_simple ) + theme_bw() + geom_sf()\n\n\n\nApproximation of the administrative boundaries of the 16 German states with simplified borders.\n\n\n\nWe see only very minor changes in the plot, and these are unlikely to affect our analysis. If we set dTolerance too large, we may get a shape that looks too different from the original file. As such, we have to be a bit careful with the value we specify for dTolerance.\nMoving on with our analysis, the final step is to add the data points displayed in Figure @ref(fig:Germany0) to the plot above. As such, our data graphic is made up of two layers - one for the shapefile and one for the data points. The order is important now because ggplot2 builds the plot layer by layer:\n\nggplot( GER ) + theme_bw() + geom_sf() + \n  geom_point( data=Temperature, aes(x=longitude, y=latitude, color=max.temp), size=3 ) + \n  scale_color_distiller( palette=\"Reds\", trans=\"reverse\" ) +\n  labs( x=\"Longitude\", y=\"Latitude\", color=\"Temperature\" )\n\n\n\nIllustration of daily maximum temperature recorded at 75 weather stations across Germany. The visual cue colour is used to represent maximum temperature.\n\n\n\nNote, we deviate from the format used in Chapter [2][Data Visualization], because we are now working with two data frames - one for the boundaries and another for the weather data. Therefore, we have to specify the data for the points separately in geom_point().\nProjections\nSo far we have used longitude and longitude within a Cartesian coordinate system. However, the earth is a three-dimensional flattened sphere and thus it is not flat. The process of converting locations in a three-dimensional geographic coordinate system to a two-dimensional representation is called projection.\nThere exists a wide range of projections because no projection can preserve all properties at the same time, in particular shape/angle and area.\nWe can change projections with the coord_sf() function from the sf package. Let’s compare some possible projections applied to Canada and Norway:\n\nlibrary(patchwork)\nCANADA   &lt;- read_sf(\"Data/gadm41_CAN_shp/gadm41_CAN_1.shp\")\nCANADA   &lt;- st_simplify( CANADA, dTolerance = 5000 )\nCANPlot1 &lt;- ggplot( data=CANADA ) + theme_bw() + geom_sf()\nCANPlot2 &lt;- CANPlot1 + coord_sf( crs=st_crs(3347) )\n\nNORWAY   &lt;- read_sf(\"Data/gadm41_NOR_shp/gadm41_NOR_1.shp\")\nNORWAY   &lt;- st_simplify( NORWAY, dTolerance = 2000 )\nNORPlot1 &lt;- ggplot( data=NORWAY ) + theme_bw() + geom_sf()\nNORPlot2 &lt;- NORPlot1 + coord_sf( crs=st_crs(3346) )\n\n( CANPlot1 + CANPlot2 ) / ( NORPlot1 + NORPlot2 )\n\n\n\nAdministrative boundaries of Canada (top) and Norway (bottom). The left plots preserve the angels, while the right plots do better at highlighting that the countries are located on a sphere.\n\n\n\nIn the left plots the meridians and parallels are orthogonal and, thus, these projections are angle-preserving. This projection is also known as the WGS84 coordinate reference system, which is the standard for GPS systems and Google Earth. In the right plots, we do not preserve the angels, but we better visualize that we are working with data on a three-dimensional sphere. As data scientists, we should be aware that our choice of projection may have a direct influence on how the plot is perceived by others. While such aspects are negligible when analyzing data across small spatial areas, we should keep this in mind when considering spatial data collected across large spatial scales.\nRemark: You have to experiment a bit with the value in st_crs() when creating plots. A value of 4326 corresponds to the WGS84 system.\nMaps\nSo far we added boundaries to a plot to aid the reader’s orientation. When we consider the German weather data, we find that the data set also provides altitude. However, such information is difficult to incorporate within a shape file. Therefore, it may be better to provide a map that indicates larger water bodies and possibly elevation.\nWe use the ggspatial R package to import maps provided by OpenStreetMap and then plot the points onto them. This requires us to include the annotation_map_tile() function to our ggplot2 commands and replace geom_point() by geom_spatial_point():\n\nlibrary(ggspatial)\nggplot( Temperature, aes( x=longitude, y=latitude) ) + \n  annotation_map_tile() + \n  geom_spatial_point( aes(color=max.temp), size=3 ) + \n  scale_color_distiller( palette=\"Reds\", trans=\"reverse\" ) +\n  labs( x=\"Longitude\", y=\"Latitude\", color=\"Temperature\" )\n\n\n\nTemperature recorded at 75 weather stations across Germany, with Open Street Map being used to illustrate geographical features.\n\n\n\nRemark: There are different types of maps provided by the ggspatial package, but they are not relevant for the examples we consider in this course.\nWhile this type of map is not great at visualizing altitude, we can observe that the temperature tends to be lower for higher altitudes and for locations close to the sea (this is what we would expect).",
    "crumbs": [
      "Lecture Notes",
      "Spatial Data Analysis"
    ]
  },
  {
    "objectID": "lecture_notes/04-SpatialDataAnalysis.html#inverse-distance-weighting",
    "href": "lecture_notes/04-SpatialDataAnalysis.html#inverse-distance-weighting",
    "title": "Spatial Data Analysis",
    "section": "Inverse distance weighting",
    "text": "Inverse distance weighting\nAfter visualizing the data we now consider the task of making a prediction at an unobserved location. Let \\(x_1,\\ldots,x_n\\) and \\(\\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) denote the observed values and locations respectively; \\(\\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are often described by their longitude and latitude and are located within a bounded area \\(\\mathcal{S}\\subset\\mathbb{R}^2\\).\nConsider the German temperature data in Figure @ref(fig:GERMap). How would you use the data across the 75 weather stations if you were asked to provide a prediction \\(x^*\\) at an unobserved location \\(\\mathbf{s}^*\\)?\nIntuitively, the values at locations geographically close to \\(\\mathbf{s}^*\\) may provide the most useful information. Inverse distance weighting puts this idea into a mathematical framework, and it falls into the category of spatial interpolation techniques. The approach postulates that the predicted value \\(x^*\\) at \\(\\mathbf{s}^*\\) is a weighted average of the observed data points, with higher weighting given to locations that are geographically close to \\(\\mathbf{s}^*\\). We will formalize this approach in the following and see how to perform it in R.\nMathematical framework\nLet \\(d:\\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}_+\\) be a metric that defines a distance between any two points \\(\\mathbf{s}_1=(s_{11},s_{12})\\in\\mathbb{R}^2\\) and \\(\\mathbf{s}_2=(s_{21}, s_{22})\\in \\mathbb{R}^2\\). The most commonly used distance metric is the L\\(_2\\) norm/Euclidian distance:\n\\[\\begin{equation}\nd(\\mathbf{s}_1, \\mathbf{s}_2) = ||\\mathbf{s}_1-\\mathbf{s}_2||_2= \\sqrt{(s_{11}-s_{21})^2 + (s_{12}-s_{22})^2}.\n(\\#eq:Euclidian)\n\\end{equation}\\]\nInverse distance weighting defines the predicted value \\(x^*\\) at the unobserved location \\(\\mathbf{s}^*\\) as \\[\\begin{equation}\nx^* =\n\\begin{cases}\n\\dfrac{\\sum_{i=1}^n w(\\mathbf{s}_i,\\mathbf{s}^*) x_i}{\\sum_{i=1}^n, w(\\mathbf{s}_i,\\mathbf{s}^*)} & \\mbox{if}~d(\\mathbf{s}^*, \\mathbf{s}_i)&gt;0~\\mbox{for all } i=1,\\ldots,n,\\\\\nx_i & \\mathrm{if}~d(\\mathbf{s}^*, \\mathbf{s}_i)=0,\n\\end{cases}\n(\\#eq:IDW)\n\\end{equation}\\] where \\(w(\\mathbf{s}_i,\\mathbf{s}^*) = \\left[d(\\mathbf{s}_i,\\mathbf{s}^*)\\right]^{-p}\\) and \\(p&gt;0\\) is called the power parameter and has to be selected by us. One important rule to remember is that the weight given to the observation closest to \\(\\mathbf{s}^*\\) increases with \\(p\\); we will see this when plotting the calculated predicted values later for different values of \\(p\\).\nPerforming inverse distance weighting in R\nThere exist R functions to perform inverse distance weighting, but we will implement our own function IDW() combines equations @ref(eq:Euclidian) and @ref(eq:IDW). This will make it easier to plot our predictions.\n\nIDW &lt;- function( X, S, s_star, p){\n  d &lt;- sqrt( (S[,1]-s_star[1])^2 + (S[,2]-s_star[2])^2 )\n  w &lt;- d^(-p)\n  if( min(d) &gt; 0 )\n    return( sum( X * w ) / sum( w ) )\n  else \n    return( X[d==0] )\n}\n\nSuppose we want to use the German weather data to predict the maximum temperature on 1 May 2020 at an unobserved location with longitude 13.1 and latitude 51.0, \\(\\mathbf{s}^* = (13.1, 51.0)\\). We consider the settings \\(p=0.5\\) and \\(p=2\\) for the power parameter and obtain the following values for \\(x^*\\):\n\ncoord &lt;- cbind( Temperature$longitude, Temperature$latitude )\ns_star &lt;- c( 13.1, 51.0 )\nIDW( X=Temperature$max.temp, S=coord, s_star, p=0.5 )\n\n[1] 23.4665\n\nIDW( X=Temperature$max.temp, S=coord, s_star, p=2.0 )\n\n[1] 22.67209\n\n\nWe see that the two settings for \\(p\\) give predictions that differ by almost 1 degree Celsius. Consequently, the selection of \\(p\\) is indeed crucial and we will consider its selection in more detail in the next part.\nCreating spatial plots with inverse distance weighting\nWe want to make predictions across the whole study space, instead of a single location \\(\\mathbf{s}^*\\). Here we visualize these predictions across a map, which is also useful to identify suitable values for \\(p\\).\nThe first step is to define a grid that covers the study space \\(\\mathcal{S}\\). Here, we will simply define \\(\\mathcal{S}\\) as a rectangle (based on longitude and latitude coordinates):\n\npoints_lat &lt;- seq( 47.1, 55.1, by=0.05 )\npoints_lon &lt;- seq( 5.8, 15.6, by=0.05 )\npixels &lt;- as.matrix( expand.grid( points_lon, points_lat ) )\n\nWe then apply our implemented IDW() function to the grid points with the power parameter being set to \\(p=0.5\\), \\(p=2.0\\) and \\(p=20\\):\n\np05 &lt;- p2 &lt;- p20 &lt;- c()\nfor( j in 1:length(pixels[,1]) ){\n  p05[j] &lt;- IDW( X=Temperature$max.temp, S=coord, s_star=pixels[j,], p=0.5 )\n  p2[j]  &lt;- IDW( X=Temperature$max.temp, S=coord, s_star=pixels[j,], p=2.0 )\n  p20[j] &lt;- IDW( X=Temperature$max.temp, S=coord, s_star=pixels[j,], p=20  )\n}\n\nThe next step is to collate the estimates into a data frame so that we can plot the predictions:\n\nlibrary(tidyr)\nPredict &lt;- data.frame( \"Lon\"=pixels[,1], \"Lat\"=pixels[,2], \n                       \"p0.5\"=p05, \"p2\"=p2, \"p20\"=p20 )\nPredict &lt;- Predict %&gt;% pivot_longer( cols=p0.5:p20, names_to = \"Power\" )\nPredict &lt;- Predict %&gt;% \n  mutate( Power = case_when( Power == \"p0.5\" ~ \"p=0.5\", Power == \"p2\" ~ \"p=2.0\",\n                             .default = \"p=20\" ) )\n\nFinally, we plot the grid and the calculated predictions. We add the boundaries of Germany and the locations of the weather stations to the plot to provide some spatial context:\n\nggplot( data=Predict ) + theme_bw() +\n  geom_raster( aes( x=Lon, y=Lat, fill=value ) ) + \n  facet_wrap( ~Power ) + \n  scale_fill_distiller( palette=\"Reds\", trans=\"reverse\" ) +\n  geom_sf( data=GER, alpha=0.0, color=\"white\" ) +\n  geom_point( data=Temperature, aes(x=longitude, y=latitude) ) + \n  labs( fill=\"°C\", x=\"Longitude\", y=\"Latitude\" )\n\n\n\nPredicted values of maximum daily temperature on May 1, 2020 for Germany and adjacent regions using inverse distance weighting with three values for the power paramter.\n\n\n\nWhat do we notice?\nWhen considering the range of the colour bars, we notice that \\(p=0.5\\) leads to the estimated temperatures being much less varied than for \\(p=2\\). Further, \\(p=20\\) produces unrealistic predictions and should not be selected, since we predict sudden changes in temperature across space, with the predicted value being almost only determined by the value observed at the location closest to \\(\\mathbf{s}^*\\); the latter is also referred to as a tessellation of the region.\nThe difference in the range of predicted values for the different values of \\(p\\) can be explained by that the weight given to the observation closest to \\(\\mathbf{s}^*\\) increases with \\(p\\) - we highlighted this aspect earlier. These findings give us some guidance on the selection of \\(p\\):\n\n\\(p\\) should be small enough to avoid tessellation, unless we believe such a feature is realistic;\n\\(p\\) should be large enough to give predictions that are similar in range to the observed values.\n\nThe range of predicted values for \\(p=0.5\\) is in fact substantially smaller than the range of observed values in Figure @ref(fig:GERMap) - we predict temperatures above 20 degrees Celsius even for the highest mountains, although the true value is probably much lower. This suggests that \\(p=0.5\\) is too small and that \\(p=2\\) performs best amongst the considered values, because we have both a range of predictions similar to the observed data and the produced prediction map still appears relatively smooth.\nRemark 1: We ignored that temperature depends on altitude in our analysis. If we had access to altitude for each point on the grid, we could extend the distance measure in Equation @ref(eq:Euclidian) to incorporate latitude, longitude and altitude in the inverse distance weighting estimates.\nRemark 2: We selected \\(p\\) “by eye”. We may use cross-validation (which some of you may have seen in MA22018) to find the value \\(p\\) that performs best in terms of prediction.\nRemark 3: Care should be taken when using inverse distance weighting to make predictions for a location \\(\\mathbf{s}^*\\) that is not close to \\(\\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\). For instance, we should not use the temperature data from Germany to predict the temperature for Bath.",
    "crumbs": [
      "Lecture Notes",
      "Spatial Data Analysis"
    ]
  },
  {
    "objectID": "lecture_notes/04-SpatialDataAnalysis.html#variogram",
    "href": "lecture_notes/04-SpatialDataAnalysis.html#variogram",
    "title": "Spatial Data Analysis",
    "section": "Variogram",
    "text": "Variogram\nMotivation\nWe now move on to exploring and quantifying spatial dependence, that is, how informative is the data at one location to say something about near-by locations. This is a key aspect underlying the majority of spatial data analysis methods. For instance, inverse distance weighting makes the assumption that data collected at the locations \\(\\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) is useful to make predictions at unobserved locations, and that locations closest to the unobserved location provide the most information.\nIn the examples considered so far, such as the German weather data in Figure @ref(fig:GERMap), we found that spatially close sites tend to have similar values.\nAs a second motivating data example, let’s consider the recorded sea surface temperature anomalies for the North Sea for 1 November 2024 (provided in the file “SeaSurface.csv”) as recorded by NOAA:\n\nSSTA &lt;- read.csv(\"data/sea_surface_temperature_anomalies.csv\" )\n\nObservations are on a 5km resolution. To create a plot of the data, we first have to convert the data frame providing longitude, latitude and sea surface temperature into a spatial data object using st_as_sf():\n\nSSTA_sf &lt;- st_as_sf( SSTA, coords=c(\"lon\", \"lat\") ) %&gt;% st_set_crs( 4326 )\n\nWe can now create the plot with geom_sf():\n\nggplot( data=SSTA_sf ) + theme_bw() + geom_sf( aes(color=Anomaly) ) + \n  labs( x=\"Longitude\", y=\"Latitude\", color=\"SST Anomaly\" )\n\n\n\nSea surface temperature anomalies on 1 November 2024 for the North Sea as recorded by NOAA.\n\n\n\nThe plot shows that locations which are spatially close observe a similar value. We now want to quantify this similarity of values of spatially close sites using the variogram which is based on concepts we are familiar with from Year 1 Probability and Statistics.\nMathematical framework\nWe already came across dependence of two random variables in Year 1. To explore dependence between two random variables \\(X\\) and \\(Y\\), we considered the covariance [ (X,Y) = = (XY) - X Y. ] In a spatial context, we now consider the observation at a spatial site as the realization of a random variable, with the random variables at any two sites being potentially dependent. This leads to the concept of a random field, which is also referred to as a spatial random process.\nDefinition: A random field \\(\\{X(\\mathbf{s}):\\mathbf{s}\\in\\mathcal{S}\\}\\) is a family of random variables that are defined on the same probability space and indexed by the spatial location \\(\\mathbf{s}\\in\\mathcal{S}\\).\nFor the sea surface data, \\(X(\\mathbf{s})\\) describes the distribution of the sea surface temperature anomaly at location \\(\\mathbf{s}\\in\\mathcal{S}\\). Further, Figure @ref(fig:SST) corresponds to a single sample from the random field.\nConsider an arbitrary pair \\((\\mathbf{s},\\tilde{\\mathbf{s}})\\) of sites across the region \\(\\mathcal{S}\\). While dependence of \\(X(\\mathbf{s})\\) and \\(X(\\tilde{\\mathbf{s}})\\) may be quantified via their covariance, we usually use the semi-variogram in spatial data analysis:\nDefinition: The semi-variogram for the locations \\(\\mathbf{s}\\) and \\(\\tilde{\\mathbf{s}}\\) is defined as [ (,) = . ]\nFrom now on we make the assumption that \\(\\mathbb{E}\\left[X(\\mathbf{s})\\right]\\) is the same for all \\(\\mathbf{s}\\in\\mathcal{S}\\). Consequently, the expression for the semi-variogram simplifies to half the average squared difference between the values at these locations,\n\\[\\begin{equation}\n\\gamma(\\mathbf{s},\\tilde{\\mathbf{s}}) = \\frac{1}{2} \\mathbb{E}\\left[\\{X(\\mathbf{s})-X(\\tilde{\\mathbf{s}})\\}^2\\right] = \\frac{1}{2} \\mathrm{Var}\\left[X(\\mathbf{s})-X(\\tilde{\\mathbf{s}})\\right].\n(\\#eq:Semivariogram)\n\\end{equation}\\]\nThe semi-variogram has several important properties:\n\n\\(\\gamma(\\mathbf{s},\\tilde{\\mathbf{s}})\\geq 0\\) since it is the expectation of a square, with smaller values corresponding to stronger dependence.\nIf \\(\\mathbf{s}=\\tilde{\\mathbf{s}}\\), we have \\(\\gamma(\\mathbf{s},\\tilde{\\mathbf{s}})=0\\).\nIf \\(X(\\mathbf{s})\\) and \\(X(\\tilde{\\mathbf{s}})\\) are independent and identically distributed, [ (,)=[X()]=[X()]. ]\nEstimation of the semi-variogram\nWe want to estimate \\(\\gamma(\\mathbf{s},\\tilde{\\mathbf{s}})\\) in expression @ref(eq:Semivariogram) using the observations \\(x_1,\\ldots,x_n\\) and locations \\(\\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\).\nThe main challenge is that we only have a single observation at each of the \\(n\\) locations. For instance, if we were to derive an estimate for \\(\\gamma(\\mathbf{s}_1,\\mathbf{s}_2)\\) using \\(x_1\\) and \\(x_2\\) only, we would have \\(\\gamma(\\mathbf{s}_1,\\mathbf{s}_2)=\\frac{1}{2}(x_1-x_2)^2\\). However, such an estimate is likely to be poor. This is a common problem in spatial data analysis and applies to all the examples considered so far.\nTo make use of all the data available, we make the assumptions that the random field is stationary and isotropic. Simply speaking, these assumptions imply that for any pair (\\(\\mathbf{s},\\tilde{\\mathbf{s}}\\)) of locations \\(\\gamma(\\mathbf{s},\\tilde{\\mathbf{s}})\\) is fully specified by their spatial distance. As such, there exists a function \\(\\tilde{\\gamma}:\\mathbb{R}\\to\\mathbb{R}_+\\) such that for all \\(\\mathbf{s},\\tilde{\\mathbf{s}}\\in\\mathcal{S}\\) [ (,) = ( || - || ). ]\nImportant: You have to carefully consider whether the assumption holds that dependence is fully described by spatial distance. If this is not the case, you should not apply the following estimation procedure.\nRemark: The concepts of stationarity and isotropy will be introduced more formally in MA32024 Statistical Modelling and Data Analytics 3B. In this course it’s only important to discuss the general assumptions we make when estimating the semi-variogram.\nFor a stationary and isotropic random process, we can estimate \\(\\tilde{\\gamma}(h)\\) at distance \\(h&gt;0\\) as follows:\n\nFind all pairs of sites with a distance similar to \\(h\\). This gives the set \\(\\mathcal{N}_h =\\{(i,j):||\\mathbf{s}_i - \\mathbf{s}_j||\\approx h\\}\\).\nCalculate the estimate for \\(\\tilde\\gamma(h)\\) as [ (h) = _{(i,j) _h} (x_i-x_j)^2. ]\n\nThe R package gstat provides the function variogram() to perform the estimation. We will see, however, in the following examples that some data wrangling is required.\nAnalysis of sea surface temperature anomalies\nThe first step is to remove any locations with missing data (represented by NA) from the data frame - we have no data for land areas and the variogram() function cannot handle such data sets:\n\nSSTA_gamma &lt;- drop_na( SSTA, Anomaly )\n\nBefore we can use the variogram() function, we have to carefully consider whether all our key assumptions are reasonable:\n\n\\(\\mathbb{E}\\left[X(\\mathbf{s})\\right]\\) is constant across \\(\\mathcal{S}\\):\n\nSince we consider anomalies, i.e., deviations from the mean, we may assume that the mean anomaly is 0 (or at least very close to it) for all locations in the North Sea.\n\nDependence only depends on spatial distance:\n\nFigure @ref(fig:SST) shows a similar level of variability in the values of close-by sites for all areas of the North Sea. While this is not a sufficient condition, there is no clear reason for us to reject the assumption.\n\n\nNow that we verified that our assumptions are not unreasonable, we are ready to estimate the semi-variogram for the random field \\(\\{X(\\mathbf{s}):\\mathbf{s}\\in\\mathcal{S}\\}\\). The first step is to convert the data frame SST_gamma to make it clear that the spatial locations are specified by their latitude and longitude coordinates:\n\nlibrary(sp)\ncoordinates( SSTA_gamma ) &lt;- ~lon+lat\n\nWe can now use the variogram() function to calculate and plot the estimated function \\(\\hat\\gamma(h)\\):\n\nlibrary(gstat)\ngamma_hat &lt;- variogram( Anomaly~1, SSTA_gamma, width=0.1, cutoff=2  )\nggplot( gamma_hat, aes( x=dist, y=gamma/2 ) ) + geom_point( size=2 ) + \n  theme_bw() + labs( x=\"Distance\", y=\"Semi-variogram\" )\n\n\n\nEstimated semi-variogram for sea surface temperature anomalies in the North Sea based on the data for 1 November 2024.\n\n\n\nFrom the plotted semi-variogram we conclude that the dependence between pairs of sites decreased with increasing spatial distance. It’s also important to consider whether the function levels off at some distance, which would indicate independence at large spatial distances. This seems not to be the case here, and thus observations are dependent even at a distance of \\(h=2\\).\nRemark: The options and in the variogram function() specify the size of \\(\\mathcal{N}_h\\) and the maximum distance \\(h\\) to be considered in the estimation.",
    "crumbs": [
      "Lecture Notes",
      "Spatial Data Analysis"
    ]
  },
  {
    "objectID": "lecture_notes/04-SpatialDataAnalysis.html#principal-component-analysis",
    "href": "lecture_notes/04-SpatialDataAnalysis.html#principal-component-analysis",
    "title": "Spatial Data Analysis",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\nIn many applications, interest lies in identifying the key data structures and to perform dimension reduction. There exists a range of techniques, such as cluster analysis, in data science and machine learning to achieve this. To conclude our trip through the world of point-referenced data, we introduce principal component analysis (PCA) and illustrate how it may be used to explore the spatial structure in the data. You may learn more about this technique in the Year 3 unit Applied Data Science.\nMotivation\nLet’s consider a toy example with two sites and \\(T\\) data points for each site. We use \\(x_{i,t}\\) to denote the observation for site \\(i\\in\\{1,2\\}\\) on day \\(t\\in\\{1,\\ldots,T\\}\\). For illustration, we simulate some data with \\(T=200\\) and visualize it using a scatter plot:\n\nset.seed(2025)\nx1 &lt;- rnorm( 200, mean=2, sd=4 )\nx2 &lt;- rnorm( 200, mean=x1, sd=1 )\nX  &lt;- data.frame( \"Site1\"=x1, \"Site2\"=x2 )\nggplot( X, aes(x=Site1, y=Site2) ) + geom_point() + \n  labs( x=\"Data for Site 1\", y=\"Data for Site 2\" ) + theme_bw()\n\n\n\nToy data set with two spatial sites and 200 observations per site.\n\n\n\nWe can see that the simulated data is positively correlated, with most of the points lying close to the line \\(y=x\\). Principal component analysis is concerned with finding these important directions in cases where we have many variables / a large number of spatial sites.\nIn this course, we focus on deriving these directions as they give insight into the spatial structure of the data. Another important aspect of PCA, which we will not cover, is that of dimension reduction, which is particularly useful for modelling. For the data above, we may use PCA to project the two-dimensional data onto a one-dimensional subspace, which is illustrated below.\n\n\n\n\nOriginal data (black) and the data points projected onto a one-dimensional linear subspace using PCA (red).\n\n\n\nWe won’t cover the mathematical background on PCA, and more details will be covered in the Year 3 Applied Data Science unit.\nAnalyzing spatial structure\nSuppose we have \\(T\\) observations \\(x_{i,1},\\ldots,x_{i,T}\\) for the \\(i\\)-th site with location \\(\\mathbf{s}_i\\) (\\(i=1,\\ldots,n\\)), and that \\(x_{1,t},\\ldots,x_{n,t}\\) (\\(t=1,\\ldots,T\\)) are recorded at the same time. We have to perform the following steps which are done in one go by the R function prcomp():\n\nCalculate \\(\\tilde{x}_{i,t} = (x_{i,t} - \\bar{x}_i)/\\hat\\sigma_i\\), where [ {x}_i = ] denotes the average value for the observations at the location \\(\\mathbf{s}_i\\), and \\(\\hat\\sigma_i\\) is the sample standard deviation.\nGiven the vectors \\(\\left\\{\\tilde{\\mathbf{x}}_t =(\\tilde{x}_{1,t},\\ldots,\\tilde{x}_{n,t}):t=1,\\ldots,T \\right\\}\\), we derive the matrix [ = _{t=1}^T _t _t^{}. ] This matrix is also known as the empirical covariance matrix, and it is a multivariate extension of the covariance covered in Year 1 Probability and Statistics.\nDerive the eigenvalues and eigenvectors of \\(\\Sigma\\), i.e., we are seeking the matrix \\(\\mathbf{U}\\) of eigenvectors and the diagonal matrix \\(\\mathbf{D}\\) of eigenvalues \\(\\lambda_1,\\ldots,\\lambda_n\\) such that \\(\\Sigma = \\mathbf{UDU}^{\\mathrm{T}}\\).\n\nHow do we use the eigenvectors and eigenvalues to analyze the structure in the spatial data?\n\nWe visualize the eigenvectors using the techniques covered so far. Each entry of an eigenvector corresponds to one of the sites and we study their spatial structure, starting from the eigenvector with the largest eigenvalue.\nWe use the eigenvalues \\(\\lambda_1,\\ldots,\\lambda_n\\) to determine how many eigenvectors we need to explore. The eigenvalues are ordered, i.e., \\(\\lambda_1 &gt; \\lambda_2 &gt; \\cdots &gt; \\lambda_n\\). For each \\(m=1,\\ldots,n\\), we compute the ratio \\[\\begin{equation}\n\\frac{\\sum_{j=1}^m \\lambda_j}{\\sum_{j=1}^n \\lambda_j}.\n(\\#eq:Ratio)\n\\end{equation}\\] We then look for the first value of \\(m\\) for which the ratio exceeds 0.9. This corresponds to these \\(m\\) components explaining at least 90% of the variation in the data. This is a widely used rule-of-thumb in PCA.\n\nImportant: For the eigenvalues and eigenvectors to accurately reflect the spatial structure, we have to make the assumption of linearity. This means that for any \\(i=1,\\ldots,n\\), we can approximate \\(X_{i,t}\\) by a linear combination of the (\\(n-1\\)) remaining variables \\(X_{1,t},\\ldots,X_{i-1,t},X_{i+1,t},\\ldots,X_{n,t}\\).\nPCA may seem very abstract at this point, but the following example with illustrate how to use it in practice.\nExample: Precipitation across Colorado, United States\nThe file “PrecipitationColorado.csv” contains the amount of precipitation per month for 30 cities in the state and the period 2010-2023. So we have \\(n=30\\) sites and \\(T=168\\) observations per site.\nLet’s import the precipitation data and the information on the spatial sites.\n\nColoradoPrecip &lt;- read.csv(\"data/precipitationcolorado.csv\" )\nColoradoCities &lt;- read.csv(\"data/cities_colorado.csv\" )\n\nBefore we perform PCA, we have to consider the assumption of linearity, which can be difficult to verify in practice. In this example it is sufficient to just plot the data for pairs of sites against each other and to see whether there is a linear pattern\n\nggplot( ColoradoPrecip, aes( x=VALDEZ, y=WETMORE) ) + geom_point() + theme_bw() + \nggplot( ColoradoPrecip, aes( x=TOWNER, y=STONINGTON) ) + geom_point() + theme_bw() + \nggplot( ColoradoPrecip, aes( x=CARDIFF, y=NORTHGLENN) ) + geom_point() + theme_bw()\n\n\n\nScatter plots of the observed amounts of precipitation for three pairs of sites\n\n\n\nThere is clearly some positive correlation between all pairs of sites and so we can conclude that linearity is a reasonable assumption. So we can now proceed with the analysis and use the R function prcomp() to perform the computations in Section4.4.2. This function requires the data to be in a \\(T\\times n\\) matrix and thus we first have to remove the column date before calling prcomp():\n\nPrecip &lt;- ColoradoPrecip %&gt;% select( -date )\nPCA &lt;- prcomp( Precip, scale. = TRUE ) ## Combines all 3 steps\n\nThe object PCA contains the computed eigenvalues and eigenvectors:\n\nsdev - square roots of the eigenvalues.\nrotation - eigenvectors\n\nWe now have to decide which eigenvectors should be visualized. The eigenvalues in sdev are ordered in terms of magnitude, i.e., the first eigenvectors are more important than the latter. So we are computing and visualizing the ratio @ref(eq:Ratio) for \\(m=1,\\ldots,30\\)\n\nColoradoEV &lt;- data.frame( m=1:30, Ratio = cumsum(PCA$sdev^2) / sum(PCA$sdev^2) )\nggplot( ColoradoEV, aes(x=m, y=Ratio) ) + geom_point() + \n  geom_abline( intercept = 0.9, slope = 0, linewidth=1.5 ) + theme_bw()\n\n\n\nProportion of variance explained by the first m eigenvectors\n\n\n\nThe plot shows that the first three eigenvectors explain more than 90% of the variance in the data. Therefore, it is sufficient to visualize the first three eigenvectors, which we achieve by combining the values in the eigenvectors with the spatial sites:\n\nColoradoCities &lt;- ColoradoCities %&gt;%\n  mutate( PC1 = PCA$rotation[,1], PC2 = PCA$rotation[,2], PC3 = PCA$rotation[,3] ) %&gt;%\n  pivot_longer( cols=PC1:PC3, names_to=\"PC\" )\n\nFinally, we can plot the values of the first three eigenvectors using the visualizaion techniques we have seen before:\n\nggplot( ColoradoCities, aes( x=LONG, y=LAT) ) + \n  facet_wrap( ~PC ) +\n  annotation_map_tile() + \n  geom_spatial_point( aes(color=value), size=5 ) + \n  scale_color_distiller( palette=\"RdYlBu\" ) +\n  labs( x=\"Longitude\", y=\"Latitude\", color=\"value\" )\n\n\n\nIllustration of the first three eigenvectors for the precipitation data from Colorado.\n\n\n\nNow we have to interpret the plots:\n\nAll entries in the first eigenvector have the same sign. This suggests that given monthly precipitation is high at one location, it also tends to be high at the other locations. We further observe a west-east trend in the values. Looking at the geography of Colorado, this direction likely represents the effect of the Rocky Mountains, which run through the western part of the state and stretch from north to south. As such, spatial variations in the amount of precipitation are driven by the different climates of Western and Eastern Colorado.\nThe second eigenvector shows a west-east trend. This may reflect again the effect of the Rocky Mountains on the spatial variation in the amount of precipitation.\nThe third eigenvector indicates a south-east to north-west trend across Colorado. Due to its size, there is quite a difference in climate between Northern and Southern Colorado. Northern Colorado tends to observe more snowfall in winter while Southern Colorado experiences more thunderstorms in the summer. So we may conclude that the third eigenvector captures this difference in climate",
    "crumbs": [
      "Lecture Notes",
      "Spatial Data Analysis"
    ]
  },
  {
    "objectID": "lecture_notes/04-SpatialDataAnalysis.html#visualizing-point-pattern-data",
    "href": "lecture_notes/04-SpatialDataAnalysis.html#visualizing-point-pattern-data",
    "title": "Spatial Data Analysis",
    "section": "Visualizing point pattern data",
    "text": "Visualizing point pattern data\nRecall that the difference between point-referenced and point pattern data is that for the latter the points are randomly distributed across space, while they are fixed when analyzing point-referenced data. When visualizing point pattern data, there is no large difference to point-referenced data. This is due to both data types requiring us to add points (often provided by their latitude and longitude) to a shapefile or map.\nWe illustrate this aspect using the data in “Wildfires.csv”. The data file provides the locations of wildfires recorded for California between 2013 and 2020:\n\nWildFires &lt;- read.csv(\"data/wildfires.csv\" )\n\nLet’s add the points in the Californian wildfire data set to a shapefile and a map, and also highlight the size of the burned area:\n\nUSA &lt;- read_sf( \"Data/Shapefile USA/gadm41_USA_1.shp\" )\nCalifornia &lt;- filter( USA, NAME_1==\"California\" ) %&gt;% st_simplify(dTolerance = 2000)\nshape &lt;- ggplot( data=California ) + theme_bw() + geom_sf() + \n  geom_point( data=WildFires, aes(x=Longitude,y=Latitude, color=log(AcresBurned) ) ) +\n  scale_color_distiller( palette=\"Reds\", trans=\"reverse\" ) +\n  theme( legend.position=\"none\" )\nmap &lt;- ggplot( WildFires, aes( Longitude, y=Latitude ) ) + \n  annotation_map_tile() + geom_spatial_point( aes(color=log(AcresBurned)) ) + \n  scale_color_distiller( palette=\"Reds\", trans=\"reverse\" ) +\n  labs( x=\"Longitude\", y=\"Latitude\", color=\"Log( Acres burned)\" )\nshape + map \n\n\n\nLocations of wildfires in California between 2013 and 2020 highlighting state boundaries (left) and with an underlying map (right). The size of the burned area is visualized using colour.\n\n\n\nThe plots highlight that there are areas of California that did not record any wildfires. A look on the map reveals that the south-east consists of the Mojave and Colorado Deserts and thus we would not expect to see wildfires there.",
    "crumbs": [
      "Lecture Notes",
      "Spatial Data Analysis"
    ]
  },
  {
    "objectID": "lecture_notes/04-SpatialDataAnalysis.html#intensity-of-a-point-process",
    "href": "lecture_notes/04-SpatialDataAnalysis.html#intensity-of-a-point-process",
    "title": "Spatial Data Analysis",
    "section": "Intensity of a point process",
    "text": "Intensity of a point process\nMotivation\nWhen analyzing point pattern data across a region \\(\\mathcal{S}\\), we want to identify areas with a high density of points and explore possible reasons for the observed point patterns. For the wildfire data in Figure @ref(fig:Wildfires), we already found that some variation in the density of the points can be explained by the varied landscape and climate.\nHowever, such conclusions may be difficult in some applications. Let’s consider a data set on tornadoes across the United States (excluding Alaska and Hawaii) between 1950 and 2021:\n\nTornadoes &lt;- read.csv(\"data/tornadoes.csv\" )\nggplot( Tornadoes, aes( Lon, y=Lat ) ) + annotation_map_tile() + \n  geom_spatial_point() + labs( x=\"Longitude\", y=\"Latitude\" )\n\n\n\nLocations of tornadoes across the United States\n\n\n\nFrom this plot we cannot identify where the most tornadoes occur. In such cases a common approach is to consider the intensity of the point process, which is directly related to the expected number of points over any subregion \\(\\mathcal{B}\\subseteq\\mathcal{S}\\).\nImportant: When working with point pattern data, both the number of points and their locations are random. For instance, we do not know how many wildfires will we observe and where they will occur.\nLet’s formally define the intensity of a point process. For a subregion \\(\\mathcal{B}\\subseteq\\mathcal{S}\\), we introduce the random variable \\(N(\\mathcal{B})\\) as the number of points of our sample that lie within \\(\\mathcal{B}\\). The intensity function \\(\\lambda:\\mathcal{S}\\to\\mathbb{R}_+=\\{x\\in\\mathbb{R}:x\\geq0\\}\\) then describes the expectation of \\(N(\\mathcal{B})\\) with [ ({}) = = _{} () . ] When \\(\\lambda(\\cdot)\\) is constant across \\(\\mathcal{S}\\), we term the point process homogeneous. Otherwise, we call the point process non-homogeneous.\nIn what follows we describe two methods for visualizing the intensity: quadrat counting (Section 4.6.2) and the kernel smoothed intensity function (Section 4.6.3). These plots may also help us to explore whether a process is homogeneous or non-homogeneous, but we will see that it is not straightforward.\nQuadrat counting\nLet \\(\\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\in\\mathcal{S}\\) denote the locations of the observed points. We then partition \\(\\mathcal{S}\\) into disjoint subregions \\(\\mathcal{B}_1,\\ldots,\\mathcal{B}_m\\), with \\(\\bigcup_{j=1}^m \\mathcal{B}_j = \\mathcal{S}\\), and count the number of points lying within each of them. This gives us an estimate \\(\\widehat{\\mu(\\mathcal{B}_j)}\\) for \\(\\mu(\\mathcal{B}_j)~(j=1,\\ldots,m)\\), with \\[\\begin{equation}\n\\widehat{\\mu(\\mathcal{B}_j)} = \\sum_{i=1}^n \\mathbb{I}\\{\\mathbf{s}_i \\in \\mathcal{B}_j\\},\n(\\#eq:Quadrant)\n\\end{equation}\\] where \\(\\mathbb{I}\\{\\mathbf{s}_i \\in \\mathcal{B}_j\\}=1\\) if \\(\\mathbf{s}_i \\in \\mathcal{B}_j\\) and \\(\\mathbb{I}\\{\\mathbf{s}_i \\in \\mathcal{B}_j\\}=0\\) when \\(\\mathbf{s}_i \\notin \\mathcal{B}_j\\). One common choice is to set \\(\\mathcal{B}_1,\\ldots,\\mathcal{B}_m\\) to rectangles or quadrats. This is why this approach is called quadrat counting.\nIf we assume that \\(\\lambda(\\mathbf{s})\\) is constant for all \\(\\mathbf{s}\\in\\mathcal{B}_j\\), we can estimate the value of \\(\\lambda(\\mathbf{s})~(\\mathbf{s}\\in\\mathcal{B}_j)\\) as \\[\\begin{equation}\n\\hat{\\lambda}^{(Q)}(\\mathbf{s}) = \\frac{\\widehat{\\mu(\\mathcal{B}_j)}}{|\\mathcal{B}_j|},\n(\\#eq:Quadrat)\n\\end{equation}\\] where \\(|\\mathcal{B}_j|\\) is the area of the subregion \\(\\mathcal{B}_j\\).\nWhen performing this approach, we have to balance two aspects:\n\n\\(\\mathcal{B}_1,\\ldots,\\mathcal{B}_m\\) should be small enough such that \\(\\lambda(\\mathbf{s})\\) is approximately constant for all \\(\\mathbf{s}\\in\\mathcal{B}_j\\)\n\\(\\mathcal{B}_1,\\ldots,\\mathcal{B}_m\\) should be large enough such that \\(\\widehat{\\mu(\\mathcal{B}_j)}\\) in @ref(eq:Quadrant) provides a reliable estimate for \\(\\mu(\\mathcal{B}_j)\\).\n\nLet’s again consider the wildfire data set. While we may implement the approach from scratch, we us the spatstat R package. The first step is to combine the shapefile and observed points into a ppp object:\n\nlibrary(spatstat)\nWildFires_ppp &lt;- ppp( WildFires$Longitude, WildFires$Latitude,\n                      poly = California$geometry[[1]][[1]] )\n\nWe now use the quadratcount() function and compare results for two choices for the number of subregions:\n\npar( mfrow=c(1,2), mai=c(0.01,0.01,0.5,0.01) )\nWildFireQuadrants &lt;- quadratcount( WildFires_ppp, nx=2, ny=2 )\nplot( WildFireQuadrants )\nWildFireQuadrants &lt;- quadratcount( WildFires_ppp, nx=6, ny=6 )\nplot( WildFireQuadrants )\n\n\n\nQuadrat counting estimates based on a 2x2 grid (left) and a 6x6 grid (right).\n\n\n\nThe plots indicate that the highest intensities are observed close to San Francisco and in the south-west, close to the Mexican border. We may argue that setting \\(m=4\\) is too small as we cannot identify a clear structure, while the \\(6\\times6\\) grid offers a good balance between the two aspects outlined above. Due to the large differences in the estimated intensities, we may conclude that the point process is non-homogeneous.\nImportant: The quadratcount() function counts the number of points, but does not calculate the intensity. So we would need to divide by the size of the subregions to obtain \\(\\hat{\\lambda}^{(Q)}(\\cdot)\\).\nKernel smoothed intensity\nOur second method for exploring the intensity of a point process is based on the concept of density plots we studied in Section [2.2.4][Histograms and density plots]. Given a probability density function (kernel) \\(K\\), we define the kernel smoothed intensity estimate for \\(\\lambda(\\mathbf{s})\\), \\(\\mathbf{s}\\in\\mathcal{S}\\), as \\[\\begin{equation}\n\\hat\\lambda^{(K)}(\\mathbf{s}) = \\sum_{i=1}^{n} K\\left(||\\mathbf{s}-\\mathbf{s}_i||_2\\right),\n(\\#eq:SKI)\n\\end{equation}\\] where \\(||\\mathbf{s}-\\mathbf{s}_i||_2\\) denotes the Euclidian distance of \\(\\mathbf{s}\\) and \\(\\mathbf{s}_i\\). The difference to Section [2.2.4][Histograms and density plots] is that \\(K\\) is the density of a random vector and not of a random variable.\nThe estimate \\(\\hat{\\lambda}^{(K)}\\) defined in equation @ref(eq:SKI) satisfies [ _{^2} ^{(K)}() = n. ] However, we may have \\(\\int_{\\mathcal{S}} \\hat{\\lambda}^{(K)}(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\neq n\\), because \\(\\hat{\\lambda}^{(K)}(\\mathbf{s})\\) may be positive outside the region \\(\\mathcal{S}.\\) This is also referred to as the edge effect bias, because it is usually caused by points that lie close to the boundary of \\(\\mathcal{S}\\).\nThis leads to the uniformly corrected kernel smoothed intensity function [ ^{(C)}() = _{i=1}^{n} K(||-_i||2), g() = {} K(-) . ] This alternative estimate corrects for the edge effect bias.\nThere are multiple choices for the kernel \\(K\\) and the correction, but we will limit our analysis to \\(\\hat\\lambda^{(K)}\\) and \\(\\hat\\lambda^{(C)}\\), because they are the default settings for the density.ppp() function in the spatstat R package:\nExample: Let’s consider the Californian wildfire data set again, and we want to compare the two estimates \\(\\hat\\lambda^{(K)}\\) and \\(\\hat\\lambda^{(C)}\\). We can directly apply the density.ppp() function to the ppp object we computed earlier:\n\nlambdaK &lt;- density.ppp( WildFires_ppp, edge=FALSE )\nplot( lambdaK, main=\"Uncorrected\" )\nlambdaC &lt;- density.ppp( WildFires_ppp, edge=TRUE )\nplot( lambdaC, main=\"Corrected\" )\n\n\n\nUncorrected (left) and corrected (right) kernel smoothed intensity for Californian wildfires.\n\n\n\nThe range of values in the legend shows that the estimates for \\(\\lambda^{(C)}\\) are higher than for \\(\\lambda^{(K)}\\) due to the correction and we also see some differences in the colour pattern close to the boundaries. Further, the spatial structure is similar to that of \\(\\hat{\\lambda}^{(Q)}\\) in Figure @ref(fig:quadrat) - so we again conclude that the point process describing the occurrence of wildfires is non-homogeneous.\nRemark: Similar to density plots we can also introduce the notion of bandwidth in equation @ref(eq:SKI). We can manually set the bandwidth in the density.ppp() function using the argument sigma, with a smaller value leading to the kernels being more concentrated around the observed locations, as the following plot illustrates:\n\nlambda_sigma0.1 &lt;- density.ppp( WildFires_ppp, edge=TRUE, sigma = 0.1 )\nplot( lambda_sigma0.1, main=\"sigma=0.1\" )\nlambda_sigma10 &lt;- density.ppp( WildFires_ppp, edge=TRUE, sigma = 10 )\nplot( lambda_sigma10, main=\"sigma=10\" )\n\n\n\nCorrected kernel smoothed intensity estimates for Californian wildfires with sigma=0.1 (left) and sigma=10 (right).\n\n\n\nNote, both of these choices for the bandwidth are poor. A bandwidth of \\(\\sigma=0.1\\) gives a too strong concentration, with wildfires predicted with very high probability at the same locations as the observed fires. On the other hand, a bandwidth of \\(\\sigma=10\\) leads to the estimated intensity function being smoothed too much over space, with all areas of California predicted to have a similar risk of wildfires, which is in contrast to our previous findings.\nTornadoes across the United States\nLet’s return to the example in Figure @ref(fig:TORNADO). While there is a strong indication that the point process is non-homogeneous, we need to analyze the data in more detail. We start by again loading the data\n\nTornadoes &lt;- read.csv(\"data/tornadoes.csv\" )\n\nThere are a total of 68663 tornadoes in the data set. In the next Section 4.7 we will count the number of storms per state and visualize the extracted numbers. Here we focus on the states of Kansas and Texas and estimate the intensity function. For this, we first need to load a shapefile for the USA, which includes the state boundaries:\n\nUSA &lt;- read_sf( \"Data/gadm41_USA_shp/gadm41_USA_1.shp\" )\n\nWhen analyzing the California wildfire data, we created the ppp object using the largest polygon in the shapefile. However, this may not be ideal in all cases, in particular, when we are working with multiple islands. Here we introduce an alternative approach for constructing the ppp object, which performs better in cases where we want to include the full shapefile.\nThe first step is to extract the shapefiles for Kansas and Texas from the shapefile for the USA, reduce their complexity and then change the projection (we cannot use the WGS84 coordinate system here):\n\nKS &lt;- filter( USA, NAME_1==\"Kansas\" ) %&gt;% \n  st_simplify( dTolerance = 2000 )%&gt;% \n  st_transform( crs=3857 )\nTX &lt;- filter( USA, NAME_1==\"Texas\" ) %&gt;% \n  st_simplify( dTolerance = 2000 ) %&gt;% \n  st_transform( crs=3857 )\n\nNow we have to transform the observed locations of Tornadoes to the same coordinate system as the shapefiles:\n\nTornadoes_transformed &lt;- Tornadoes %&gt;%\n  st_as_sf( coords=c(\"Lon\",\"Lat\"), crs=4326 ) %&gt;%\n  st_transform( crs=3857 ) %&gt;% \n  st_coordinates( )\n\nFinally, we create the ppp objects for Kansas and Texas:\n\nTexas_ppp &lt;- ppp( Tornadoes_transformed[,1], Tornadoes_transformed[,2], \n                  window = as.owin(TX) )\nKansas_ppp &lt;- ppp( Tornadoes_transformed[,1], Tornadoes_transformed[,2],\n                   window = as.owin(KS) )\n\nRemark: The warning messages we received are due to not all tornadoes we observed being located in these states.\nHaving converted the data to a ppp object, we derive and plot the corrected kernel smoothed intensity function:\n\nlambdaC_KS &lt;- density.ppp( Kansas_ppp, edge=TRUE )\nplot( lambdaC_KS, main=\"Kansas\" )\nlambdaC_TX &lt;- density.ppp( Texas_ppp, edge=TRUE )\nplot( lambdaC_TX, main=\"Texas\" )\n\n\n\nCorrected smoothed kernel intensity for the occurrence of tornadoes across Kansas and Texas.\n\n\n\nWhile the intensity values are very small, there are clear conclusions we can draw from the plots. For Texas, the intensity is much higher in the north and east than along the border to Mexico. So it’s reasonable to conclude that the point process describing tornadoes across Texas is non-homogeneous. When comparing the results of Texas and Kansas, the peak intensities are comparable, but Kansas shows less variance in the values of the intensity function. Consequently, we may argue that the point process describing the locations of tornadoes across Kansas is homogeneous.\nImportant: When discussing whether a point process is homogeneous or not, we also have to consider whether the locations of points depend on each other. For instance for wildfires, this may correspond to the probability of a wildfire being dependent on whether wildfires occurred recently and in close proximity. It’s presumably fine to assume that tornadoes occur independently of each other, and thus our conclusions are reasonable. However, in other cases we may be unable to make such conclusions as the techniques we covered in this course do not allow us to distinguish between whether (a) the point process is non-homogeneous and (b) the point process is homogeneous but dependent. In these cases we need to account for the context of the study.",
    "crumbs": [
      "Lecture Notes",
      "Spatial Data Analysis"
    ]
  },
  {
    "objectID": "lecture_notes/04-SpatialDataAnalysis.html#visualizing-lattice-data",
    "href": "lecture_notes/04-SpatialDataAnalysis.html#visualizing-lattice-data",
    "title": "Spatial Data Analysis",
    "section": "Visualizing lattice data",
    "text": "Visualizing lattice data\nWhen working with lattice data we have to plot values for a fixed number of areas instead of a fixed number of points. These areas are usually defined by shapefiles. We will consider two examples to demonstrate how to visualize lattice/areal data using shapefiles.\nPopulation density across London boroughs\nSuppose we have observations which refer to the different boroughs of London, such as average income for each borough. Let’s load the shapefiles for the boroughs (and the City of London):\n\nLondon &lt;- read_sf(\"Data/London.shp\")\nclass( London )\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWe see that the object London consists of: (1) a sf part which includes the data to produce the shapefiles and (2) a data frame to store the information for each borough. The information provided in the data frame includes, for instance, area size in hectares.\nWe want to explore and visualize the population density in 2020. For this we import the data on the number of residents in 2020 for each borough provided in the file “London.csv”. We load the population data and create a new variable which represents population density in the data frame of the object London\n\nLondon_population &lt;- read.csv(\"data/london.csv\" )\nLondon_population &lt;- London_population %&gt;% \n  mutate( Population = as.numeric( gsub(\"\\\\,\",\"\",Population) ) )\nLondon &lt;- inner_join( x=London, y=London_population, by=c(\"NAME\"=\"Borough\") )\nLondon &lt;- London %&gt;% mutate( Density = Population / HECTARES )\n\nTo visualize the calculated population densities, we have two options:\n\nUse color as a visual cue\nPlace points within the boroughs, with their size depending on the density\n\nLet’s create both options:\n\nggplot( data=London, aes(fill=Density) ) + geom_sf() + theme_bw() +\n  scale_fill_distiller( palette=\"Reds\", trans=\"reverse\" ) + \n  theme( axis.title=element_text(size=15), axis.text=element_text(size=15) ) + \n  labs( x=\"Longitude\", y=\"Latitude\" )\n\n\n\nPopulation density in people per hectare for the 33 London boroughs.\n\n\n\n\nggplot( data=London ) + geom_sf() + theme_bw() + \n  geom_point( data=st_centroid(London), aes( size = Density, geometry = geometry), \n              stat = \"sf_coordinates\" ) +\n  labs( x=\"Longitude\", y=\"Latitude\" ) + \n  theme( axis.title=element_text(size=15), axis.text=element_text(size=15) )\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\nPopulation density for the 33 London boroughs, with size of points used as visual cure.\n\n\n\nWe see that Central London (with the exception of the City of London) records the highest population densities. The outer boroughs tend to have a population density between 40 and 80 people per hectare.\nRemark: When we download shapefiles from gadm.org, we can get the different administrative levels and assign values in the same way. We will demonstrate this in the next example.\nTornadoes across the United States\nWe want to visualize the number of tornadoes for each US state in the continental United States. Let’s load the data from “Tornadoes.csv” and extract the number of tornadoes for each state between 1950 and 2021:\n\nTornadoes &lt;- read.csv(\"data/tornadoes.csv\" )\nTornadoesNumbers &lt;- Tornadoes %&gt;% count( State )\n\nWe then load the shapefile and combine it with the extracted number of tornadoes:\n\nUSA &lt;- read_sf(\"Data/gadm41_USA_shp/gadm41_USA_1.shp\")\nUSA &lt;- st_simplify( USA, dTolerance = 5000, preserveTopology = TRUE )\nUSA$HASC_1 &lt;- gsub( \".*\\\\.\", \"\", USA$HASC_1)\nUSA &lt;- inner_join( USA, TornadoesNumbers, by=c(\"HASC_1\"=\"State\") ) \n\nRemark: The third line of code is necessary because the states in TornadoesCount are represented as “TX”, “KS”, etc., but these abbreviations do not exist in USA. Therefore, we change the entries HASC_1 column to use it for combining the data frames.\nFor this analysis we Alaska and Hawaii from the analysis as we have no data for it. The observations can be removed using the filter function and we rename some of the variables:\n\nUSA &lt;- USA %&gt;% \n  filter( !NAME_1 %in% c(\"Alaska\", \"Hawaii\") ) %&gt;%\n  rename( State = NAME_1, Number=n )\n\nWe can now plot the data as we did in the example considering the data for London. We make one addition and also add a map to the plot using the ggspatial R package:\n\nggplot( USA, aes(fill=Number) ) + theme_bw() + \n  annotation_map_tile( zoom=5 ) + geom_sf() +\n  scale_fill_distiller( palette=\"Reds\", trans=\"reverse\" ) + \n  labs( fill=\"Number\", x=\"Longitude\", y=\"Latitude\" ) +\n  theme( axis.title=element_text(size=15), axis.text=element_text(size=15) )\n\n\n\nNumber of tornadoes for each state in the continential US for 1950-2021.\n\n\n\nUsing the map we conclude that the highest number of tornadoes occur in Texas, Oklahoma, Kansas and Florida, while the Western United States observes a relatively small number of tornadoes.",
    "crumbs": [
      "Lecture Notes",
      "Spatial Data Analysis"
    ]
  },
  {
    "objectID": "lecture_notes/04-SpatialDataAnalysis.html#summary",
    "href": "lecture_notes/04-SpatialDataAnalysis.html#summary",
    "title": "Spatial Data Analysis",
    "section": "Summary",
    "text": "Summary\nIn this chapter we considered the analysis of spatial data:\n\nThere are three types of spatial data: point-referenced, point pattern and lattice\nAll types of spatial data can be visualized using shapefiles (using the sf package) and/or maps (using ggspatial). In these cases we often have to resort to colour as a visual cue.\nWhen the data covers a large spatial area/region, we should carefully select the projection we use for our shapefiles\n\nWhen working with point-referenced data, we can\n\nEmploy inverse distance weighting to make predictions at unobserved sites\nEstimate the semi-variogram or perform principal component analysis to analyse spatial dependence\nUse principal component analysis to analyze the spatial structure in the data\n\n\nQuadrat counting and kernel intensity function for analyzing point pattern data;\n\nThese techniques allow us to explore features of the spatial data and are essential for developing models for spatial data - these models are however too complex to be fully considered in a Year 2 unit. Some of them will be covered in MA32024 Statistical Modelling and Data Analytics 3B.",
    "crumbs": [
      "Lecture Notes",
      "Spatial Data Analysis"
    ]
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Lecture slides are available below.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nAnalysis of German Temperature Data\n\n\nJul 3, 2024\n\n\n\n\n\n\nVisualizing population density across London boroughs\n\n\nFeb 4, 2025\n\n\n\n\n\n\nAnalysis of Tornado Data\n\n\nFeb 4, 2025\n\n\n\n\n\n\nAnalysis of Wildfires\n\n\nFeb 4, 2025\n\n\n\n\n\n\nMA22019 - Overview\n\n\nFeb 5, 2025\n\n\n\n\n\n\nMA22019 - Data Cleaning and Wrangling\n\n\nFeb 5, 2025\n\n\n\n\n\n\nMA22019 - Combining data frames\n\n\nFeb 12, 2025\n\n\n\n\n\n\nAnalysis of Australian weather data\n\n\nFeb 12, 2025\n\n\n\n\n\n\nMA22019 - Data Visualization (Part 1)\n\n\nFeb 12, 2025\n\n\n\n\n\n\nMA22019 - Data Visualization (Part 2)\n\n\nFeb 19, 2025\n\n\n\n\n\n\nMA22019 - Coursework 1 Q&A\n\n\nFeb 26, 2025\n\n\n\n\n\n\nMA22019 - Text Data Analysis (Part 1)\n\n\nMar 5, 2025\n\n\n\n\n\n\nMA22019 - Text Data Analysis (Part 2)\n\n\nMar 12, 2025\n\n\n\n\n\n\nMA22019 - Spatial Data Analysis (Part 1)\n\n\nMar 19, 2025\n\n\n\n\n\n\nMA22019 - Spatial Data Analysis (Part 2)\n\n\nMar 26, 2025\n\n\n\n\n\n\nMA22019 - Spatial Data Analysis (Part 3)\n\n\nApr 2, 2025\n\n\n\n\n\n\nMA22019 - Revision Class\n\n\nApr 23, 2025\n\n\n\n\n\n\nAnalysis of NRFA Data\n\n\nMay 2, 2025\n\n\n\n\n\n\nAnalysis of Tuscany Data Set\n\n\nMay 2, 2025\n\n\n\n\n\n\nComparison of Jane Eyre and Wuthering Heights\n\n\nMay 3, 2025\n\n\n\n\n\n\nAnalysis of Jane Eyre\n\n\nMay 3, 2025\n\n\n\n\n\n\nAnalysis of NRFA Data\n\n\nDec 2, 2025\n\n\n\n\n\n\nAnalysis of the work by Charles Dickens\n\n\nDec 3, 2025\n\n\n\n\n\n\nAnalysis of NYT Articles\n\n\nDec 3, 2025\n\n\n\n\n\n\nChanging scales - Example\n\n\nJul 2, 2026\n\n\n\n\n\n\nSwitching Data Formats Example\n\n\nJul 2, 2026\n\n\n\n\n\n\nAnalysis of wind in Bela Vista\n\n\nJul 2, 2026\n\n\n\n\n\n\nAnalysis of NYT Articles\n\n\nNov 4, 2026\n\n\n\n\n\n\nAnalysis of Monthly Precipitation in Colorado\n\n\nFeb 3, 2027\n\n\n\n\n\n\nAnalysis of Sea Surface Temperature Anomalies\n\n\nFeb 3, 2027\n\n\n\n\n\n\nMA22019 2025 - Revision Class\n\n\n \n\n\n\n\n\n\nMarking of homework questions - example\n\n\n \n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Homework & Quizzes",
    "section": "",
    "text": "Homework assignments and quizzes.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nSubtitle\n\n\n\n\n\n\n\n\nHomework 2\n\n\nMise en place\n\n\n\n\n\n\nHomework 3\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Homework 5\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Homework 6\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Homework 7\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Homework 8\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Homework 9\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Quiz 2\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Quiz 4\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Quiz 5\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Quiz 6\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Quiz 7\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Quiz 8\n\n\nMise en place\n\n\n\n\n\n\nMA22019 2025 - Quiz 9\n\n\nMise en place\n\n\n\n\n\n\nQuiz 3\n\n\nMise en place\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Homework & Quizzes"
    ]
  },
  {
    "objectID": "lecture_notes/02-DataVisualization.html",
    "href": "lecture_notes/02-DataVisualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "In Chapter 1 we explored how to restructure a data set and extract summaries. In this chapter we will focus on data visualization, i.e., the creation and interpretation of plots that give us further insight into the data. Data visualization is not just important for data exploration, but also for presenting and communicating results.\nThere are two very important aspects we need to keep in mind:\n\nEffective data visualization is more about clear communication than creating impressive plots. Your analysis may be excellent, but it won’t attract any attention if you cannot convey your results effectively.\nPlots support our arguments and/or highlight the reason for our conclusion. As such we should interpret plots in the context of the research question and not just provide a plot as the answer.\n\nWe start by introducing a general framework for describing data graphics in Section 2.1. Sections 2.2 and 2.3 then demonstrate how to use the R package ggplot2 for data visualization. Finally, some further aspects are considered in Section 2.4.\nRemark: The methods and techniques considered in this chapter cover general aspects of data visualization. Specific methods for illustrating text and spatial data are left to the next chapters.\nImportant: Data visualization is to some degree subjective, because there is often not just one way to visualize the data. However, you should follow the principles outlined in this chapter, and your conclusions need to be supported by your plot. Do not claim something that is not clearly visible in your output!\n\nBefore starting to create plots in R, we establish a framework to analyze plots in terms of four basic elements: visual cues, coordinate system, scale and context. Understanding these elements will help us with producing our own plots later.\n\nVisual cues are graphical elements that draw the audience to the aspects we want them to focus on. The book “Data points: Visualization that means something” by Nathan Yau (link provided on Moodle) lists nine distinct visual cues to encode a category or quantity:\n\nPosition (quantity) - relation to other things\nLength (quantity) - size in one dimension\nAngle (quantity) - width of angle may, for instance, represent proportions (pie chart)\nDirection (quantity) - slope of line\nShape (category) - which observations are in the same group\nArea (quantity) - size in two dimensions\nVolume (quantity) - size in three dimensions\nShade (quantity or category) - shade in comparison to others, or grouping\nColour (quantity or category) - colour in comparison to others, or grouping\n\nResearch has shown that our ability to perceive differences in magnitude descends in this order. One of many publications supporting this argument is “Graphical perception: Theory, experimentation, and application to the development of graphical methods.”, which you can find on Moodle.\nImportant: One crucial conclusion is that we should not rely too much on colour. Many people have colour deficiencies, which makes it very hard for them to distinguish certain colours. Consequently, before using colour, we should consider whether we could use shapes or shades instead.\nRemark: In this course we focus on creating 2D graphics. While 3D plots and animations allow us to visualize a larger number of variables (and you may think they look more impressive), I would avoid using such plots except for a very limited number of cases. That’s because it is often difficult to see the exact positions of the points.\n\nThe data set “Freediving Records.csv” provides information on the progression of the world record in multiple disciplines for men and women. We will focus on the discipline “dynamic apnea with fins (DYN)” and visualize how the world records for men and women have progressed over time:\n\n\n\n\nDevelopment of the world record in dynamic apnea with fins (DYN) for men and women between 1993 and 2020.\n\n\n\nWe can identify that the following visual cues have been used:\n\nShape and colour indicate whether the observations refer to the world record for men or women. Note, there is no issue with using multiple cues for the same information.\nLength of line is used to convey two pieces of information: (i) the length of the line in horizontal direction indicates the time it took until the world record was broken, and (ii) the length of the line in vertical direction represents the magnitude of improvement.\nPosition is used to compare the world records for men and women (both lines are provided in the plot).\nShape is used to highlight the times when a world record was broken - this makes it is easy to count the number of times the record was broken. Without this visual cue, such information would be much harder to extract from the plot.\n\nChoosing a suitable coordinate system is critical to present the data accurately and in a meaningful way. The three most common coordinate systems used in data science are\n\nCartesian:\n\nOur familiar (\\(x,y\\))-rectangular coordinate system with two perpendicular axes.\n\nPolar:\n\nPoints are identified by their radius \\(r\\) (distance from the origin) and angle \\(\\theta\\). A point (\\(x,y\\)) in Cartesian coordinates can be transformed to polar coordinates using the relationship [ (x,y) = (r, r), r=. ]\n\nGeographical:\n\nIn the chapter on spatial data analysis, we will work with points located across the earth, with their positions being defined by longitude and latitude.\n\n\nThere are two common cases when we may want to consider polar coordinates:\n\nPie charts\nVariables which naturally lie on a circle, such as wind direction.\n\nTip: Since Position and Length are better visual cues than Angle, we should prefer Cartesian coordinates to polar coordinates. For instance, before using pie charts, consider whether the same information can be displayed effectively in a bar plot. However, we will see some examples where polar coordinates may be considered the better choice.\n\nHourly weather data was collected for Bela Vista, Brazil, for 2017 and 2018. Suppose we were asked to explore the distribution of wind direction and the relation between wind direction and speed of wind gusts.\nTo analyse the distribution of wind direction, we may create a histogram of the collected data. Let’s compare the histograms obtained for the Cartesian and polar coordinate systems (both display frequency):\n\n\n\n\nFrequency of wind direction for Bela Vista, Brazil, in 2017 and 2018 displayed using Cartesian coordinates (left) and polar coordinates (right).\n\n\n\nBoth plots, Cartesian (left) and polar (right), show that the wind predominately comes from a north-easterly direction. The right plot is a tick nicer, because it better handles that \\(0^{\\circ}=360^{\\circ}\\) in terms of wind direction, which is recorded as an angle.\nLet’s turn to analyzing the relation between wind direction and speed of wind gusts. For this we create plots which map wind direction against the speed of the wind gusts, again using different coordinate systems:\n\n\n\n\nWind direction versus speed of wind gust for Bela Vista in 2017 and 2018 displayed using Cartesian coordinates (left) and polar coordinates (right).\n\n\n\nWhat can we conclude about the relation between wind direction and speed of wind gusts?\nI personally prefer the left plot because it indicates the difference in frequency of wind directions. The right plot (using polar coordinates) does not provide this information and it seems almost as if wind direction is uniformly distributed - so we would need the histogram in Figure @ref(fig:BVWind1) to correctly interpret the plot.\n\nThe concepts of scale and context refer to the choice of axes and the labeling of the data graphic respectively.\n\nScales allow us to translate values into visual cues by influencing, for instance, the distance (length) between points in a scatter plot. When choosing scales, we have to consider how the displayed distances translate into meaningful differences. Each coordinate axis can have its own scale, and we have three different choices:\nNumerical: Numeric quantities, such as speed, age, etc., are commonly set on a linear, logarithmic or percentage scale.\nCategorical: Categorical variables may have no ordering (political parties), or be ordinal (restaurant ratings). Ordinal variables differ from numeric quantities in that distances are unknown (or not meaningful). For instance, a first class degree is better than a 2.1, but what does this mean in terms of distance?\nTime: While being a numeric quantity, time has some special properties: 1) it can be demarcated by year, month,.. and 2) it can be considered periodical. This second aspect can be highlighted using polar coordinates as seen in Section 2.1.4.\n\nData graphics are provided to aid the reader/viewer with making meaningful comparisons. Context can be added in the form of titles, subtitles and axis labels that explain what is being shown, including the scales and units. It sometimes also helps to include reference points or lines. While we should avoid cluttering or providing excessive annotations, it is necessary to provide proper context.\n\nThe following two plots both illustrate the relation between body weight and brain weight for several mammals, but they use different scales:\n\n\n\n\nBody weight vs brain weight for 62 mammals on linear scale (left) and logarithmic scale (right).\n\n\n\nWe make two observations:\n\nLinear scales were used in the left plot, while logarithmic scales were used in the right plot - this context was provided via the title.\nThe right plot is more informative as it shows a linear relationship between body and brain weight on logarithmic scale. This aspect is not clear in the left plot, and a viewer may focus on the three mammals with the highest brain weight as they appear to be quite different from the rest.\n\nAfter outlining the different elements of a plot in the previous section, we now explore how we can create informative plots using the ggplot2 R package\n\nlibrary( ggplot2 )\n\nWe will introduce and utilize a wide range of plots to analyze historical weather data for five Australian cities. The data is provided on Moodle as “WeatherAustralia.csv” and we start by loading the data and looking at the first few entries:\n\nAUS &lt;- read.csv(\"data/weatheraustralia.csv\" )\nslice_head( AUS, n=5 )\n\n        Date Location MinTemp MaxTemp Rainfall WindGustSpeed\n1 01/01/2009   Sydney    17.7    35.1        0            72\n2 02/01/2009   Sydney    18.5    23.0        0            63\n3 03/01/2009   Sydney    16.9    23.2        0            NA\n4 04/01/2009   Sydney    18.7    27.1        0            65\n5 05/01/2009   Sydney    20.2    31.6        0            63\n\n\nWe see that the data provide daily information on minimum and maximum temperature, amount of rainfall and wind speed. Further, the data also include some missing values, as highlighted by the \\(\\mathrm{\\texttt{NA}}\\) entries.\nIn the following, we perform a data exploration which includes the creation and interpretation of multiple plots. At the end of the analysis, you will be familiar with the general functionality of the ggplot2 package. Further types of plots are listed on the ggplot2 cheat sheet (provided on Moodle). I would advise you to try creating some of them for practice, as they may be useful.\nImportant: In this section we focus on the different types of plots and visual cues. The other elements described in Section 2.1, coordinate system, scale and context, will be considered in Section 2.3.\n\nThe first step is to call the function ggplot() and we usually specify the following inputs:\n\nThe name of the data frame that contains the data we want to plot\nThe names of the variables which specify the axes. This is done using the function aes(), which refers to aesthetics (we will see this very often).\n\nSuppose we want to create a scatter plot of minimum against maximum temperature for the five Australian cities. To initialize the plot, we specify\n\nPlotAUS &lt;- ggplot( AUS, aes( x=MinTemp, y=MaxTemp ) )\n\nHowever, if we call PlotAUS in R, we will see a Cartesian coordinate system, but no points. This is because the coordinate system and visual cues are specified using separate functions in ggplot2. We will consider in the next subsection how to add points as an additional layer.\nRemark: While it is important to use meaningful variable names when programming, I usually make an exception when using ggplot2. For instance, instead of PlotAUS, we could also call it g (for graphic) - I will later use this for brevity.\n\nThe function geom_point() adds the points to the coordinate system created by ggplot(). Returning to the Australian weather data, we generate the scatter plot of daily minimum and maximum temperature using\n\nPlotAUS + geom_point()\n\n\n\nMinimum versus maximum observed daily temperature for five Australian cities between 2008 and 2017.\n\n\n\nWe observe a positive correlation between minimum and maximum daily temperature. However, this plot does not highlight that the data are coming from five different cities. In other words, the plot does not allow us to explore differences between the cities.\nOne possible solution is to use the visual cues shape and colour to indicate to which city a data point belongs. Specifically, we specify via aes() that the variable Location should be used to determine the shape and colour of the points created by the geom_point() function:\n\nPlotAUS + geom_point( aes( shape=Location, color=Location ) )\n\n\n\nMinimum versus maximum observed daily temperature for five Australian cities between 2008 and 2017, with the city being highlighted using the visual cues of shape and colour.\n\n\n\nRemark: If we wanted to use shade, we would have specified \\(\\mathrm{\\texttt{aes( alpha=Location )}}\\).\nWhile our plot now makes clear that the data come from different cities, it is still very cluttered. This makes it hard to draw conclusions on differences between the five Australian cities. Luckily, there is a better option available, which we consider next.\n\nInstead of creating a single plot which contains all points, it seems a nice idea to have a separate plot for each city. In other words, we want to split the data using the variable Location and create one plot per subset.\nThis type of plot is called a facet plot and it provides a simple and effective way to display the data for the separate levels of a categorical variable.\nTo create the facet plot, we use the function facet_wrap() and change the axes labels using the labs() function:\n\nPlotAUS + geom_point() +\n  facet_wrap( ~Location ) +\n  labs( x=\"Minimum daily temperature\", y=\"Maximum daily temperature\" )\n\n\n\nPlots of minimum versus maximum daily temperature between 2008 and 2017 for five Australian cities.\n\n\n\nOur plot now reveals that Darwin is quite different from the other four cities in terms of the distribution of minimum and maximum daily temperature, in particular, in terms of the range of values.\nWhat may be the reason for this difference? What else can we conclude?\n\nAs for the river flow data in Section [1.1.4][Example: Loading and cleaning NRFA river flow data], we may want to explore how a variable changes over time for one of the five cities.\nScatter plots are often not the right choice for such a task, because it’s hard to see patterns in the data. Instead we create a line plot, where the length of the line connecting consecutive dates illustrates the magnitude of change in the values of the variable.\nLet’s see how we can create a line plot for daily maximum temperature over time for Darwin. We start by converting the variable Date in the data frame to the correct type,\n\nAUS &lt;- AUS %&gt;% mutate( Date = as_date( Date, format=\"%d/%m/%Y\" ) )\n\nThe next step is to extract the subset of observations for Darwin, which we store in a separate data frame called Darwin:\n\nDarwin &lt;- filter( AUS, Location == \"Darwin\" )\n\nA line plot is then created by using the geom_line() function, again by first defining the axes in the ggplot() function:\n\nggplot( Darwin, aes( x=Date, y=MaxTemp ) ) + geom_line() + \n  labs( x=\"Date\", y=\"Maximum daily temperature\" )\n\n\n\nTime series plot of daily maximum temperature for Darwin between 2008 and 2017.\n\n\n\nTo create such a (time series) plot for all five cities, we again use facet_wrap(), and we use colour and shape (line type) to highlight the different cities:\n\nggplot( AUS, aes( x=Date, y=MaxTemp ) ) + facet_wrap( ~Location ) + \n  geom_line( aes( color=Location, linetype=Location ) ) + \n  labs( x=\"Date\", y=\"Maximum daily temperature\" )\n\n\n\nTime series plot of daily maximum temperature for five Australian cities between 2008 and 2017.\n\n\n\nThe gap in the plot for Melbourne is due to the data being missing for this time period. One conclusion we draw from the plot is that the maximum daily temperature is less varied for Darwin than the other cities, which exhibit a clear seasonal pattern.\nRemark 1: To draw steps as in Section 2.1.2, we would use geom_step() instead of geom_line().\nRemark 2: The use of colour and shape in Figure @ref(fig:DateTempFacet) is not really necessary. We just did it do illustrate that visual cues can be used together with facet_wrap().\n\nWe now want to investigate the distribution of a single variable, such as the speed of the wind gusts in one (or more) of the cities. Histograms and density plots are useful in such cases. You already created histograms in Year 1 Probability & Statistics and we analyzed such a plot in Section 2.1.4. We start by producing a histogram using ggplot2 and then introduce the density plot in more detail.\n\nThe ggplot2 package provides the function geom_histogram() to create a histogram. As for the hist() function you used before, we have to set a suitable number of bins, and this is done using the bins option inside the geom_histogram() function.\nLet’s return to the data for Darwin. To create a histogram for wind speed gusts, we have to call ggplot() and geom_histogram(), but we now only specify the x-axis:\n\nggplot( Darwin, aes( x=WindGustSpeed ) ) + geom_histogram( bins=20 ) + \n  labs( x=\"Speed of wind gust in km/h\", y=\"Count\" )\n\n\n\nHistogram of the speed of wind gusts for Darwin between 2008 and 2017.\n\n\n\nWhat could be a sensible distribution for the speed of wind gusts, based on the histogram?\n\nA density plot is pretty much a smoothed version of the histogram. Given data \\(x_1,\\ldots,x_n\\), we define the estimate \\(\\hat{f}_X(\\cdot)\\) for the probability density function \\(f_X(\\cdot)\\) of the random variable \\(X\\) as \\[\\begin{equation}\n\\hat{f}_X(x) = \\frac{1}{nh}\\sum_{i=1}^{n} K\\left(\\frac{x-x_i}{h}\\right), \\qquad x\\in\\mathbb{R},\n(\\#eq:KDE)\n\\end{equation}\\] where \\(K(\\cdot)\\) is termed the kernel and \\(h\\) is called the bandwidth. The kernel is a non-negative probability density function. One common choice is to set \\(K(\\cdot)\\) as the density of the standard normal distribution, with [ X(x) = {i=1}^{n}{-}. ]\nIn principle, the bandwidth \\(h\\) has to be set by us. If \\(h\\) is too small, the density plot will look very jittered, while a too large \\(h\\) will obscure the underlying data structure. Consequently, care should be taken when setting the bandwidth.\nRemark 1: When generating density plots in R, a suitable value for \\(h\\) is often provided automatically. However, we sometimes have to set the bandwidth manually.\nRemark 2: It is usually sufficient to provide either the histogram or the density plot, because they visualize similar aspects of the data. Note, the histogram should be used for discrete data.\nIn ggplot2, geom_density() is used to create density plots and, as for geom_histogram(), we only specify the x-axis. Let’s use this type of plot to compare the distributions of the speed of wind gusts for Darwin and Adelaide. To aid the comparison, we place the estimates for the two cities in the same plot:\n\nWeatherAD &lt;- filter( AUS, Location %in% c(\"Adelaide\",\"Darwin\") )\nggplot( WeatherAD, aes( x=WindGustSpeed ) ) +\n  geom_density( aes( linetype=Location, color=Location ), size=1.2 ) +\n  labs( x=\"Speed of wind gust in km/h\", y=\"Density\" )\n\n\n\nDensity plots of the speed of wind gusts for Adelaide (solid) and Darwin (dashed) for 2008–2017.\n\n\n\nThe two estimated curves indicate that Adelaide and Darwin differ in the distribution of wind gust speeds. For instance, Adelaide observes wind gust speeds below 30 km/h more often than Darwin.\nRemark 1: The option \\(\\mathrm{\\texttt{size=1.2}}\\) in geom_density() increases the thickness of the line in the plot - you can set size to any positive value, and higher values will give thicker lines. If you wanted to specify the bandwidth \\(h\\) in @ref(eq:KDE), you have the option \\(\\mathrm{\\texttt{bw=..}}\\).\nRemark 2: Density plots can sometimes be misleading because they are a smoothed version of the histogram. Although Adelaide and Darwin appear to have a similar density in Figure @ref(fig:DAWind) when it comes to high wind speeds, the highest value for Adelaide is 86 km/h, while it is 126 km/h for Darwin. Consequently, we should be cautious when drawing conclusions about the distribution tails based on density plots.\n\nBox plots and violin plots are useful for comparing a number of distributions. For instance, we may want to compare the distributions of wind gust speed for the five Australian cities. While we could produce a separate density plot for each city, box and violin plots tend to be a better choice.\nThe two types of plot differ in terms of the information they provide:\n\nBox plots visualize the median, interquartile range (25% and 75% quantile) and outliers.\nViolin plots visualize the density estimate \\(\\hat{f}_X(\\cdot)\\) as defined in Equation @ref(eq:KDE).\n\n\nIn ggplot2, box plots are created using the geom_boxplot() function. Let’s use this type of plot to compare the wind gust speeds across the five cities. We set the x-axis to the categorical variable Location, and the y-axis as speed of wind gust:\n\nggplot( AUS, aes( x=Location, y=WindGustSpeed ) ) + \n  geom_boxplot( aes( fill=Location ) ) + \n  labs( y=\"Speed of wind gust in km/h\" )\n\n\n\nBox plots of the speed of wind gusts for five Australian cities between 2008 and 2017.\n\n\n\nWe can extract a few details about the distributions of the speed of wind gusts for the five cities. For instance, when considering the median, Melbourne and Sydney record the strongest wind gusts, while Adelaide and Perth record the lowest wind gust speeds.\nRemark 1: The option \\(\\mathrm{\\texttt{fill=Location}}\\) gives each box plot a different colour.\nRemark 2: If we wanted a single box plot for a variable, we would only specify one of the axes.\n\nTo conclude our analysis, we compare the distribution of daily wind gust speeds across the five Australian cities using violin plots. This type of plot is generated using the geom_violin() function, and the syntax is the same as for geom_boxplot(). We make two additions compared to Figure @ref(fig:Box) though:\n\nA plot title is set using the labs() function.\nIt’s good practice to order box and violin plots, for instance, based on the median of the different subgroups. This is achieved using the reorder() function.\n\nPutting everything together, the five violin plots are generated using\n\nggplot( AUS, aes( x=reorder(Location, WindGustSpeed, median, na.rm=TRUE), \n                  y=WindGustSpeed ) ) + geom_violin() + \n  labs( x=\"Location\", y=\"Wind gust speed in mph\", \n        title=\"Wind gust speed across five Australian cities\" )\n\n\n\nViolin plots of wind speed across five cities in Australia. The cities are ordered in ascending order in terms of their median wind gust speed.\n\n\n\nRemark: The four components within reorder() should be read as follows: (i) We want to reorder the values in Location, (ii) which should be based on the variable WindGustSpeed, (iii) the value to be used for the reordering is the median and (iv) ignore any missing values when calculating the median.\nBy ordering the violin plots, we can directly draw the conclusion that Adelaide has the lowest median wind gust speed, while Sydney has the highest. There are also other conclusions that are worth pointing out, such as the differences in terms of highest observed wind gust speeds.\n\nIn the previous section we generated plots using the structure [ + _. ] and this allowed us to create several type of plots and to use multiple visual cues. The following table lists the types of plots we explored so far (plus the functions for generating bar plots):\n\n\n\n\n\n\n\nType of plot\n\\(\\mathrm{\\texttt{&lt;GEOM}}\\_\\mathrm{\\texttt{FUNCTION&gt;}}\\)\nVisual cues we may use\n\n\n\nScatter plot\ngeom_point\nshape, shade, colour, size\n\n\nLine plot\ngeom_line\nshade, colour, linetype, size\n\n\n\ngeom_step\nshade, colour, linetype, size\n\n\nBar plot\ngeom_bar\nshade, colour, size\n\n\n\ngeom_col\nshade, colour, size\n\n\nHistogram\ngeom_histogram\nshade, colour, linetype\n\n\nBox plot\ngeom_boxplot\nshade, colour\n\n\nDensity plot\ngeom_density\nshade, colour, linetype\n\n\nViolin plot\ngeom_violin\nshade, colour, linetype\n\n\n\n\nWe also covered a few additional aspects, such as using facet_wrap() to create facets and labs() to specify the axes labels. In this section we consider the remaining plot elements described in Section 2.1:\n\nChanging the coordinate system to polar coordinates\nChanging the scale of the x-axis (y-axis)\n\nWe will also explore how to change the font size of the labels and the colour scheme in our plot. This will give us a lot of flexibility to visualize data.\n\nWe saw in Section 2.2 that ggplot() uses the Cartesian coordinate system by default. In Section 2.1.3 we highlighted that we sometimes want to use polar coordinates, for instance, when displaying wind direction. The coord_polar() function in ggplot2 allows us to switch from Cartesian to polar coordinates. Let’s consider two cases where such a transformation may be useful.\n\nIn Figures @ref(fig:BVWind1) and @ref(fig:BVWind2) we used plots with polar coordinates to explore wind direction and speed for Bela Vista, Brazil. We can reproduce these plots in ggplot2:\n\nlibrary(patchwork) # required to place plots next to each other\nwind_BV &lt;- read.csv(\"data/wind_bela_vista.csv\" )\n\ng1 &lt;- ggplot( wind_BV, aes( x=Wind.Direction ) ) + \n  geom_histogram( bins = 120 ) + coord_polar() + \n  labs( x=\"Wind Direction\" )\ng2 &lt;- ggplot( wind_BV, aes( x=Wind.Direction, y=Gust.Speed ) ) +\n  geom_point() + geom_smooth() + coord_polar( theta=\"x\" )  + \n  labs( x=\"Wind Direction\", y=\"Wind gust in m/s\" )\ng1 + g2\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\nHistogram illustrating the frequency of wind direction (left) and scatter plot of wind direction vs wind speed (right) for Bela Vista, Brazil. Both plots use polar coordinates. The solid line in the right plot shows the average wind speed for each wind direction.\n\n\n\nThe option \\(\\mathrm{\\texttt{theta=\"x\"}}\\) in coord_polar() specifies that the angle should be defined by the x-axis. We do not have to do anything else - ggplot2 is doing all the calculations for us.\nRemark 1: The R package patchwork makes it easy to place two graphics created by ggplot2 next to each other by using the + sign. There are more options available - have a look at the documentation for the package.\nRemark 2: The example also shows that we can use multiple geom_..() functions in the same plot. Here, we used geom_smooth() to add a line representing the average wind speed in each direction to the plot. However, we note that the line does not match up at \\(0^{\\circ}\\). This is not particularly elegant, but there is not a very easy fix to this.\nAre there any other plots we should create to explore the distribution of wind direction and speed of wind gusts at Bela Vista, Brazil?\n\nIn a pie chart we use the size of the angle as a visual cue and such a plot may be useful to visualize proportions. The following R code illustrates how to create a pie chart using ggplot2:\n\ndf &lt;- data.frame( \"prob\" = c(0.3,0.4,0.3), \"group\" = c(\"A\",\"B\",\"C\") )\nggplot( df, aes( x=\"\", y=prob, fill=group ) ) + \n  geom_col() + coord_polar( theta=\"y\" ) + labs( x=\"\", y=\"\" )\n\n\n\nExample of a pie chart with three slices.\n\n\n\nThe code first creates a plot on Cartesian coordinates - a stacked bar plot in this case - which is then converted to polar coordinates. To see this, you may want to have a look at the plot you obtain when removing coord_polar() from the code above.\n\nWe saw in Section 2.1.5 that changing the scale of one (or more) variables in a plot can help with the data analysis. In applications, transformations are often used to reduce the influence of extreme outliers in the plot. For instance, by considering logarithmic scales in Section 2.1.5, the three mammals with very high brain weight in the data appeared less extreme, and we were instead able to see the possible linear relationship on logarithmic scale between brain and body weight.\nWhen using ggplot(), a linear scale is used by default. The function coord_trans() enables us to change the scale of the x-axis or y-axis. The most common transformations are logarithmic (\\(\\mathrm{\\texttt{x=\"log\"}}\\) or \\(\\mathrm{\\texttt{x=\"log10\"}}\\)) and square root (\\(\\mathrm{\\texttt{x=\"sqrt\"}}\\)).\n\nThe data in “Facebook.csv” contains data related to messages posted by a few Olympic athletes. We want to explore the relation between number of likes (received for individual posts) and the number of followers of the athlete. Let’s produce two plots, one with linear scales and one with logarithmic scales:\n\nFacebook &lt;- read.csv(\"data/facebook.csv\", header=TRUE )\n\ng  &lt;- ggplot( Facebook, aes( x=follow, y=postlikes ) ) + \n  labs( x=\"Number of Followers\", y=\"Number of likes\" )\ng1 &lt;- g + geom_point() \ng2 &lt;- g + geom_point() + coord_trans( x=\"log\", y=\"log\" )\ng1 + g2\n\n\n\nNumber of followers vs number of likes on linear scale (left) and logarithmic scale (right) for Facebook posts by a group of Olympic athletes.\n\n\n\nWe see that the two plots provide quite different information: The graphic with log-transformed scales shows a possible linear relationship between log(number of followers) and log(number of likes), which is not visible when considering the plot with linear scales.\nRemark: If you want to use another transformation than logarithmic or square root, you can specify it within ggplot. For instance, we could use\n\nggplot( Facebook, aes( x=log(follow), y=log(postlikes) ) ) + geom_point() +\n  labs( x=\"log(Number of Followers)\", y=\"log(Number of likes)\" )\n\nYou have to be aware that the numbers along the axes differ in this case, and you have to change the axis labels to highlight that the values are on logarithmic scale.\n\nWhen we produce plots, we may find that, for instance, the axis labels are too small. Further, we may want to change the margins around our graphics. The theme() function allows us to do all this (and much more).\nIn Figure @ref(fig:Facebook) the axis labels are quite small and it may be nice to have a little bit of a gap between the plots. Let’s use the theme() function to achieve this. To visualize the difference, we only alter the setup of the second plot.\n\ng2 &lt;- g2 +\n  theme( plot.margin=margin( t=0, l=100, b=0, r=0 ),\n         axis.title=element_text(size=16),\n         axis.text=element_text(size=14) )\ng1 + g2\n\n\n\nNumber of followers vs number of likes on linear scale (left) and logarithmic scale (right) for Facebook posts by a group of Olympic athletes.\n\n\n\nWe see that the gap between the plots has widened, and that the font size of the axis labels and numbers in the right plot have increased. Font size in ggplot2 is specified via element_text(size=..):\n\nFont size of the title is changed with theme( title=element_text(size=..) )\nFont size of the axis labels is changed with theme( axis.title=element_text(size=..) )\nFont size in the legend can be changed with theme( legend.title=element_text(size=..) ) and theme( legend.text=element_text(size=..) )\n\nRemark: Making sure that labels are provided in a suitable font size is important. The gap between the plots is more of a personal preference.\n\nSo far we used the default colour scheme provided by ggplot2. There are situations, however, where we may want to change the colour scheme. One easy way to achieve this is by using the functions scale_filler_brewer() and scale_colour_brewer() in ggplot2 - which of these to use depends on whether you specified fill=.. or colour=.. in aes().\nExample: Let’s reproduce the box plots in Section 2.2.6, but with an orange colour scheme\n\nAUS &lt;- read.csv(\"data/weatheraustralia.csv\" )\nggplot( AUS, aes( x=Location,y=WindGustSpeed ) ) +\n  geom_boxplot( aes(fill=Location) ) + \n  labs( y=\"Speed of wind gust in km/h\" ) + \n  scale_fill_brewer( palette=\"Oranges\" ) + theme_bw() + \n  theme( axis.title=element_text(size=14), axis.text=element_text(size=14) )\n\n\n\nBox plots of the speed of wind gust for 5 cities in Australia.\n\n\n\nRemark: We used theme_bw() to change the background colour from grey to white.\nThere are multiple colour blind friendly patterns available for scale_filler_brewer() and scale_colour_brewer(), which you can view using\n\nlibrary( RColorBrewer )\ndisplay.brewer.all( colorblindFriendly = TRUE )\n\n\n\nList of colour blind friendly palettes that can be used to visualize discrete variables in ggplot2.\n\n\n\nNote, the colour schemes above are useful when applied to visualize a discrete variable with a small number of different values, such as the five Australian cities. We will see in Chapter [4][Spatial Data Analysis] how to change the colour scheme when visualizing a continuous random variable.\n\nGenerally speaking, ggplot2 plots different columns in a data frame against each other, and each row is considered as a single observation. However, in practice the data structure may not be as required.\nThe functions pivot_wider() and pivot_longer() in the tidyr package may provide one way to address this. Before considering an example, let’s load the tidyr package:\n\nlibrary( tidyr )\n\n\nThe file “Manaus Temperature.csv” provides the average temperature for each month in the years 1910-2019 for the city of Manaus, Brazil. Let’s load and investigate the data:\n\nManaus_raw &lt;- read.csv(\"data/manaus_temperature.csv\" )\nslice_head( Manaus_raw, n=4 )\n\n  YEAR  JAN  FEB  MAR  APR  MAY  JUN  JUL  AUG  SEP  OCT  NOV  DEC\n1 1910 27.3 27.0 26.5 26.2 27.2 27.5 27.7 28.0 29.0 28.3 28.3 27.8\n2 1911 27.0 27.4 27.4 27.3 27.2 27.0 27.4 28.3 29.0 29.1 28.8 28.3\n3 1912 29.0 28.8 28.3 28.0 27.3 28.1 27.4 28.8 28.3 29.3 29.2 27.5\n4 1913 27.2 28.1 27.3 27.6 27.0 27.6 27.7 27.7 28.7 28.7 28.8 28.5\n\n\nWe see that each row corresponds to one year of data. If we wanted to compare two months with each other, the data structure would be ideal for us to create a scatter plot with ggplot2. Let’s compare the temperatures for January and February:\n\nggplot( Manaus_raw, aes( \"x\"=JAN, \"y\"=FEB ) ) + geom_point() + \n  labs( x=\"Average Temperature in January\", y=\"Average Temperature in February\")\n\n\n\nScatter plot of the reported average temperatures for January and February for Manaus, Brazil.\n\n\n\nThe plot looks very odd and some of the data points have a value of \\(999.9\\). Since such high temperatures are highly unlikely, we conclude that the value \\(999.9\\) is used to indicate missing data. Let’s replace these values for January and February by \\(\\mathrm{\\texttt{NA}}\\). We can do this using case_when() or by using the na_if() function in dplyr:\n\nManaus &lt;- Manaus_raw %&gt;% \n  mutate( JAN = na_if( JAN, 999.9 ), FEB = na_if( FEB, 999.9 ) )\n\nWe can now create the plot again\n\nggplot( Manaus, aes( \"x\"=JAN, \"y\"=FEB ) ) + geom_point() + \n  labs( x=\"Average Temperature in January\", y=\"Average Temperature in February\") +\n  theme( axis.title=element_text(size=16), axis.text=element_text(size=14) )\n\nWarning: Removed 16 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\nScatter plot of the monthly average temperature for January and February for Manaus, Brazil.\n\n\n\nWe see that there is a positive correlation between the average monthly temperatures for Sao Paulo and Rio de Janeiro.\n\nLet’s assume that we also wanted to explore average temperature over time. In this case, a line plot would be a good choice. However, the data frame provides no single column which contains the monthly average temperatures.\nThe function pivot_longer() allows us to change the structure of the data frame by combining the columns JAN to DEC into a single column\n\nManaus_long &lt;- Manaus_raw %&gt;%\n  pivot_longer( cols=JAN:DEC, names_to=\"Month\" ) %&gt;%\n  rename( Temperature = value )\nslice_head( Manaus_long, n=3 )\n\n# A tibble: 3 × 3\n   YEAR Month Temperature\n  &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;\n1  1910 JAN          27.3\n2  1910 FEB          27  \n3  1910 MAR          26.5\n\n\nSo we have converted the original table to a narrower but substantially longer table with 3 columns; the pivot_longer() function by default stores the observed monthly averages in a column named value, which we renamed to Temperature.\nIf we wanted to reverse back to the original format, we would use the function pivot_wider(),\n\nManaus_wide &lt;- Manaus_long %&gt;% \n  pivot_wider( names_from=Month, values_from=Temperature )\n\nEach row in Manaus_long now corresponds to one month instead of one year. The one challenge that remains is to convert the variables YEAR and MONTH into a single variable that represents the date. There is unfortunately no easy way, but the following code does the job\n\n## Convert abbreviation for month into number\nManaus_long &lt;- \n  Manaus_long %&gt;% \n  mutate( Month = case_when( \n    Month == \"JAN\" ~ \"01\", Month == \"FEB\" ~ \"02\", Month == \"MAR\" ~ \"03\", \n    Month == \"APR\" ~ \"04\", Month == \"MAY\" ~ \"05\", Month == \"JUN\" ~ \"06\",\n    Month == \"JUL\" ~ \"07\", Month == \"AUG\" ~ \"08\", Month == \"SEP\" ~ \"09\", \n    Month == \"OCT\" ~ \"10\", Month == \"NOV\" ~ \"11\", Month == \"DEC\" ~ \"12\"\n    ) \n  )\n\n## Combine year and month and convert to date\nManaus_long &lt;- Manaus_long %&gt;%\n  mutate( Date = paste( Manaus_long$YEAR, Manaus_long$Month, sep=\"-\" ) ) %&gt;%\n  mutate( Date = ym( Date ) )\n\nFinally, we can create our line plot. We have to keep in mind that we still have entries of value 999.9 which we do not want to plot. We again replace values of 999.9 with \\(\\mathrm{\\texttt{NA}}\\):\n\nManaus_long %&gt;%\n  mutate( Temperature = na_if( Temperature, 999.9 ) ) %&gt;%\n  ggplot( aes(\"x\"=Date, \"y\"=Temperature) ) + \n  geom_line() + geom_smooth() +\n  labs( y=\"Average Monthly Temperature\") + theme_bw() +\n  theme( axis.title=element_text(size=16), axis.text=element_text(size=14) )\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\nAverage monthly temperature for Manaus, Brazil, between 1910 and 2019.\n\n\n\nThe plot indicates an increase in the average monthly temperature from about 27.5 to 29 degree Celsius over the time period.\nRemark: The last piece of code shows that we can use the pipe together with the functions from ggplot2.\n\nWe covered some of the principles regarding data visualization:\n\nWhen designing your data graphic, ensure that the main message/aspect is clearly visible and provide an interpretation of your plot. Avoid presenting plots that are not focused towards answering the research question.\nThe important types of graphics we use for data visualization and exploration are line plot, scatter plot, bar plot, histogram, density plot, box plot and violin plot.\n\nGraphics can be discussed in terms of four elements:\n\nVisual cues\nCoordinate system\nScale\nContext\n\n\nThe ggplot2 R package provides a wide range of tools for creating data graphics from a given data frame. This includes changing the coordinate system, the scale and selecting a wide range of types of graphics.\nFor complex data sets it is often useful to use facets and a wide range of visual cues. This may improve accessibility for the reader.\nIn practice we often have to restructure the data using the dplyr and tidyr packages before creating the data graphic.",
    "crumbs": [
      "Lecture Notes",
      "Data Visualization"
    ]
  },
  {
    "objectID": "lecture_notes/02-DataVisualization.html#background-on-data-visualization",
    "href": "lecture_notes/02-DataVisualization.html#background-on-data-visualization",
    "title": "Data Visualization",
    "section": "",
    "text": "Before starting to create plots in R, we establish a framework to analyze plots in terms of four basic elements: visual cues, coordinate system, scale and context. Understanding these elements will help us with producing our own plots later.\n\nVisual cues are graphical elements that draw the audience to the aspects we want them to focus on. The book “Data points: Visualization that means something” by Nathan Yau (link provided on Moodle) lists nine distinct visual cues to encode a category or quantity:\n\nPosition (quantity) - relation to other things\nLength (quantity) - size in one dimension\nAngle (quantity) - width of angle may, for instance, represent proportions (pie chart)\nDirection (quantity) - slope of line\nShape (category) - which observations are in the same group\nArea (quantity) - size in two dimensions\nVolume (quantity) - size in three dimensions\nShade (quantity or category) - shade in comparison to others, or grouping\nColour (quantity or category) - colour in comparison to others, or grouping\n\nResearch has shown that our ability to perceive differences in magnitude descends in this order. One of many publications supporting this argument is “Graphical perception: Theory, experimentation, and application to the development of graphical methods.”, which you can find on Moodle.\nImportant: One crucial conclusion is that we should not rely too much on colour. Many people have colour deficiencies, which makes it very hard for them to distinguish certain colours. Consequently, before using colour, we should consider whether we could use shapes or shades instead.\nRemark: In this course we focus on creating 2D graphics. While 3D plots and animations allow us to visualize a larger number of variables (and you may think they look more impressive), I would avoid using such plots except for a very limited number of cases. That’s because it is often difficult to see the exact positions of the points.\n\nThe data set “Freediving Records.csv” provides information on the progression of the world record in multiple disciplines for men and women. We will focus on the discipline “dynamic apnea with fins (DYN)” and visualize how the world records for men and women have progressed over time:\n\n\n\n\nDevelopment of the world record in dynamic apnea with fins (DYN) for men and women between 1993 and 2020.\n\n\n\nWe can identify that the following visual cues have been used:\n\nShape and colour indicate whether the observations refer to the world record for men or women. Note, there is no issue with using multiple cues for the same information.\nLength of line is used to convey two pieces of information: (i) the length of the line in horizontal direction indicates the time it took until the world record was broken, and (ii) the length of the line in vertical direction represents the magnitude of improvement.\nPosition is used to compare the world records for men and women (both lines are provided in the plot).\nShape is used to highlight the times when a world record was broken - this makes it is easy to count the number of times the record was broken. Without this visual cue, such information would be much harder to extract from the plot.\n\nChoosing a suitable coordinate system is critical to present the data accurately and in a meaningful way. The three most common coordinate systems used in data science are\n\nCartesian:\n\nOur familiar (\\(x,y\\))-rectangular coordinate system with two perpendicular axes.\n\nPolar:\n\nPoints are identified by their radius \\(r\\) (distance from the origin) and angle \\(\\theta\\). A point (\\(x,y\\)) in Cartesian coordinates can be transformed to polar coordinates using the relationship [ (x,y) = (r, r), r=. ]\n\nGeographical:\n\nIn the chapter on spatial data analysis, we will work with points located across the earth, with their positions being defined by longitude and latitude.\n\n\nThere are two common cases when we may want to consider polar coordinates:\n\nPie charts\nVariables which naturally lie on a circle, such as wind direction.\n\nTip: Since Position and Length are better visual cues than Angle, we should prefer Cartesian coordinates to polar coordinates. For instance, before using pie charts, consider whether the same information can be displayed effectively in a bar plot. However, we will see some examples where polar coordinates may be considered the better choice.\n\nHourly weather data was collected for Bela Vista, Brazil, for 2017 and 2018. Suppose we were asked to explore the distribution of wind direction and the relation between wind direction and speed of wind gusts.\nTo analyse the distribution of wind direction, we may create a histogram of the collected data. Let’s compare the histograms obtained for the Cartesian and polar coordinate systems (both display frequency):\n\n\n\n\nFrequency of wind direction for Bela Vista, Brazil, in 2017 and 2018 displayed using Cartesian coordinates (left) and polar coordinates (right).\n\n\n\nBoth plots, Cartesian (left) and polar (right), show that the wind predominately comes from a north-easterly direction. The right plot is a tick nicer, because it better handles that \\(0^{\\circ}=360^{\\circ}\\) in terms of wind direction, which is recorded as an angle.\nLet’s turn to analyzing the relation between wind direction and speed of wind gusts. For this we create plots which map wind direction against the speed of the wind gusts, again using different coordinate systems:\n\n\n\n\nWind direction versus speed of wind gust for Bela Vista in 2017 and 2018 displayed using Cartesian coordinates (left) and polar coordinates (right).\n\n\n\nWhat can we conclude about the relation between wind direction and speed of wind gusts?\nI personally prefer the left plot because it indicates the difference in frequency of wind directions. The right plot (using polar coordinates) does not provide this information and it seems almost as if wind direction is uniformly distributed - so we would need the histogram in Figure @ref(fig:BVWind1) to correctly interpret the plot.\n\nThe concepts of scale and context refer to the choice of axes and the labeling of the data graphic respectively.\n\nScales allow us to translate values into visual cues by influencing, for instance, the distance (length) between points in a scatter plot. When choosing scales, we have to consider how the displayed distances translate into meaningful differences. Each coordinate axis can have its own scale, and we have three different choices:\nNumerical: Numeric quantities, such as speed, age, etc., are commonly set on a linear, logarithmic or percentage scale.\nCategorical: Categorical variables may have no ordering (political parties), or be ordinal (restaurant ratings). Ordinal variables differ from numeric quantities in that distances are unknown (or not meaningful). For instance, a first class degree is better than a 2.1, but what does this mean in terms of distance?\nTime: While being a numeric quantity, time has some special properties: 1) it can be demarcated by year, month,.. and 2) it can be considered periodical. This second aspect can be highlighted using polar coordinates as seen in Section 2.1.4.\n\nData graphics are provided to aid the reader/viewer with making meaningful comparisons. Context can be added in the form of titles, subtitles and axis labels that explain what is being shown, including the scales and units. It sometimes also helps to include reference points or lines. While we should avoid cluttering or providing excessive annotations, it is necessary to provide proper context.\n\nThe following two plots both illustrate the relation between body weight and brain weight for several mammals, but they use different scales:\n\n\n\n\nBody weight vs brain weight for 62 mammals on linear scale (left) and logarithmic scale (right).\n\n\n\nWe make two observations:\n\nLinear scales were used in the left plot, while logarithmic scales were used in the right plot - this context was provided via the title.\nThe right plot is more informative as it shows a linear relationship between body and brain weight on logarithmic scale. This aspect is not clear in the left plot, and a viewer may focus on the three mammals with the highest brain weight as they appear to be quite different from the rest.",
    "crumbs": [
      "Lecture Notes",
      "Data Visualization"
    ]
  },
  {
    "objectID": "lecture_notes/02-DataVisualization.html#analysis-of-australian-weather-data-using-ggplot2",
    "href": "lecture_notes/02-DataVisualization.html#analysis-of-australian-weather-data-using-ggplot2",
    "title": "Data Visualization",
    "section": "",
    "text": "After outlining the different elements of a plot in the previous section, we now explore how we can create informative plots using the ggplot2 R package\n\nlibrary( ggplot2 )\n\nWe will introduce and utilize a wide range of plots to analyze historical weather data for five Australian cities. The data is provided on Moodle as “WeatherAustralia.csv” and we start by loading the data and looking at the first few entries:\n\nAUS &lt;- read.csv(\"data/weatheraustralia.csv\" )\nslice_head( AUS, n=5 )\n\n        Date Location MinTemp MaxTemp Rainfall WindGustSpeed\n1 01/01/2009   Sydney    17.7    35.1        0            72\n2 02/01/2009   Sydney    18.5    23.0        0            63\n3 03/01/2009   Sydney    16.9    23.2        0            NA\n4 04/01/2009   Sydney    18.7    27.1        0            65\n5 05/01/2009   Sydney    20.2    31.6        0            63\n\n\nWe see that the data provide daily information on minimum and maximum temperature, amount of rainfall and wind speed. Further, the data also include some missing values, as highlighted by the \\(\\mathrm{\\texttt{NA}}\\) entries.\nIn the following, we perform a data exploration which includes the creation and interpretation of multiple plots. At the end of the analysis, you will be familiar with the general functionality of the ggplot2 package. Further types of plots are listed on the ggplot2 cheat sheet (provided on Moodle). I would advise you to try creating some of them for practice, as they may be useful.\nImportant: In this section we focus on the different types of plots and visual cues. The other elements described in Section 2.1, coordinate system, scale and context, will be considered in Section 2.3.\n\nThe first step is to call the function ggplot() and we usually specify the following inputs:\n\nThe name of the data frame that contains the data we want to plot\nThe names of the variables which specify the axes. This is done using the function aes(), which refers to aesthetics (we will see this very often).\n\nSuppose we want to create a scatter plot of minimum against maximum temperature for the five Australian cities. To initialize the plot, we specify\n\nPlotAUS &lt;- ggplot( AUS, aes( x=MinTemp, y=MaxTemp ) )\n\nHowever, if we call PlotAUS in R, we will see a Cartesian coordinate system, but no points. This is because the coordinate system and visual cues are specified using separate functions in ggplot2. We will consider in the next subsection how to add points as an additional layer.\nRemark: While it is important to use meaningful variable names when programming, I usually make an exception when using ggplot2. For instance, instead of PlotAUS, we could also call it g (for graphic) - I will later use this for brevity.\n\nThe function geom_point() adds the points to the coordinate system created by ggplot(). Returning to the Australian weather data, we generate the scatter plot of daily minimum and maximum temperature using\n\nPlotAUS + geom_point()\n\n\n\nMinimum versus maximum observed daily temperature for five Australian cities between 2008 and 2017.\n\n\n\nWe observe a positive correlation between minimum and maximum daily temperature. However, this plot does not highlight that the data are coming from five different cities. In other words, the plot does not allow us to explore differences between the cities.\nOne possible solution is to use the visual cues shape and colour to indicate to which city a data point belongs. Specifically, we specify via aes() that the variable Location should be used to determine the shape and colour of the points created by the geom_point() function:\n\nPlotAUS + geom_point( aes( shape=Location, color=Location ) )\n\n\n\nMinimum versus maximum observed daily temperature for five Australian cities between 2008 and 2017, with the city being highlighted using the visual cues of shape and colour.\n\n\n\nRemark: If we wanted to use shade, we would have specified \\(\\mathrm{\\texttt{aes( alpha=Location )}}\\).\nWhile our plot now makes clear that the data come from different cities, it is still very cluttered. This makes it hard to draw conclusions on differences between the five Australian cities. Luckily, there is a better option available, which we consider next.\n\nInstead of creating a single plot which contains all points, it seems a nice idea to have a separate plot for each city. In other words, we want to split the data using the variable Location and create one plot per subset.\nThis type of plot is called a facet plot and it provides a simple and effective way to display the data for the separate levels of a categorical variable.\nTo create the facet plot, we use the function facet_wrap() and change the axes labels using the labs() function:\n\nPlotAUS + geom_point() +\n  facet_wrap( ~Location ) +\n  labs( x=\"Minimum daily temperature\", y=\"Maximum daily temperature\" )\n\n\n\nPlots of minimum versus maximum daily temperature between 2008 and 2017 for five Australian cities.\n\n\n\nOur plot now reveals that Darwin is quite different from the other four cities in terms of the distribution of minimum and maximum daily temperature, in particular, in terms of the range of values.\nWhat may be the reason for this difference? What else can we conclude?\n\nAs for the river flow data in Section [1.1.4][Example: Loading and cleaning NRFA river flow data], we may want to explore how a variable changes over time for one of the five cities.\nScatter plots are often not the right choice for such a task, because it’s hard to see patterns in the data. Instead we create a line plot, where the length of the line connecting consecutive dates illustrates the magnitude of change in the values of the variable.\nLet’s see how we can create a line plot for daily maximum temperature over time for Darwin. We start by converting the variable Date in the data frame to the correct type,\n\nAUS &lt;- AUS %&gt;% mutate( Date = as_date( Date, format=\"%d/%m/%Y\" ) )\n\nThe next step is to extract the subset of observations for Darwin, which we store in a separate data frame called Darwin:\n\nDarwin &lt;- filter( AUS, Location == \"Darwin\" )\n\nA line plot is then created by using the geom_line() function, again by first defining the axes in the ggplot() function:\n\nggplot( Darwin, aes( x=Date, y=MaxTemp ) ) + geom_line() + \n  labs( x=\"Date\", y=\"Maximum daily temperature\" )\n\n\n\nTime series plot of daily maximum temperature for Darwin between 2008 and 2017.\n\n\n\nTo create such a (time series) plot for all five cities, we again use facet_wrap(), and we use colour and shape (line type) to highlight the different cities:\n\nggplot( AUS, aes( x=Date, y=MaxTemp ) ) + facet_wrap( ~Location ) + \n  geom_line( aes( color=Location, linetype=Location ) ) + \n  labs( x=\"Date\", y=\"Maximum daily temperature\" )\n\n\n\nTime series plot of daily maximum temperature for five Australian cities between 2008 and 2017.\n\n\n\nThe gap in the plot for Melbourne is due to the data being missing for this time period. One conclusion we draw from the plot is that the maximum daily temperature is less varied for Darwin than the other cities, which exhibit a clear seasonal pattern.\nRemark 1: To draw steps as in Section 2.1.2, we would use geom_step() instead of geom_line().\nRemark 2: The use of colour and shape in Figure @ref(fig:DateTempFacet) is not really necessary. We just did it do illustrate that visual cues can be used together with facet_wrap().\n\nWe now want to investigate the distribution of a single variable, such as the speed of the wind gusts in one (or more) of the cities. Histograms and density plots are useful in such cases. You already created histograms in Year 1 Probability & Statistics and we analyzed such a plot in Section 2.1.4. We start by producing a histogram using ggplot2 and then introduce the density plot in more detail.\n\nThe ggplot2 package provides the function geom_histogram() to create a histogram. As for the hist() function you used before, we have to set a suitable number of bins, and this is done using the bins option inside the geom_histogram() function.\nLet’s return to the data for Darwin. To create a histogram for wind speed gusts, we have to call ggplot() and geom_histogram(), but we now only specify the x-axis:\n\nggplot( Darwin, aes( x=WindGustSpeed ) ) + geom_histogram( bins=20 ) + \n  labs( x=\"Speed of wind gust in km/h\", y=\"Count\" )\n\n\n\nHistogram of the speed of wind gusts for Darwin between 2008 and 2017.\n\n\n\nWhat could be a sensible distribution for the speed of wind gusts, based on the histogram?\n\nA density plot is pretty much a smoothed version of the histogram. Given data \\(x_1,\\ldots,x_n\\), we define the estimate \\(\\hat{f}_X(\\cdot)\\) for the probability density function \\(f_X(\\cdot)\\) of the random variable \\(X\\) as \\[\\begin{equation}\n\\hat{f}_X(x) = \\frac{1}{nh}\\sum_{i=1}^{n} K\\left(\\frac{x-x_i}{h}\\right), \\qquad x\\in\\mathbb{R},\n(\\#eq:KDE)\n\\end{equation}\\] where \\(K(\\cdot)\\) is termed the kernel and \\(h\\) is called the bandwidth. The kernel is a non-negative probability density function. One common choice is to set \\(K(\\cdot)\\) as the density of the standard normal distribution, with [ X(x) = {i=1}^{n}{-}. ]\nIn principle, the bandwidth \\(h\\) has to be set by us. If \\(h\\) is too small, the density plot will look very jittered, while a too large \\(h\\) will obscure the underlying data structure. Consequently, care should be taken when setting the bandwidth.\nRemark 1: When generating density plots in R, a suitable value for \\(h\\) is often provided automatically. However, we sometimes have to set the bandwidth manually.\nRemark 2: It is usually sufficient to provide either the histogram or the density plot, because they visualize similar aspects of the data. Note, the histogram should be used for discrete data.\nIn ggplot2, geom_density() is used to create density plots and, as for geom_histogram(), we only specify the x-axis. Let’s use this type of plot to compare the distributions of the speed of wind gusts for Darwin and Adelaide. To aid the comparison, we place the estimates for the two cities in the same plot:\n\nWeatherAD &lt;- filter( AUS, Location %in% c(\"Adelaide\",\"Darwin\") )\nggplot( WeatherAD, aes( x=WindGustSpeed ) ) +\n  geom_density( aes( linetype=Location, color=Location ), size=1.2 ) +\n  labs( x=\"Speed of wind gust in km/h\", y=\"Density\" )\n\n\n\nDensity plots of the speed of wind gusts for Adelaide (solid) and Darwin (dashed) for 2008–2017.\n\n\n\nThe two estimated curves indicate that Adelaide and Darwin differ in the distribution of wind gust speeds. For instance, Adelaide observes wind gust speeds below 30 km/h more often than Darwin.\nRemark 1: The option \\(\\mathrm{\\texttt{size=1.2}}\\) in geom_density() increases the thickness of the line in the plot - you can set size to any positive value, and higher values will give thicker lines. If you wanted to specify the bandwidth \\(h\\) in @ref(eq:KDE), you have the option \\(\\mathrm{\\texttt{bw=..}}\\).\nRemark 2: Density plots can sometimes be misleading because they are a smoothed version of the histogram. Although Adelaide and Darwin appear to have a similar density in Figure @ref(fig:DAWind) when it comes to high wind speeds, the highest value for Adelaide is 86 km/h, while it is 126 km/h for Darwin. Consequently, we should be cautious when drawing conclusions about the distribution tails based on density plots.\n\nBox plots and violin plots are useful for comparing a number of distributions. For instance, we may want to compare the distributions of wind gust speed for the five Australian cities. While we could produce a separate density plot for each city, box and violin plots tend to be a better choice.\nThe two types of plot differ in terms of the information they provide:\n\nBox plots visualize the median, interquartile range (25% and 75% quantile) and outliers.\nViolin plots visualize the density estimate \\(\\hat{f}_X(\\cdot)\\) as defined in Equation @ref(eq:KDE).\n\n\nIn ggplot2, box plots are created using the geom_boxplot() function. Let’s use this type of plot to compare the wind gust speeds across the five cities. We set the x-axis to the categorical variable Location, and the y-axis as speed of wind gust:\n\nggplot( AUS, aes( x=Location, y=WindGustSpeed ) ) + \n  geom_boxplot( aes( fill=Location ) ) + \n  labs( y=\"Speed of wind gust in km/h\" )\n\n\n\nBox plots of the speed of wind gusts for five Australian cities between 2008 and 2017.\n\n\n\nWe can extract a few details about the distributions of the speed of wind gusts for the five cities. For instance, when considering the median, Melbourne and Sydney record the strongest wind gusts, while Adelaide and Perth record the lowest wind gust speeds.\nRemark 1: The option \\(\\mathrm{\\texttt{fill=Location}}\\) gives each box plot a different colour.\nRemark 2: If we wanted a single box plot for a variable, we would only specify one of the axes.\n\nTo conclude our analysis, we compare the distribution of daily wind gust speeds across the five Australian cities using violin plots. This type of plot is generated using the geom_violin() function, and the syntax is the same as for geom_boxplot(). We make two additions compared to Figure @ref(fig:Box) though:\n\nA plot title is set using the labs() function.\nIt’s good practice to order box and violin plots, for instance, based on the median of the different subgroups. This is achieved using the reorder() function.\n\nPutting everything together, the five violin plots are generated using\n\nggplot( AUS, aes( x=reorder(Location, WindGustSpeed, median, na.rm=TRUE), \n                  y=WindGustSpeed ) ) + geom_violin() + \n  labs( x=\"Location\", y=\"Wind gust speed in mph\", \n        title=\"Wind gust speed across five Australian cities\" )\n\n\n\nViolin plots of wind speed across five cities in Australia. The cities are ordered in ascending order in terms of their median wind gust speed.\n\n\n\nRemark: The four components within reorder() should be read as follows: (i) We want to reorder the values in Location, (ii) which should be based on the variable WindGustSpeed, (iii) the value to be used for the reordering is the median and (iv) ignore any missing values when calculating the median.\nBy ordering the violin plots, we can directly draw the conclusion that Adelaide has the lowest median wind gust speed, while Sydney has the highest. There are also other conclusions that are worth pointing out, such as the differences in terms of highest observed wind gust speeds.",
    "crumbs": [
      "Lecture Notes",
      "Data Visualization"
    ]
  },
  {
    "objectID": "lecture_notes/02-DataVisualization.html#creating-advanced-plots-with-ggplot2",
    "href": "lecture_notes/02-DataVisualization.html#creating-advanced-plots-with-ggplot2",
    "title": "Data Visualization",
    "section": "",
    "text": "In the previous section we generated plots using the structure [ + _. ] and this allowed us to create several type of plots and to use multiple visual cues. The following table lists the types of plots we explored so far (plus the functions for generating bar plots):\n\n\n\n\n\n\n\nType of plot\n\\(\\mathrm{\\texttt{&lt;GEOM}}\\_\\mathrm{\\texttt{FUNCTION&gt;}}\\)\nVisual cues we may use\n\n\n\nScatter plot\ngeom_point\nshape, shade, colour, size\n\n\nLine plot\ngeom_line\nshade, colour, linetype, size\n\n\n\ngeom_step\nshade, colour, linetype, size\n\n\nBar plot\ngeom_bar\nshade, colour, size\n\n\n\ngeom_col\nshade, colour, size\n\n\nHistogram\ngeom_histogram\nshade, colour, linetype\n\n\nBox plot\ngeom_boxplot\nshade, colour\n\n\nDensity plot\ngeom_density\nshade, colour, linetype\n\n\nViolin plot\ngeom_violin\nshade, colour, linetype\n\n\n\n\nWe also covered a few additional aspects, such as using facet_wrap() to create facets and labs() to specify the axes labels. In this section we consider the remaining plot elements described in Section 2.1:\n\nChanging the coordinate system to polar coordinates\nChanging the scale of the x-axis (y-axis)\n\nWe will also explore how to change the font size of the labels and the colour scheme in our plot. This will give us a lot of flexibility to visualize data.\n\nWe saw in Section 2.2 that ggplot() uses the Cartesian coordinate system by default. In Section 2.1.3 we highlighted that we sometimes want to use polar coordinates, for instance, when displaying wind direction. The coord_polar() function in ggplot2 allows us to switch from Cartesian to polar coordinates. Let’s consider two cases where such a transformation may be useful.\n\nIn Figures @ref(fig:BVWind1) and @ref(fig:BVWind2) we used plots with polar coordinates to explore wind direction and speed for Bela Vista, Brazil. We can reproduce these plots in ggplot2:\n\nlibrary(patchwork) # required to place plots next to each other\nwind_BV &lt;- read.csv(\"data/wind_bela_vista.csv\" )\n\ng1 &lt;- ggplot( wind_BV, aes( x=Wind.Direction ) ) + \n  geom_histogram( bins = 120 ) + coord_polar() + \n  labs( x=\"Wind Direction\" )\ng2 &lt;- ggplot( wind_BV, aes( x=Wind.Direction, y=Gust.Speed ) ) +\n  geom_point() + geom_smooth() + coord_polar( theta=\"x\" )  + \n  labs( x=\"Wind Direction\", y=\"Wind gust in m/s\" )\ng1 + g2\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\nHistogram illustrating the frequency of wind direction (left) and scatter plot of wind direction vs wind speed (right) for Bela Vista, Brazil. Both plots use polar coordinates. The solid line in the right plot shows the average wind speed for each wind direction.\n\n\n\nThe option \\(\\mathrm{\\texttt{theta=\"x\"}}\\) in coord_polar() specifies that the angle should be defined by the x-axis. We do not have to do anything else - ggplot2 is doing all the calculations for us.\nRemark 1: The R package patchwork makes it easy to place two graphics created by ggplot2 next to each other by using the + sign. There are more options available - have a look at the documentation for the package.\nRemark 2: The example also shows that we can use multiple geom_..() functions in the same plot. Here, we used geom_smooth() to add a line representing the average wind speed in each direction to the plot. However, we note that the line does not match up at \\(0^{\\circ}\\). This is not particularly elegant, but there is not a very easy fix to this.\nAre there any other plots we should create to explore the distribution of wind direction and speed of wind gusts at Bela Vista, Brazil?\n\nIn a pie chart we use the size of the angle as a visual cue and such a plot may be useful to visualize proportions. The following R code illustrates how to create a pie chart using ggplot2:\n\ndf &lt;- data.frame( \"prob\" = c(0.3,0.4,0.3), \"group\" = c(\"A\",\"B\",\"C\") )\nggplot( df, aes( x=\"\", y=prob, fill=group ) ) + \n  geom_col() + coord_polar( theta=\"y\" ) + labs( x=\"\", y=\"\" )\n\n\n\nExample of a pie chart with three slices.\n\n\n\nThe code first creates a plot on Cartesian coordinates - a stacked bar plot in this case - which is then converted to polar coordinates. To see this, you may want to have a look at the plot you obtain when removing coord_polar() from the code above.\n\nWe saw in Section 2.1.5 that changing the scale of one (or more) variables in a plot can help with the data analysis. In applications, transformations are often used to reduce the influence of extreme outliers in the plot. For instance, by considering logarithmic scales in Section 2.1.5, the three mammals with very high brain weight in the data appeared less extreme, and we were instead able to see the possible linear relationship on logarithmic scale between brain and body weight.\nWhen using ggplot(), a linear scale is used by default. The function coord_trans() enables us to change the scale of the x-axis or y-axis. The most common transformations are logarithmic (\\(\\mathrm{\\texttt{x=\"log\"}}\\) or \\(\\mathrm{\\texttt{x=\"log10\"}}\\)) and square root (\\(\\mathrm{\\texttt{x=\"sqrt\"}}\\)).\n\nThe data in “Facebook.csv” contains data related to messages posted by a few Olympic athletes. We want to explore the relation between number of likes (received for individual posts) and the number of followers of the athlete. Let’s produce two plots, one with linear scales and one with logarithmic scales:\n\nFacebook &lt;- read.csv(\"data/facebook.csv\", header=TRUE )\n\ng  &lt;- ggplot( Facebook, aes( x=follow, y=postlikes ) ) + \n  labs( x=\"Number of Followers\", y=\"Number of likes\" )\ng1 &lt;- g + geom_point() \ng2 &lt;- g + geom_point() + coord_trans( x=\"log\", y=\"log\" )\ng1 + g2\n\n\n\nNumber of followers vs number of likes on linear scale (left) and logarithmic scale (right) for Facebook posts by a group of Olympic athletes.\n\n\n\nWe see that the two plots provide quite different information: The graphic with log-transformed scales shows a possible linear relationship between log(number of followers) and log(number of likes), which is not visible when considering the plot with linear scales.\nRemark: If you want to use another transformation than logarithmic or square root, you can specify it within ggplot. For instance, we could use\n\nggplot( Facebook, aes( x=log(follow), y=log(postlikes) ) ) + geom_point() +\n  labs( x=\"log(Number of Followers)\", y=\"log(Number of likes)\" )\n\nYou have to be aware that the numbers along the axes differ in this case, and you have to change the axis labels to highlight that the values are on logarithmic scale.\n\nWhen we produce plots, we may find that, for instance, the axis labels are too small. Further, we may want to change the margins around our graphics. The theme() function allows us to do all this (and much more).\nIn Figure @ref(fig:Facebook) the axis labels are quite small and it may be nice to have a little bit of a gap between the plots. Let’s use the theme() function to achieve this. To visualize the difference, we only alter the setup of the second plot.\n\ng2 &lt;- g2 +\n  theme( plot.margin=margin( t=0, l=100, b=0, r=0 ),\n         axis.title=element_text(size=16),\n         axis.text=element_text(size=14) )\ng1 + g2\n\n\n\nNumber of followers vs number of likes on linear scale (left) and logarithmic scale (right) for Facebook posts by a group of Olympic athletes.\n\n\n\nWe see that the gap between the plots has widened, and that the font size of the axis labels and numbers in the right plot have increased. Font size in ggplot2 is specified via element_text(size=..):\n\nFont size of the title is changed with theme( title=element_text(size=..) )\nFont size of the axis labels is changed with theme( axis.title=element_text(size=..) )\nFont size in the legend can be changed with theme( legend.title=element_text(size=..) ) and theme( legend.text=element_text(size=..) )\n\nRemark: Making sure that labels are provided in a suitable font size is important. The gap between the plots is more of a personal preference.\n\nSo far we used the default colour scheme provided by ggplot2. There are situations, however, where we may want to change the colour scheme. One easy way to achieve this is by using the functions scale_filler_brewer() and scale_colour_brewer() in ggplot2 - which of these to use depends on whether you specified fill=.. or colour=.. in aes().\nExample: Let’s reproduce the box plots in Section 2.2.6, but with an orange colour scheme\n\nAUS &lt;- read.csv(\"data/weatheraustralia.csv\" )\nggplot( AUS, aes( x=Location,y=WindGustSpeed ) ) +\n  geom_boxplot( aes(fill=Location) ) + \n  labs( y=\"Speed of wind gust in km/h\" ) + \n  scale_fill_brewer( palette=\"Oranges\" ) + theme_bw() + \n  theme( axis.title=element_text(size=14), axis.text=element_text(size=14) )\n\n\n\nBox plots of the speed of wind gust for 5 cities in Australia.\n\n\n\nRemark: We used theme_bw() to change the background colour from grey to white.\nThere are multiple colour blind friendly patterns available for scale_filler_brewer() and scale_colour_brewer(), which you can view using\n\nlibrary( RColorBrewer )\ndisplay.brewer.all( colorblindFriendly = TRUE )\n\n\n\nList of colour blind friendly palettes that can be used to visualize discrete variables in ggplot2.\n\n\n\nNote, the colour schemes above are useful when applied to visualize a discrete variable with a small number of different values, such as the five Australian cities. We will see in Chapter [4][Spatial Data Analysis] how to change the colour scheme when visualizing a continuous random variable.",
    "crumbs": [
      "Lecture Notes",
      "Data Visualization"
    ]
  },
  {
    "objectID": "lecture_notes/02-DataVisualization.html#changing-the-data-structure",
    "href": "lecture_notes/02-DataVisualization.html#changing-the-data-structure",
    "title": "Data Visualization",
    "section": "",
    "text": "Generally speaking, ggplot2 plots different columns in a data frame against each other, and each row is considered as a single observation. However, in practice the data structure may not be as required.\nThe functions pivot_wider() and pivot_longer() in the tidyr package may provide one way to address this. Before considering an example, let’s load the tidyr package:\n\nlibrary( tidyr )\n\n\nThe file “Manaus Temperature.csv” provides the average temperature for each month in the years 1910-2019 for the city of Manaus, Brazil. Let’s load and investigate the data:\n\nManaus_raw &lt;- read.csv(\"data/manaus_temperature.csv\" )\nslice_head( Manaus_raw, n=4 )\n\n  YEAR  JAN  FEB  MAR  APR  MAY  JUN  JUL  AUG  SEP  OCT  NOV  DEC\n1 1910 27.3 27.0 26.5 26.2 27.2 27.5 27.7 28.0 29.0 28.3 28.3 27.8\n2 1911 27.0 27.4 27.4 27.3 27.2 27.0 27.4 28.3 29.0 29.1 28.8 28.3\n3 1912 29.0 28.8 28.3 28.0 27.3 28.1 27.4 28.8 28.3 29.3 29.2 27.5\n4 1913 27.2 28.1 27.3 27.6 27.0 27.6 27.7 27.7 28.7 28.7 28.8 28.5\n\n\nWe see that each row corresponds to one year of data. If we wanted to compare two months with each other, the data structure would be ideal for us to create a scatter plot with ggplot2. Let’s compare the temperatures for January and February:\n\nggplot( Manaus_raw, aes( \"x\"=JAN, \"y\"=FEB ) ) + geom_point() + \n  labs( x=\"Average Temperature in January\", y=\"Average Temperature in February\")\n\n\n\nScatter plot of the reported average temperatures for January and February for Manaus, Brazil.\n\n\n\nThe plot looks very odd and some of the data points have a value of \\(999.9\\). Since such high temperatures are highly unlikely, we conclude that the value \\(999.9\\) is used to indicate missing data. Let’s replace these values for January and February by \\(\\mathrm{\\texttt{NA}}\\). We can do this using case_when() or by using the na_if() function in dplyr:\n\nManaus &lt;- Manaus_raw %&gt;% \n  mutate( JAN = na_if( JAN, 999.9 ), FEB = na_if( FEB, 999.9 ) )\n\nWe can now create the plot again\n\nggplot( Manaus, aes( \"x\"=JAN, \"y\"=FEB ) ) + geom_point() + \n  labs( x=\"Average Temperature in January\", y=\"Average Temperature in February\") +\n  theme( axis.title=element_text(size=16), axis.text=element_text(size=14) )\n\nWarning: Removed 16 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\nScatter plot of the monthly average temperature for January and February for Manaus, Brazil.\n\n\n\nWe see that there is a positive correlation between the average monthly temperatures for Sao Paulo and Rio de Janeiro.\n\nLet’s assume that we also wanted to explore average temperature over time. In this case, a line plot would be a good choice. However, the data frame provides no single column which contains the monthly average temperatures.\nThe function pivot_longer() allows us to change the structure of the data frame by combining the columns JAN to DEC into a single column\n\nManaus_long &lt;- Manaus_raw %&gt;%\n  pivot_longer( cols=JAN:DEC, names_to=\"Month\" ) %&gt;%\n  rename( Temperature = value )\nslice_head( Manaus_long, n=3 )\n\n# A tibble: 3 × 3\n   YEAR Month Temperature\n  &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;\n1  1910 JAN          27.3\n2  1910 FEB          27  \n3  1910 MAR          26.5\n\n\nSo we have converted the original table to a narrower but substantially longer table with 3 columns; the pivot_longer() function by default stores the observed monthly averages in a column named value, which we renamed to Temperature.\nIf we wanted to reverse back to the original format, we would use the function pivot_wider(),\n\nManaus_wide &lt;- Manaus_long %&gt;% \n  pivot_wider( names_from=Month, values_from=Temperature )\n\nEach row in Manaus_long now corresponds to one month instead of one year. The one challenge that remains is to convert the variables YEAR and MONTH into a single variable that represents the date. There is unfortunately no easy way, but the following code does the job\n\n## Convert abbreviation for month into number\nManaus_long &lt;- \n  Manaus_long %&gt;% \n  mutate( Month = case_when( \n    Month == \"JAN\" ~ \"01\", Month == \"FEB\" ~ \"02\", Month == \"MAR\" ~ \"03\", \n    Month == \"APR\" ~ \"04\", Month == \"MAY\" ~ \"05\", Month == \"JUN\" ~ \"06\",\n    Month == \"JUL\" ~ \"07\", Month == \"AUG\" ~ \"08\", Month == \"SEP\" ~ \"09\", \n    Month == \"OCT\" ~ \"10\", Month == \"NOV\" ~ \"11\", Month == \"DEC\" ~ \"12\"\n    ) \n  )\n\n## Combine year and month and convert to date\nManaus_long &lt;- Manaus_long %&gt;%\n  mutate( Date = paste( Manaus_long$YEAR, Manaus_long$Month, sep=\"-\" ) ) %&gt;%\n  mutate( Date = ym( Date ) )\n\nFinally, we can create our line plot. We have to keep in mind that we still have entries of value 999.9 which we do not want to plot. We again replace values of 999.9 with \\(\\mathrm{\\texttt{NA}}\\):\n\nManaus_long %&gt;%\n  mutate( Temperature = na_if( Temperature, 999.9 ) ) %&gt;%\n  ggplot( aes(\"x\"=Date, \"y\"=Temperature) ) + \n  geom_line() + geom_smooth() +\n  labs( y=\"Average Monthly Temperature\") + theme_bw() +\n  theme( axis.title=element_text(size=16), axis.text=element_text(size=14) )\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\nAverage monthly temperature for Manaus, Brazil, between 1910 and 2019.\n\n\n\nThe plot indicates an increase in the average monthly temperature from about 27.5 to 29 degree Celsius over the time period.\nRemark: The last piece of code shows that we can use the pipe together with the functions from ggplot2.",
    "crumbs": [
      "Lecture Notes",
      "Data Visualization"
    ]
  },
  {
    "objectID": "lecture_notes/02-DataVisualization.html#summary",
    "href": "lecture_notes/02-DataVisualization.html#summary",
    "title": "Data Visualization",
    "section": "",
    "text": "We covered some of the principles regarding data visualization:\n\nWhen designing your data graphic, ensure that the main message/aspect is clearly visible and provide an interpretation of your plot. Avoid presenting plots that are not focused towards answering the research question.\nThe important types of graphics we use for data visualization and exploration are line plot, scatter plot, bar plot, histogram, density plot, box plot and violin plot.\n\nGraphics can be discussed in terms of four elements:\n\nVisual cues\nCoordinate system\nScale\nContext\n\n\nThe ggplot2 R package provides a wide range of tools for creating data graphics from a given data frame. This includes changing the coordinate system, the scale and selecting a wide range of types of graphics.\nFor complex data sets it is often useful to use facets and a wide range of visual cues. This may improve accessibility for the reader.\nIn practice we often have to restructure the data using the dplyr and tidyr packages before creating the data graphic.",
    "crumbs": [
      "Lecture Notes",
      "Data Visualization"
    ]
  },
  {
    "objectID": "lecture_notes/index.html",
    "href": "lecture_notes/index.html",
    "title": "MA22019 Introduction to Data Science - Lecture Notes",
    "section": "",
    "text": "Overview\nContent\nIn practice we often have to deal with large and complex data sets (“big data”) - many organisations and companies have collected vast amounts of data over the last decade. Analyzing such data and extracting valuable insights from it lies at the heart of data science.\nAlthough there exists no unique definition, the consensus is that data science is interdisciplinary and requires combining expertise across mathematics, computer science and the area of application. As such, we as mathematicians need to have at least some understanding of the context in which the data were collected / are analyzed in order to extract the important information and to communicate effectively with other disciplines.\nIn this course we will focus on four key areas of data science:\n\nData wrangling\nData visualization\nText data analysis\nSpatial data analysis\n\nThroughout this unit we will usually work with real-world data sets which you can download from the MA22019 Moodle page.\nAims and Objectives\nAfter taking this unit, you should be able to:\n\nDemonstrate knowledge of data science techniques used for data wrangling and visualization.\nUse R for data visualization and wrangling.\nShow awareness of the applications of these methods.\nUnderstand some of the mathematical models underlying spatial and text data analysis.\nApply these methods in R to analyse large and complex data sets. This includes demonstrating a practical and critical approach to data analysis, including the ability to select a suitable data science method, explaining your reason for choosing it, and correctly interpreting the results.\nPrerequisites\nYou should be familiar with the basics of R introduced in the Probability and Statistics component of MA12003. On Moodle, you can find the “Brief introduction to R” provided by Professor Jennison in case you are unsure about certain aspects.\nThis course will also use some of the fundamentals introduced in Year 1, including the various probability distributions, and the concepts of mean, variance and correlation.\nAssessment\nSummative Assessment\nYour mark for MA22019 is based on two individual pieces of coursework:\n\nCoursework 1: 40% of unit mark. Set at the end of Week 3 and due at the end of Week 4. This coursework will focus on data wrangling and data visualization.\nCoursework 2: 60% of unit mark. Set at the end of Week 10 and due at the start of Revision Week. This coursework will focus on text and spatial data analysis, but you will also have to demonstrate your ability to use data wrangling and data visualization techniques.\n\nYou have to submit your solutions to the coursework via Moodle.\nFormative Assessment\nProblem sheets will be set at the beginning of the week (except for Weeks 4, 10 and 11) and include:\n\nExercises:\n\nFocus more on the programming aspects of the course and help you to revise the content covered in that week’s lectures. You can submit your solutions to most questions via a Moodle quiz and you will receive direct feedback.\n\nTutorial questions:\n\nTo be be attempted in the weekly tutorials. Solutions will be made available on Tuesday evening after the last tutorial has finished.\n\nHomework question:\n\nOpen-ended question, similar in style to the coursework questions. Your answer can be submitted via Moodle to your tutor for feedback. After the submission deadline, I will provide a solution which demonstrates the aspects one may consider to address the question (and achieve a first-class mark), but I won’t provide a fully formulated answer - I want you to practice discussing your results in your own words.\n\nOrganisation\nLecture Notes\nLecture notes are available from the MA22019 Moodle page. The notes are available in two formats (HTML and PDF) with identical content but different layouts.\nImportant: While the lecture notes and problem class notes provide most of the relevant information on the mathematics/statistics and programming aspects of the course, they do not fully explore (1) how to decide which plots are best suited for addressing a research question and (2) how to interpret these plots in the context of the application. These skills are best trained by looking at a range of data applications, and we will do so in the lectures and problem classes. The open-ended question on the problem sheets is designed to support your learning and to provide additional practice on this topic. As such, you are strongly encouraged to actively engage with the lectures and problem sheets.\nLectures and Tutorials\nIn person lecture: Wednesday 10:15-12:05 (Weeks 1-3,5-9) or 10:15-11:05 (Weeks 4,10,11) in 3WN 2.1\nLive online learning session (LOIL): Friday 14:15-15:05 (Weeks 1-3) on Zoom (link on Moodle)\nComputer Lab: You will be assigned to a small group which will meet weekly to go over the tutorial questions on the problem sheet. Please check your timetable to identify the time and location of your tutorial.\nSchedule\nThe rough outline for the individual weeks is as follows:\n\n\nWeek\nTopic\nRemarks\n\n\n\n1\nData Wrangling\nTutorial on RMarkdown\n\n\n2\nData Wrangling + Data Visualization\nProblem Sheet 1\n\n\n3\nData Visualization\nProblem Sheet 2\n\n\n\n\nRelease of Coursework 1\n\n\n4\nQ&A Coursework 1\nDeadline Coursework 1\n\n\n5\nText Data Analysis 1\nNo tutorials\n\n\n6\nText Data Analysis 2\nProblem Sheet 3\n\n\n7\nSpatial Data Analysis 1\nProblem Sheet 4\n\n\n8\nSpatial Data Analysis 2\nProblem Sheet 5\n\n\n9\nSpatial Data Analysis 3\nProblem Sheet 6\n\n\n10\nRevision Class\nRelease of Coursework 2\n\n\n11\nQ&A Coursework 2\n\n\n\n12\n\nDeadline Coursework 2\n\n\n\nWe will finish the core content by the end of Week 9. In Weeks 4, 10 and 11, we will only have Revision and Coursework Q&A sessions.\nR and RStudio\nWe will use R and RStudio throughout this course. You can access RStudio via the UniDesk, but I would recommend that you install R and RStudio on your own computer / laptop. If you already have R installed, please check that you have at least version 4.4.0.\nTo install all the relevant packages, run the R code in the file “InstallPackages.R”, which you can find on Moodle. The individual packages are then loaded using the library() function, e.g.\n\nlibrary(dplyr) \n\nThis has to be done every time you start R/RStudio.\nTo load data from external files, we will use the following two functions\n\nload( \"...\" )     # To load .RData files\nread.csv( \"...\" ) # To load .csv files\n\nResources\nThis unit is self-contained in the sense that you will not need to read text books. However, you may wish to consult the following books, available as ebooks via the University Library, to support your learning and understanding:\nBoehmke, Bradley C. Data Wrangling with R. 1st Edition. 2016. Springer.\nMailund, Thomas. Beginning Data Science in R : Data Analysis, Visualization, and Modelling for the Data Scientist. 1st Edition 2017. Apress.\nWickham, Hadley. ggplot2 : Elegant Graphics for Data Analysis. 2nd Edition. 2016. Springer.\nSilge, Julia and Robinson, David. Text Mining with R: A Tidy Approach. 2015. O’Reilly.\nRemark: Some examples in these books use R functions that have been superseded, i.e., it is advised to use an alternative, more recent function (which may have a different syntax). I will use the most recent functions in this course.",
    "crumbs": [
      "Lecture Notes",
      "Introduction"
    ]
  },
  {
    "objectID": "lecture_notes/03-Text-Data-Analysis.html",
    "href": "lecture_notes/03-Text-Data-Analysis.html",
    "title": "Text Data Analysis",
    "section": "",
    "text": "Text data occurs in various forms: short messages (Twitter/X, Instagram, etc.), websites, books, etc. There are many real-world applications where we need to extract information from text data. Just to list a few examples:\n\nSearch engines need to evaluate whether a website is relevant to us;\nIdentification of messages (out of billions) that are relevant to prevent serious crimes;\nInteractive customer service pages have to provide a good response to our question.\n\nText data analysis (also known as text mining) refers to the area of data science that considers the derivation of information from text data. These methods are also important in the context of natural language processing.\nIn this chapter we explore some important text data analysis techniques. Section 3.1 introduces methods for visualizing the frequency of words within a text. Section 3.2 then explores sentiment analysis, which is concerned with studying the intention of a text. In Section 3.3 we outline approaches for comparing text data sets based on word frequency. Finally, Section 3.4 outlines a statistical framework for document classification.\nRemark: We will focus on analyzing individual words within a document, but there are many more aspects that could be studied, such as the structure of sentences.\n\n\nConsider the following quote by Hermann Hesse (a German-Swiss poet):\n\n  \"Some of us think holding on makes us strong,\nbut sometimes it is letting go.\"\n  \nThe text is stored in the file “Hesse quote.txt” on Moodle, with the quote being split into two lines as above. To load the data into R, we use the function readLines():\n\ntext_Hesse &lt;- readLines(\"data/hesse_quote.txt\")\ntext_Hesse\n\n[1] \"Some of us think holding on makes us strong,\"\n[2] \"but sometimes it is letting go.\"             \n\n\nWe see that text_Hesse is a vector with two entries, each entry corresponding to a line in the .txt file. The entries are of type character and are also referred to as strings, i.e, combinations of words.\nLet’s store the data as a data frame:\n\nquote_Hesse &lt;- data.frame(line = 1:2, text = text_Hesse)\n\nThe text data in its current format is of little use, because we usually want to analyze single words within a text. As such, we need to separate the strings into individual words. Further, any punctuation should be removed because we are not interested in it.\nThe function unnest_tokens() in the tidytext R package does all of this for us:\n\nlibrary(tidytext)\nlibrary(dplyr)\nHesse_tidy &lt;- quote_Hesse %&gt;% unnest_tokens(output = word, input = text)\nHesse_tidy %&gt;% slice_head(n = 5)\n\n  line    word\n1    1    some\n2    1      of\n3    1      us\n4    1   think\n5    1 holding\n\n\nThe data are now stored in the column word. This format is known as the tidy text format and it follows two principles:\n\nEach variable is a column: our variable of interest is word;\nEach observation is a row: we have one word per row.\n\nNow that the text is in this new format, compared to the previous chapters, the only difference is that we have to deal with non-numerical observations. However, there are a wide range of techniques unique to analyze of such data.\n\nWhen analyzing text data, we often want to extract words which are used frequently. Bar plots and word clouds are widely applied graphics to visualize word frequency, i.e. the number of times a word appears. In the following, we demonstrate how these plots can be created in R for the book Jane Eyre by Charlotte Brontë.\n\nThe full text for Jane Eyre is freely available from Project Gutenberg; we only require the Gutenberg ID for the book, which is 1260.\n\nURL &lt;- \"https://www.gutenberg.org/cache/epub/1260/pg1260.txt\"\nJaneEyre_raw &lt;- readLines(URL, encoding = \"UTF-8\")\n\nWe have to remove the meta data and disclaimers at the beginning and end of the file. We do this by detecting the lines containing the word “EBOOK”, which signals the beginning and end of the book:\n\nind &lt;- grep(\"EBOOK\", JaneEyre_raw)\nJaneEyre_raw &lt;- data.frame(text = JaneEyre_raw[(ind[1] + 1):(ind[2] - 1)])\n\nThe individual rows in JaneEyre_raw represent one line from the book, as provided in the printed version stored on the Project Gutenberg website, with the first lines being from the title page.\nAfter importing the data, we separate the lines of text into individual words using unnest_tokens():\n\nJaneEyre &lt;- JaneEyre_raw %&gt;% unnest_tokens(word, text)\n\nA bit more data cleaning is required before starting the analysis. Specifically, some of the extracted words appear with an “_“, such as the eighth extracted word -”_illustrated”. This indicates that the word is printed in italics, but we do not want to tread “the” and “the” differently. To remove these underscores, we apply the gsub() function we saw in Problem Class 1:\n\nJaneEyre$word &lt;- gsub(\"_\", \"\", JaneEyre$word)\n\n\nWith the data in the tidy text format, the function count() from the dplyr R package can be used to extract the number of times each word was used. We further calculate each word’s proportion amongst the total number of words, called the term frequency, and its rank:\n\nJaneEyre_Count &lt;- JaneEyre %&gt;%\n    count(word, sort = TRUE) %&gt;%\n    mutate(\"term frequency\" = n / sum(n), rank = row_number())\nslice_head(JaneEyre_Count, n = 5)\n\n  word    n term frequency rank\n1  the 7856     0.04169475    1\n2    i 7172     0.03806451    2\n3  and 6632     0.03519852    3\n4   to 5238     0.02780004    4\n5    a 4475     0.02375051    5\n\n\nLooking at the output, the words with the highest term frequency are the ones we usually use when writing a longer piece of text.\n\nBefore analyzing the words most commonly used in Jane Eyre in more detail, we create a plot of term frequency versus rank, both on logarithmic scale:\n\nlibrary(ggplot2)\nggplot(JaneEyre_Count, aes(x = rank, y = `term frequency`)) +\n    geom_point(size = 1.5) +\n    coord_trans(x = \"log10\", y = \"log10\") +\n    theme_bw() +\n    theme(axis.title = element_text(size = 15))\n\n\n\nPlot of rank versus term frequency for the words in Jane Eyre, illustrating the negative proportionality stated by Zipf’s Law.\n\n\n\nThere is pretty much a linear relationship between rank and term frequency on logarithmic scale (apart from the words with the highest term frequency). This is known as Zipf’s Law which states that empirically a word’s term frequency is inversely proportional to its rank.\nRemark: Creating this plot is not necessary in a text analysis. We just produced it to illustrate Zipf’s Law.\n\nWe saw that the five words with the highest term frequency in JaneEyre_Count include “the”, “I” and “and”. This is quite common when analyzing longer pieces of text. One may argue that such words are not of interest in an analysis, because we cannot write a text without using them.\nThis has led to the concept of stop words. The idea is to specify words that are dropped from the analysis, because they are not relevant. A set of stop words is called a stop list.\nThe tidytext R package provides its own list, stop_words, which includes 1149 stop words. We can remove these stop words from JaneEyre_Count using the function anti_join() from the dplyr R package:\n\ndata(stop_words)\nJaneEyre_Count &lt;- JaneEyre_Count %&gt;% anti_join(stop_words)\nslice_head(JaneEyre_Count, n = 5)\n\n       word   n term frequency rank\n1      jane 341    0.001809815   69\n2 rochester 317    0.001682438   71\n3       sir 316    0.001677131   72\n4      miss 310    0.001645287   73\n5      time 244    0.001295000   98\n\n\n\nLet’s create a bar plot for the most commonly used words in Jane Eyre that are not on the stop list:\n\nJaneEyre_Count %&gt;%\n    slice_head(n = 10) %&gt;%\n    mutate(word = reorder(word, n)) %&gt;%\n    ggplot(aes(x = n, y = word)) +\n    geom_col() +\n    labs(x = \"Count\", y = \"Word\") +\n    theme_bw() +\n    theme(axis.title = element_text(size = 17), axis.text = element_text(size = 15))\n\n\n\nWord frequency of the most commonly used words in Jane Eyre.\n\n\n\nWe see that the names of two main characters, Jane Eyre and Edward Rochester, and their titles appear the most often, followed by “time”, “day”, “looked” and “night”.\nRemark: When using ggplot2, words are by default ordered in alphabetical order, which is not what we want here. Therefore, mutate( word = reorder(word,n) ) is used to ensure that the words are instead ordered based on word frequency.\n\nWord clouds are a type of graphic that we can use to visualize the frequency of words within a text. Instead of using the visual cues “position” and “length” as in a bar plot, frequency is illustrated via the “size” of the words.\nThe R package wordcloud provides a function to produce a word cloud. For Jane Eyre, we produce a word cloud of the 40 most common words (ignoring the stop words) as follows:\n\nlibrary(wordcloud)\nJaneEyre_Count %&gt;%\n    with(wordcloud(word, n, max.words = 40, colors = topo.colors(n = 40)))\n\n\n\nWord cloud illustrating the frequency of the 40 most common words in Jane Eyre.\n\n\n\nRemark: Word clouds are good when we want to visualize term frequency for a large number of words. In all other cases, a bar plot is the better choice in terms of accessibility and the amount of information it provides.\n\n\nIn many applications we want to understand the emotional intent (sentiment) of a text. For instance, we may want to quickly determine whether a product has generally received more positive than negative reviews or not.\nHere we make the (strong) assumption that the sentiment of a text can be described by the aggregated sentiment of the individual words within it. This leads to the task of measuring the sentiment of a word. In text mining and natural language processing, sentiment lexicons are usually used when analyzing individual words. The tidydata R package provides access two sentiment lexicons:\n\nAFINN: Words are assigned a sentiment score between -5 and +5, with lower values corresponding to a more negative sentiment. For instance, “anxious” has a score of -2, while “pretty” has a score of 1.\nBing: Words are categorized as “positive” (e.g. “pretty”) or “negative” (e.g. “anxious”).\n\nWords not listed within the lexicon are considered “neutral” in terms of sentiment. We now apply these two sentiment lexicons to analyze the sentiment in Jane Eyre.\n\nTo load the AFINN sentiment lexicon, we use the get_sentiments() function in the tidytext package:\n\n# library(textdata)\n# lexicon_afinn()\nAFINN &lt;- get_sentiments(\"afinn\")\n\nLet’s again prepare the data and store the line number for later use:\n\nJaneEyre &lt;- JaneEyre_raw %&gt;%\n    mutate(line = row_number()) %&gt;%\n    unnest_tokens(word, text) %&gt;%\n    mutate(word = gsub(\"_\", \"\", word))\n\nThe next step is to extract the words that are in both Jane Eyre and the AFINN sentiment lexicon:\n\nJaneEyre_AFINN &lt;- inner_join(JaneEyre, AFINN)\n\nIt is important to note that sentiment lexicons have certain limitations. For instance, the word “miss” is associated with a negative sentiment within the AFINN sentiment lexicon, but in Jane Eyre it is usually used as the title of a young, unmarried woman - so we should better ignore the word “miss” in the analysis.\nImportant: When considering sentiment, we should not remove any stop words to ensure that our analysis considers all words in the text.\nLet’s create a plot to see how the aggregated sentiment evolves over the course of the novel, and we filter out the word “miss” based on our previous argument:\n\nJaneEyre_AFINN %&gt;%\n    filter(word != \"miss\") %&gt;%\n    mutate(sentiment = cumsum(value)) %&gt;%\n    ggplot(aes(x = line, y = sentiment)) +\n    geom_line(linewidth = 1.2) +\n    theme_bw() +\n    labs(x = \"Line number\", y = \"Sentiment\")\n\n\n\nEvolution of the AFINN sentiment score over the course of the book Jane Eyre. Negative values at the start indicate negative sentiment, but the sentiment becomes higher as the book comes to its conclusion.\n\n\n\nThe plot suggests that the first chapters of Jane Eyre have a more “negative” sentiment, while, as the story progresses, the novel develops a more “positive” sentiment.\nAnother way to visualize sentiment is to group lines into sections and to derive the sentiment of each section. Here we split the book into its chapters. We need some functions form the stringr package to identify the lines which start with “Chapter” and we then use this information to assign a chapter number to each line:\n\nlibrary(stringr)\nJaneEyre_chapters &lt;- JaneEyre_raw %&gt;%\n    mutate(chapter = cumsum(\n        str_detect(text, regex(\"^chapter \", ignore_case = TRUE))\n    ))\n\nWe now remove any lines that do not belong to one of the chapters and then split the book as before:\n\nJaneEyre_chapters &lt;- JaneEyre_chapters %&gt;%\n    filter(chapter &gt; 0) %&gt;%\n    unnest_tokens(word, text) %&gt;%\n    mutate(word = gsub(\"_\", \"\", word)) %&gt;%\n    filter(word != \"miss\")\n\nFinally, we calculate and visualize the aggregated sentiment for each chapter\n\nJaneEyre_chapters %&gt;%\n    inner_join(AFINN) %&gt;%\n    group_by(chapter) %&gt;%\n    summarise(sentiment = sum(value)) %&gt;%\n    ggplot(aes(x = chapter, y = sentiment)) +\n    geom_col() +\n    theme_bw() +\n    labs(x = \"Chapter\", y = \"AFINN sentiment score\")\n\n\n\nAFINN sentiment score for the chapters of Jane Eyre.\n\n\n\nThe plot shows a similar pattern to that identified for Figure @ref(fig:AFINN). If we consider a chapter with an AFINN score above 0 as “positive”, and “negative” otherwise, we find that the sentiment tends to remain “positive” across multiple consecutive chapters.\nWhen working with the Bing sentiment lexicon, we may count the numbers of “positive” and “negative” words in each chapter and visualize their proportions numbers using a stacked bar plot:\n\nBing &lt;- get_sentiments(\"bing\")\n\nJaneEyre_chapters %&gt;%\n    inner_join(Bing) %&gt;%\n    group_by(chapter) %&gt;%\n    count(chapter, sentiment) %&gt;%\n    ggplot(aes(x = chapter, y = n, fill = sentiment)) +\n    geom_col(position = \"fill\") +\n    geom_hline(yintercept = 0.5, color = \"black\", linewidth = 1.1) +\n    theme_bw() +\n    labs(x = \"Chapter\", y = \"Proportion\")\n\n\n\nBing sentiment score for the chapters of Jane Eyre.\n\n\n\nWhen comparing the two bar plots, and assuming that a chapter with a proportion of “positive” words above 50% is “positive”, we find good agreement in terms of whether a chapter has a “positive” or “negative” sentiment.\nImportant: Due to our assumption that the sentiment can be measured by considering the individual words, we should be cautious with making conclusions on whether a chapter is “positive” or “negative”. Nevertheless, the change in sentiment score still provides us with some information on which chapters tell a more “positive” story and which ones a more “negative” story, that is, we can still make conclusions by comparing the scores.\n\n\nSuppose that we have two separate texts / documents. In such cases, we may want to compare the term frequency of the various words in the two texts using a scatter plot.\nLet’s compare Jane Eyre to the novel Wuthering Heights by Emily Brontë, Charlotte Brontë’s sister. The first step is to calculate term frequencies for Wuthering Heights, just as we did for Jane Eyre:\n\nURL &lt;- \"https://www.gutenberg.org/files/768/768-0.txt\"\nWutheringHeights_raw &lt;- readLines(URL, encoding = \"UTF-8\")\nind &lt;- grep(\"EBOOK\", WutheringHeights_raw)\nWutheringHeights_raw &lt;- data.frame(text = WutheringHeights_raw[(ind[1] + 1):(ind[2] - 1)])\n\nWutheringHeights &lt;- WutheringHeights_raw %&gt;%\n    unnest_tokens(word, text) %&gt;%\n    mutate(word = gsub(\"_\", \"\", word)) %&gt;%\n    count(word, sort = TRUE) %&gt;%\n    mutate(\"term frequency\" = n / sum(n)) %&gt;%\n    anti_join(stop_words)\n\nTo compare term frequencies, we combine the two data frames for Jane Eyre and Wuthering Heights:\n\nfrequency_combined &lt;-\n    full_join(JaneEyre_Count, WutheringHeights, by = \"word\") %&gt;%\n    rename(JaneEyre = \"term frequency.x\", Heights = \"term frequency.y\")\n\nBefore analyzing the term frequencies, we have to account for words that only appear in one book being given a value of NA, such as the word “Jane”. Therefore, we replace any \\(\\mathrm{\\texttt{NA}}\\) entry with with a value of 0 (as we did in Problem Sheet 2, Tutorial Question 2):\n\nfrequency_combined &lt;- frequency_combined %&gt;%\n    mutate(\n        JaneEyre = case_when(is.na(JaneEyre) == TRUE ~ 0, .default = JaneEyre),\n        Heights = case_when(is.na(Heights) == TRUE ~ 0, .default = Heights)\n    )\n\nFinally, we create our scatter plot of the term frequencies\n\nggplot(frequency_combined, aes(x = JaneEyre, y = Heights)) +\n    geom_point() +\n    geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n    xlim(c(0, 0.003)) +\n    ylim(c(0, 0.003)) +\n    coord_trans(x = \"sqrt\", y = \"sqrt\") +\n    theme_bw() +\n    labs(\n        x = \"Term Frequency in Jane Eyre\",\n        y = \"Term Frequency in Wuthering Heights\"\n    )\n\n\n\nComparison of the term frequencies for the words in Jane Eyre and Wuthering Heights.\n\n\n\nWords close to the line, such as “door” or “time”, have a similar frequency in the two books while words that are far from the line, for instance, “Catherine” and “John”, are more specific to one of the books.\n\nWe already saw in the analysis of Jane Eyre that we use term frequency to measure frequency of a word \\(t\\) within a text or document \\(d\\). The formal definition of term frequency is [ (t,d)=. ]\nSuppose that we now want to study the frequency of a word \\(t\\) across a corpus \\(D\\), that is, a set of documents. The inverse document frequency \\(\\mathrm{idf}(t,D)\\) measures how common or rare the word \\(t\\) is across \\(D\\). Formally, \\(\\mathrm{idf}(t,D)\\) is defined as\n[ (t,D) = () = (), ] where \\(|D|\\) denotes the cardinality of the set \\(D\\), i.e., the number of documents in \\(D\\).\nWe can also say that \\(\\mathrm{idf}(t,D)\\) describes how specific the word \\(t\\) is to a document within \\(D\\). If \\(t\\) is contained in all documents, we get \\(\\mathrm{idf}(t,D)=0\\), while \\(\\mathrm{idf}(t,D)=\\log |D|\\) if \\(t\\) only appears in a single document. Consequently, a smaller value for \\(\\mathrm{idf}(t,D)\\) corresponds to \\(t\\) being more common across the documents in \\(D\\).\nFinally, we define the term frequency-inverse document frequency (tf-idf) as [ (t,d,D)=(t,d) (t,D). ]\nLet’s consider some scenarios\n\nTo get a very high value for \\(\\mathrm{tf.idf}(t,d,D)\\), we need \\(t\\) to occur frequently within the document \\(d\\), and not in any other document in \\(D\\).\nIf \\(t\\) occurs across all documents in the set \\(D\\), we have \\(\\mathrm{tf.idf}(t,d,D)=0\\). For instance, very common words such as \\(t\\)=“the” or \\(t\\)=“and” highly likely yield \\(\\mathrm{tf.idf}(t,d,D)=0\\). This is a nice feature, because we are usually not interested in these stop words.\n\nSo the term frequency-inverse document frequency is useful to assess how important a word is to a document, in relation to the overall set of documents. This feature is the reason that tf-idf is widely applied in search engines and text-based recommender systems. For instance, if we want to search for a website on a specific subject, described by a word \\(t\\), the suggestions provided by the search engine may use \\(\\mathrm{tf.idf}(t,d,D)\\) to rank websites, where \\(D\\) comprises all the websites considered by the search engine.\n\nSuppose our corpus comprises four books by Charles Dickens: A Christmas Carol, A Tale of Two Cities, Great Expectations and Oliver Twist. The text for the books from Project Gutenberg is available as “Dickens.csv” on Moodle:\n\nDickens_raw &lt;- read.csv(\"data/dickens.csv\" )\n\nAs in the analysis of Jane Eyre and Wuthering Heights, we use the unnest_tokens() function to separate the text into individual words, and we use gsub() to remove any underscores:\n\nDickens &lt;- Dickens_raw %&gt;%\n    unnest_tokens(word, text) %&gt;%\n    mutate(word = gsub(\"_\", \"\", word))\n\nNext we count the number of appearances of each word separately for each of the books:\n\nDickensCount &lt;- Dickens %&gt;% count(title, word, sort = TRUE)\nhead(DickensCount)\n\n                 title word    n\n1         Oliver Twist  the 9633\n2   Great Expectations  the 8145\n3 A Tale of Two Cities  the 8053\n4   Great Expectations  and 7098\n5   Great Expectations    i 6667\n6         Oliver Twist  and 5428\n\n\nThe quantities \\(\\mathrm{tf}(t,d)\\), \\(\\mathrm{idf}(t,D)\\) and \\(\\mathrm{tf.idf}(t,d,D)\\) for all terms and books are then calculated from this table using the function bind_tf_idf() in the tidytext package:\n\nDickens_tf.idf &lt;- DickensCount %&gt;%\n    bind_tf_idf(word, title, n) %&gt;%\n    arrange(desc(tf_idf)) %&gt;%\n    mutate(idf = round(idf, 2))\nhead(Dickens_tf.idf)\n\n                 title    word   n          tf  idf      tf_idf\n1    A Christmas Carol scrooge 327 0.011047297 1.39 0.015314806\n2         Oliver Twist  oliver 876 0.005396980 1.39 0.007481803\n3 A Tale of Two Cities   lorry 369 0.002664549 1.39 0.003693849\n4         Oliver Twist  bumble 397 0.002445892 1.39 0.003390726\n5         Oliver Twist   sikes 354 0.002180971 1.39 0.003023468\n6 A Tale of Two Cities defarge 302 0.002180742 1.39 0.003023150\n\n\nWe see that the words with the highest tf-idf are the names of the protagonists in the four books, while the term “the”, that had the highest count, is not there. It’s also common to say that “scrooge” is the most specific term across the considered set of books. The names with the highest tf-idf are also specific to a book and that’s why they have an idf of \\(\\log 4 \\approx 1.39\\).\n\nSo far, we have extracted the most common words within a text, and analyzed its sentiment. However, this information may not be sufficient in applications, e.g.:\n\nWhen working with a large collection of blog posts, news articles or scientific papers, we often wish to label these documents based on their content. It may be difficult to do this just based on the most common words.\nSummarizing a long piece of text in a few sentences. We cannot assume that the most common words or the sentiment are necessarily the important pieces of information.\n\nThis has led to the development of several statistical and natural language processing methods. In this chapter we introduce topic modeling, which aims to to discover “natural” groups (topics) of words, even when we are not sure what we are looking for.\n\nWhile there exists several approaches for topic modelling, we only explore Latent Dirichlet allocation (LDA), which is a particularly popular method. The key idea is to treat each document as a mixture of topics, and each topic as a mixture of words. Let’s illustrate these principles:\n\nEvery document is a mixture of topics. We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”\nEvery topic is a mixture of words. Suppose we considered BBC news and had topics such as “politics”, “sports”, “entertainment”, etc. The most common words in the “politics” topic might be “prime minister”, “Commons”, and “government”, while the “sports” topic includes “football”, “cricket”, etc. Importantly, words can be shared between topics; a word like “bench” might appear in both topics with the same frequency.\n\nThis framework allows documents to “overlap” each other in terms of content, in a way that mirrors typical use of natural language, rather than being separated into discrete groups. Based on a set of documents, the aim is then to find the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document at the same time.\nRemark: The framework above is different from the clustering methods you may have seen before, e.g., such as \\(k\\)-means where each data point belongs to exactly one cluster. LDA belongs to the class of soft (or fuzzy) clustering methods, which allow for a data point (in this case a document) to belong to more than one cluster.\n\nLet’s dive briefly into the mathematics behind the LDA approach before focusing on exploring how to fit the model and interpret the results. We start by introducing the Dirichlet distribution:\nDefinition: The Dirichlet distribution is a family of continuous multivariate probability distributions on the d-dimensional unit simplex \\(\\{(x_1,\\ldots,x_d)\\in\\mathbb{R}_+^d: x_1+\\cdots+x_d=1\\}\\) with \\(d&gt;1\\). A Dirichlet distribution is parametrized by a \\(d\\)-dimensional vector of positive reals.\nNow suppose we have a corpus with \\(N\\) documents, with a total of \\(M\\) different words, and we specify that there are different \\(K\\) topics. The LDA approach then describes two Dirichlet distributions.\n\nDistribution of words within a topic: The term frequencies with which the \\(M\\) distinct words are expected to appear within a document from topic \\(k\\in\\{1,\\ldots,K\\}\\) is described by a M-dimensional Dirichlet distribution, where each topic has its own set of parameters.\nDistribution of topics within a document: The proportions \\((\\psi_{i,1},\\ldots,\\psi_{i,K})\\) with which the \\(K\\) topics feature in document \\(i\\in\\{1,\\ldots,N\\}\\) are modelled via a \\(K\\)-dimensional Dirichlet distribution. These proportions allow us to group the different documents.\n\nWe will stop considering the theoretical details of the LDA approach at this point, because the estimation of the parameter vectors is too complex to be covered in a Year 2 unit (we would require tools from Bayesian statistics). In the following, we study two examples to illustrate how to perform topic modelling in R.\n\nTo illustrate LDA we first consider an example where we know the ‘truth’. Here we will take two books by Charles Dickens and split each up into their individual chapters. We then want to see how well LDA performs at distinguishing between the two books.\nWe consider the books Great Expectations and A Tale of Two Cities we studied previously:\n\nDickens_raw &lt;- read.csv(\"data/dickens.csv\" )\nDickens &lt;- filter(\n    Dickens_raw,\n    title %in% c(\"Great Expectations\", \"A Tale of Two Cities\")\n)\n\n\nWe want to split the books into their chapters, with each chapter being handled as a separate document. As in Section 3.2, we extract the chapter the individual lines in the text document belong to:\n\nDickens_chapters &lt;- Dickens %&gt;%\n    group_by(title) %&gt;%\n    mutate(chapter = cumsum(\n        str_detect(text, regex(\"^chapter \", ignore_case = TRUE))\n    ))\n\nThe next step is to remove any text before the first chapter, and we further us the unite() function from the tidyr R package to create an identifier document from the book title and chapter number:\n\nlibrary(tidyr)\nDickens_chapters &lt;- Dickens_chapters %&gt;%\n    filter(chapter &gt; 0) %&gt;%\n    unite(col = document, title, chapter)\n\nFinally, we split the documents into words, remove any stop words, and count the frequency for each word within each document, as we did when analyzing word frequency:\n\nDickens_chapters &lt;- Dickens_chapters %&gt;%\n    unnest_tokens(word, text) %&gt;%\n    mutate(word = gsub(\"_\", \"\", word)) %&gt;%\n    anti_join(stop_words, by = \"word\") %&gt;%\n    count(document, word, sort = TRUE)\n\nRemark: Stop words are removed because they usually perform poorly at explaining differences in the underlying topics. Further, their high prevalence in all topics leads to topics appearing more similar than they are.\n\nThe R package we are using for topic modelling requires the data to be stored as a document term matrix which takes the following form:\n\nEach row represents one document (a book chapter in our case).\nEach column corresponds to one term / word.\nEach entry contains the number of appearances of a term in a document.\n\nThe tidytext package provides the cast_dtm() function to create such a matrix, but we need to install the tm R package to do this:\n\nDickens_dtm &lt;- Dickens_chapters %&gt;% cast_dtm(document, word, n)\n\n\nWith everything in place, we now use the LDA() function in the topicmodels R package. One key decision we have to make is specifying the number \\(K\\) of topics. For the purpose of this analysis, it seems logical to set the number of topics to \\(K=2\\):\n\nlibrary(topicmodels)\nDickens_LDA &lt;- LDA(Dickens_dtm, k = 2, method = \"Gibbs\", control = list(seed = 123))\n\nRemark: The estimates are sensitive to the value we specify in control=list(seed=…). In this course you may ignore the sensitivity and focus on analyzing the results obtained.\n\nHaving fitted the model, we want to explore the estimated model parameters. Recall that we have\n\n\\(K\\) parameters per document, which represent how much the \\(K\\) topics feature in the document\n\\(M\\) parameters per topic, which give insight on how frequent each word features within a topic\n\nTo extract the \\(K\\) proportions for each of the documents, we use the following R code:\n\nDickens_topics &lt;- tidy(Dickens_LDA, matrix = \"gamma\")\nDickens_topics %&gt;% slice_head(n = 4)\n\n# A tibble: 4 x 3\n  document              topic gamma\n  &lt;chr&gt;                 &lt;int&gt; &lt;dbl&gt;\n1 Great Expectations_57     1 0.310\n2 Great Expectations_7      1 0.190\n3 Great Expectations_38     1 0.370\n4 Great Expectations_17     1 0.237\n\n\nThe values in the column gamma correspond to the estimated proportions. We see that Chapter 7, 17, 38 and 57 from Great Expectations feature more of Topic 2 than Topic 1.\nGiven the high number of chapters, it’s better to visualize the estimates in order to discuss agreement between the estimated topics and the underlying truth. One option is to create box plots:\n\nDickens_topics %&gt;%\n    separate(document, c(\"title\", \"chapter\"), sep = \"_\", convert = TRUE) %&gt;%\n    ggplot(aes(x = factor(topic), y = gamma)) +\n    facet_wrap(~title) +\n    geom_boxplot() +\n    labs(x = \"Topic\", y = \"Proportion\")\n\n\n\nIllustration of the estimated proportions for each chapter within each book.\n\n\n\nWe see a clear difference in the plots for the two books. Most of the chapters from A Tale of Two Cities mostly consist of words that belong to Topic 1, while for Great Expectations the chapters are composed of words from Topic 2. As such, the topics could be used to classify the individual chapters.\nSuppose we would label a chapter as “A Tale of Two Cities” if the proportion of text estimated to be from Topic 1 exceeds 0.5. Such a classification would work well, with only one chapter per book being misclassified. This result may be due to some chapters rarely mentioning important characters and given that both books were written by Charles Dickens, correctly labeling these chapters is more challenging.\nLet’s look at the words most common in the different topics. We extract the top five words for each topic:\n\ntidy(Dickens_LDA, matrix = \"beta\") %&gt;%\n    group_by(topic) %&gt;%\n    slice_max(beta, n = 3)\n\n# A tibble: 6 x 3\n# Groups:   topic [2]\n  topic term     beta\n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1 lorry 0.00690\n2     1 hand  0.00634\n3     1 time  0.00600\n4     2 joe   0.0146 \n5     2 miss  0.00951\n6     2 don   0.00812\n\n\nIt seems that character names plan an important role when deciding whether a text belongs to Topic 1 or Topic 2, which is kind of what we expect. Let’s produce a plot to compare the term frequencies for the different words in more detail:\n\ntidy(Dickens_LDA, matrix = \"beta\") %&gt;%\n    mutate(topic = case_when(topic == 1 ~ \"Topic1\", topic == 2 ~ \"Topic2\")) %&gt;%\n    pivot_wider(names_from = topic, values_from = beta, values_fill = 0) %&gt;%\n    ggplot(aes(x = Topic1, y = Topic2)) +\n    geom_point() +\n    geom_text(aes(label = term), check_overlap = TRUE, vjust = 1) +\n    coord_trans(x = \"sqrt\", y = \"sqrt\") +\n    theme_bw() +\n    labs(x = \"Term Frequency in Topic 1\", y = \"Term Frequency in Topic 2\")\n\n\n\nComparison of term frequencies for the two topics.\n\n\n\nThe plot shows some very interesting patterns. In particular, the two topics seem to mainly differ in terms of the term frequency of a subset of words (the ones located close to the axes), while the term frequencies are very similar for the majority of words.\n\nLet’s look at an example where there is no ‘truth’ as such. The file “NYT.csv” provides metadata for 1289 articles published by the New York Times in their “Europe” subsection between 01/01/2023 and 01/11/2024:\n\nNYT &lt;- read.csv(\"data/nyt.csv\" )\n\nSpecifically, the data file gives for each article the lead paragraph and the date it was published. We want to explore the results we obtain when applying LDA.\n\nSince we take each article to be a separate document, we first assign a unique identifier to each article:\n\nNYT &lt;- NYT %&gt;% mutate(ID = 1:nrow(NYT))\n\nNow we follow the same procedure as in the previous analysis. As such, we start by splitting the sentences into individual words, remove any stop words and count the number of occurrences for each article individually:\n\nNYT_count &lt;- NYT %&gt;%\n    unnest_tokens(word, lead_paragraph) %&gt;%\n    anti_join(stop_words, by = \"word\") %&gt;%\n    count(ID, word, sort = TRUE)\n\nThe next step is to convert the counts into a document term matrix:\n\nNYT_dtm &lt;- NYT_count %&gt;% cast_dtm(ID, word, n)\n\nFinally, we are ready to run the LDA() function and we choose to set \\(K=2\\):\n\nNYT_LDA &lt;- LDA(NYT_dtm, k = 2, method = \"Gibbs\", control = list(seed = 2024))\n\n\nLet’s start by investigating the make-up of the articles. This requires us to explore with which proportions the two topics feature in the different articles. One option is to again create a box plot:\n\ntidy(NYT_LDA, matrix = \"gamma\") %&gt;%\n    ggplot(aes(y = gamma)) +\n    theme_bw() +\n    geom_boxplot() +\n    labs(y = \"Proportion\")\n\n\n\nBoxplot of the estimated proportions for the articles.\n\n\n\nThe result seems a bit unsatisfying as the make-up only varies between a 65-35 and 35-65 split. Consequently, the estimates suggests that there are very little differences in the language used within the topics.\nTo explore the topics in more detail, we plot the estimated term frequencies for the two topics against each other:\n\ntidy(NYT_LDA, matrix = \"beta\") %&gt;%\n    mutate(topic = case_when(topic == 1 ~ \"Topic1\", topic == 2 ~ \"Topic2\")) %&gt;%\n    pivot_wider(names_from = topic, values_from = beta, values_fill = 0) %&gt;%\n    ggplot(aes(x = Topic1, y = Topic2)) +\n    coord_trans(x = \"sqrt\", y = \"sqrt\") +\n    geom_point() +\n    theme_bw() +\n    geom_text(aes(label = term), check_overlap = TRUE, vjust = 1) +\n    labs(x = \"Term Frequency in Topic 1\", y = \"Term Frequency in Topic 2\")\n\n\n\nComparison of term frequencies for the two topics.\n\n\n\nThis plot provides some more insight. We see that Topic 1 seems to feature more of the words related to the War in Ukraine, but it’s not a perfect plot - we note that for instance “missile” features more in Topic 2 although this topic mainly occurred in the context of the war.\nTo summarize, we find that LDA produces two topics which seem quite sensible, when considering the term frequencies of the words. However, it’s also important to point out that LDA does not really split the documents into separate groups, due to each document only containing a few words. This is quite a common feature, as the flexibility of the LDA approach comes at the cost of it requiring a lot of data for model fitting.\n\nWe considered several aspects related to the analysis of text data in R:\n\nThe tidy text format is useful for the analysis of text data\nWord frequency can be visualized using bar plots or word clouds\nSentiment analysis can be used to study the emotional intent. We considered one approach based on sentiment lexicons, but also highlighted that we have to make a strong assumption\nDocuments can be compared by plotting term frequencies against each other and analyzing the term frequency - inverse document frequency\nLatent Dirichlet allocation is one method that may be used for topic modelling, but results need to be studied carefully\n\nSome other aspects one should be aware of:\n\nWe exclusively based our analyses on the individual words. However, in some applications it may be better to consider n-grams, which are compositions of \\(n\\) (usually consecutive) words. The unnest_tokens() provides options to extract n-grams from a text.\nThe principles introduced in this chapter can be applied to any type of text data, such as social media messages or song lyrics, and are not limited to short texts and books.\nYou should be mindful of copyright when scraping text data from the internet. That’s why we considered older books rather than recent bestsellers.",
    "crumbs": [
      "Lecture Notes",
      "Text Data Analysis"
    ]
  },
  {
    "objectID": "lecture_notes/03-Text-Data-Analysis.html#analyzing-word-frequency",
    "href": "lecture_notes/03-Text-Data-Analysis.html#analyzing-word-frequency",
    "title": "Text Data Analysis",
    "section": "",
    "text": "Consider the following quote by Hermann Hesse (a German-Swiss poet):\n\n  \"Some of us think holding on makes us strong,\nbut sometimes it is letting go.\"\n  \nThe text is stored in the file “Hesse quote.txt” on Moodle, with the quote being split into two lines as above. To load the data into R, we use the function readLines():\n\ntext_Hesse &lt;- readLines(\"data/hesse_quote.txt\")\ntext_Hesse\n\n[1] \"Some of us think holding on makes us strong,\"\n[2] \"but sometimes it is letting go.\"             \n\n\nWe see that text_Hesse is a vector with two entries, each entry corresponding to a line in the .txt file. The entries are of type character and are also referred to as strings, i.e, combinations of words.\nLet’s store the data as a data frame:\n\nquote_Hesse &lt;- data.frame(line = 1:2, text = text_Hesse)\n\nThe text data in its current format is of little use, because we usually want to analyze single words within a text. As such, we need to separate the strings into individual words. Further, any punctuation should be removed because we are not interested in it.\nThe function unnest_tokens() in the tidytext R package does all of this for us:\n\nlibrary(tidytext)\nlibrary(dplyr)\nHesse_tidy &lt;- quote_Hesse %&gt;% unnest_tokens(output = word, input = text)\nHesse_tidy %&gt;% slice_head(n = 5)\n\n  line    word\n1    1    some\n2    1      of\n3    1      us\n4    1   think\n5    1 holding\n\n\nThe data are now stored in the column word. This format is known as the tidy text format and it follows two principles:\n\nEach variable is a column: our variable of interest is word;\nEach observation is a row: we have one word per row.\n\nNow that the text is in this new format, compared to the previous chapters, the only difference is that we have to deal with non-numerical observations. However, there are a wide range of techniques unique to analyze of such data.\n\nWhen analyzing text data, we often want to extract words which are used frequently. Bar plots and word clouds are widely applied graphics to visualize word frequency, i.e. the number of times a word appears. In the following, we demonstrate how these plots can be created in R for the book Jane Eyre by Charlotte Brontë.\n\nThe full text for Jane Eyre is freely available from Project Gutenberg; we only require the Gutenberg ID for the book, which is 1260.\n\nURL &lt;- \"https://www.gutenberg.org/cache/epub/1260/pg1260.txt\"\nJaneEyre_raw &lt;- readLines(URL, encoding = \"UTF-8\")\n\nWe have to remove the meta data and disclaimers at the beginning and end of the file. We do this by detecting the lines containing the word “EBOOK”, which signals the beginning and end of the book:\n\nind &lt;- grep(\"EBOOK\", JaneEyre_raw)\nJaneEyre_raw &lt;- data.frame(text = JaneEyre_raw[(ind[1] + 1):(ind[2] - 1)])\n\nThe individual rows in JaneEyre_raw represent one line from the book, as provided in the printed version stored on the Project Gutenberg website, with the first lines being from the title page.\nAfter importing the data, we separate the lines of text into individual words using unnest_tokens():\n\nJaneEyre &lt;- JaneEyre_raw %&gt;% unnest_tokens(word, text)\n\nA bit more data cleaning is required before starting the analysis. Specifically, some of the extracted words appear with an “_“, such as the eighth extracted word -”_illustrated”. This indicates that the word is printed in italics, but we do not want to tread “the” and “the” differently. To remove these underscores, we apply the gsub() function we saw in Problem Class 1:\n\nJaneEyre$word &lt;- gsub(\"_\", \"\", JaneEyre$word)\n\n\nWith the data in the tidy text format, the function count() from the dplyr R package can be used to extract the number of times each word was used. We further calculate each word’s proportion amongst the total number of words, called the term frequency, and its rank:\n\nJaneEyre_Count &lt;- JaneEyre %&gt;%\n    count(word, sort = TRUE) %&gt;%\n    mutate(\"term frequency\" = n / sum(n), rank = row_number())\nslice_head(JaneEyre_Count, n = 5)\n\n  word    n term frequency rank\n1  the 7856     0.04169475    1\n2    i 7172     0.03806451    2\n3  and 6632     0.03519852    3\n4   to 5238     0.02780004    4\n5    a 4475     0.02375051    5\n\n\nLooking at the output, the words with the highest term frequency are the ones we usually use when writing a longer piece of text.\n\nBefore analyzing the words most commonly used in Jane Eyre in more detail, we create a plot of term frequency versus rank, both on logarithmic scale:\n\nlibrary(ggplot2)\nggplot(JaneEyre_Count, aes(x = rank, y = `term frequency`)) +\n    geom_point(size = 1.5) +\n    coord_trans(x = \"log10\", y = \"log10\") +\n    theme_bw() +\n    theme(axis.title = element_text(size = 15))\n\n\n\nPlot of rank versus term frequency for the words in Jane Eyre, illustrating the negative proportionality stated by Zipf’s Law.\n\n\n\nThere is pretty much a linear relationship between rank and term frequency on logarithmic scale (apart from the words with the highest term frequency). This is known as Zipf’s Law which states that empirically a word’s term frequency is inversely proportional to its rank.\nRemark: Creating this plot is not necessary in a text analysis. We just produced it to illustrate Zipf’s Law.\n\nWe saw that the five words with the highest term frequency in JaneEyre_Count include “the”, “I” and “and”. This is quite common when analyzing longer pieces of text. One may argue that such words are not of interest in an analysis, because we cannot write a text without using them.\nThis has led to the concept of stop words. The idea is to specify words that are dropped from the analysis, because they are not relevant. A set of stop words is called a stop list.\nThe tidytext R package provides its own list, stop_words, which includes 1149 stop words. We can remove these stop words from JaneEyre_Count using the function anti_join() from the dplyr R package:\n\ndata(stop_words)\nJaneEyre_Count &lt;- JaneEyre_Count %&gt;% anti_join(stop_words)\nslice_head(JaneEyre_Count, n = 5)\n\n       word   n term frequency rank\n1      jane 341    0.001809815   69\n2 rochester 317    0.001682438   71\n3       sir 316    0.001677131   72\n4      miss 310    0.001645287   73\n5      time 244    0.001295000   98\n\n\n\nLet’s create a bar plot for the most commonly used words in Jane Eyre that are not on the stop list:\n\nJaneEyre_Count %&gt;%\n    slice_head(n = 10) %&gt;%\n    mutate(word = reorder(word, n)) %&gt;%\n    ggplot(aes(x = n, y = word)) +\n    geom_col() +\n    labs(x = \"Count\", y = \"Word\") +\n    theme_bw() +\n    theme(axis.title = element_text(size = 17), axis.text = element_text(size = 15))\n\n\n\nWord frequency of the most commonly used words in Jane Eyre.\n\n\n\nWe see that the names of two main characters, Jane Eyre and Edward Rochester, and their titles appear the most often, followed by “time”, “day”, “looked” and “night”.\nRemark: When using ggplot2, words are by default ordered in alphabetical order, which is not what we want here. Therefore, mutate( word = reorder(word,n) ) is used to ensure that the words are instead ordered based on word frequency.\n\nWord clouds are a type of graphic that we can use to visualize the frequency of words within a text. Instead of using the visual cues “position” and “length” as in a bar plot, frequency is illustrated via the “size” of the words.\nThe R package wordcloud provides a function to produce a word cloud. For Jane Eyre, we produce a word cloud of the 40 most common words (ignoring the stop words) as follows:\n\nlibrary(wordcloud)\nJaneEyre_Count %&gt;%\n    with(wordcloud(word, n, max.words = 40, colors = topo.colors(n = 40)))\n\n\n\nWord cloud illustrating the frequency of the 40 most common words in Jane Eyre.\n\n\n\nRemark: Word clouds are good when we want to visualize term frequency for a large number of words. In all other cases, a bar plot is the better choice in terms of accessibility and the amount of information it provides.",
    "crumbs": [
      "Lecture Notes",
      "Text Data Analysis"
    ]
  },
  {
    "objectID": "lecture_notes/03-Text-Data-Analysis.html#sentiment-analysis",
    "href": "lecture_notes/03-Text-Data-Analysis.html#sentiment-analysis",
    "title": "Text Data Analysis",
    "section": "",
    "text": "In many applications we want to understand the emotional intent (sentiment) of a text. For instance, we may want to quickly determine whether a product has generally received more positive than negative reviews or not.\nHere we make the (strong) assumption that the sentiment of a text can be described by the aggregated sentiment of the individual words within it. This leads to the task of measuring the sentiment of a word. In text mining and natural language processing, sentiment lexicons are usually used when analyzing individual words. The tidydata R package provides access two sentiment lexicons:\n\nAFINN: Words are assigned a sentiment score between -5 and +5, with lower values corresponding to a more negative sentiment. For instance, “anxious” has a score of -2, while “pretty” has a score of 1.\nBing: Words are categorized as “positive” (e.g. “pretty”) or “negative” (e.g. “anxious”).\n\nWords not listed within the lexicon are considered “neutral” in terms of sentiment. We now apply these two sentiment lexicons to analyze the sentiment in Jane Eyre.\n\nTo load the AFINN sentiment lexicon, we use the get_sentiments() function in the tidytext package:\n\n# library(textdata)\n# lexicon_afinn()\nAFINN &lt;- get_sentiments(\"afinn\")\n\nLet’s again prepare the data and store the line number for later use:\n\nJaneEyre &lt;- JaneEyre_raw %&gt;%\n    mutate(line = row_number()) %&gt;%\n    unnest_tokens(word, text) %&gt;%\n    mutate(word = gsub(\"_\", \"\", word))\n\nThe next step is to extract the words that are in both Jane Eyre and the AFINN sentiment lexicon:\n\nJaneEyre_AFINN &lt;- inner_join(JaneEyre, AFINN)\n\nIt is important to note that sentiment lexicons have certain limitations. For instance, the word “miss” is associated with a negative sentiment within the AFINN sentiment lexicon, but in Jane Eyre it is usually used as the title of a young, unmarried woman - so we should better ignore the word “miss” in the analysis.\nImportant: When considering sentiment, we should not remove any stop words to ensure that our analysis considers all words in the text.\nLet’s create a plot to see how the aggregated sentiment evolves over the course of the novel, and we filter out the word “miss” based on our previous argument:\n\nJaneEyre_AFINN %&gt;%\n    filter(word != \"miss\") %&gt;%\n    mutate(sentiment = cumsum(value)) %&gt;%\n    ggplot(aes(x = line, y = sentiment)) +\n    geom_line(linewidth = 1.2) +\n    theme_bw() +\n    labs(x = \"Line number\", y = \"Sentiment\")\n\n\n\nEvolution of the AFINN sentiment score over the course of the book Jane Eyre. Negative values at the start indicate negative sentiment, but the sentiment becomes higher as the book comes to its conclusion.\n\n\n\nThe plot suggests that the first chapters of Jane Eyre have a more “negative” sentiment, while, as the story progresses, the novel develops a more “positive” sentiment.\nAnother way to visualize sentiment is to group lines into sections and to derive the sentiment of each section. Here we split the book into its chapters. We need some functions form the stringr package to identify the lines which start with “Chapter” and we then use this information to assign a chapter number to each line:\n\nlibrary(stringr)\nJaneEyre_chapters &lt;- JaneEyre_raw %&gt;%\n    mutate(chapter = cumsum(\n        str_detect(text, regex(\"^chapter \", ignore_case = TRUE))\n    ))\n\nWe now remove any lines that do not belong to one of the chapters and then split the book as before:\n\nJaneEyre_chapters &lt;- JaneEyre_chapters %&gt;%\n    filter(chapter &gt; 0) %&gt;%\n    unnest_tokens(word, text) %&gt;%\n    mutate(word = gsub(\"_\", \"\", word)) %&gt;%\n    filter(word != \"miss\")\n\nFinally, we calculate and visualize the aggregated sentiment for each chapter\n\nJaneEyre_chapters %&gt;%\n    inner_join(AFINN) %&gt;%\n    group_by(chapter) %&gt;%\n    summarise(sentiment = sum(value)) %&gt;%\n    ggplot(aes(x = chapter, y = sentiment)) +\n    geom_col() +\n    theme_bw() +\n    labs(x = \"Chapter\", y = \"AFINN sentiment score\")\n\n\n\nAFINN sentiment score for the chapters of Jane Eyre.\n\n\n\nThe plot shows a similar pattern to that identified for Figure @ref(fig:AFINN). If we consider a chapter with an AFINN score above 0 as “positive”, and “negative” otherwise, we find that the sentiment tends to remain “positive” across multiple consecutive chapters.\nWhen working with the Bing sentiment lexicon, we may count the numbers of “positive” and “negative” words in each chapter and visualize their proportions numbers using a stacked bar plot:\n\nBing &lt;- get_sentiments(\"bing\")\n\nJaneEyre_chapters %&gt;%\n    inner_join(Bing) %&gt;%\n    group_by(chapter) %&gt;%\n    count(chapter, sentiment) %&gt;%\n    ggplot(aes(x = chapter, y = n, fill = sentiment)) +\n    geom_col(position = \"fill\") +\n    geom_hline(yintercept = 0.5, color = \"black\", linewidth = 1.1) +\n    theme_bw() +\n    labs(x = \"Chapter\", y = \"Proportion\")\n\n\n\nBing sentiment score for the chapters of Jane Eyre.\n\n\n\nWhen comparing the two bar plots, and assuming that a chapter with a proportion of “positive” words above 50% is “positive”, we find good agreement in terms of whether a chapter has a “positive” or “negative” sentiment.\nImportant: Due to our assumption that the sentiment can be measured by considering the individual words, we should be cautious with making conclusions on whether a chapter is “positive” or “negative”. Nevertheless, the change in sentiment score still provides us with some information on which chapters tell a more “positive” story and which ones a more “negative” story, that is, we can still make conclusions by comparing the scores.",
    "crumbs": [
      "Lecture Notes",
      "Text Data Analysis"
    ]
  },
  {
    "objectID": "lecture_notes/03-Text-Data-Analysis.html#comparing-text-documents",
    "href": "lecture_notes/03-Text-Data-Analysis.html#comparing-text-documents",
    "title": "Text Data Analysis",
    "section": "",
    "text": "Suppose that we have two separate texts / documents. In such cases, we may want to compare the term frequency of the various words in the two texts using a scatter plot.\nLet’s compare Jane Eyre to the novel Wuthering Heights by Emily Brontë, Charlotte Brontë’s sister. The first step is to calculate term frequencies for Wuthering Heights, just as we did for Jane Eyre:\n\nURL &lt;- \"https://www.gutenberg.org/files/768/768-0.txt\"\nWutheringHeights_raw &lt;- readLines(URL, encoding = \"UTF-8\")\nind &lt;- grep(\"EBOOK\", WutheringHeights_raw)\nWutheringHeights_raw &lt;- data.frame(text = WutheringHeights_raw[(ind[1] + 1):(ind[2] - 1)])\n\nWutheringHeights &lt;- WutheringHeights_raw %&gt;%\n    unnest_tokens(word, text) %&gt;%\n    mutate(word = gsub(\"_\", \"\", word)) %&gt;%\n    count(word, sort = TRUE) %&gt;%\n    mutate(\"term frequency\" = n / sum(n)) %&gt;%\n    anti_join(stop_words)\n\nTo compare term frequencies, we combine the two data frames for Jane Eyre and Wuthering Heights:\n\nfrequency_combined &lt;-\n    full_join(JaneEyre_Count, WutheringHeights, by = \"word\") %&gt;%\n    rename(JaneEyre = \"term frequency.x\", Heights = \"term frequency.y\")\n\nBefore analyzing the term frequencies, we have to account for words that only appear in one book being given a value of NA, such as the word “Jane”. Therefore, we replace any \\(\\mathrm{\\texttt{NA}}\\) entry with with a value of 0 (as we did in Problem Sheet 2, Tutorial Question 2):\n\nfrequency_combined &lt;- frequency_combined %&gt;%\n    mutate(\n        JaneEyre = case_when(is.na(JaneEyre) == TRUE ~ 0, .default = JaneEyre),\n        Heights = case_when(is.na(Heights) == TRUE ~ 0, .default = Heights)\n    )\n\nFinally, we create our scatter plot of the term frequencies\n\nggplot(frequency_combined, aes(x = JaneEyre, y = Heights)) +\n    geom_point() +\n    geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n    xlim(c(0, 0.003)) +\n    ylim(c(0, 0.003)) +\n    coord_trans(x = \"sqrt\", y = \"sqrt\") +\n    theme_bw() +\n    labs(\n        x = \"Term Frequency in Jane Eyre\",\n        y = \"Term Frequency in Wuthering Heights\"\n    )\n\n\n\nComparison of the term frequencies for the words in Jane Eyre and Wuthering Heights.\n\n\n\nWords close to the line, such as “door” or “time”, have a similar frequency in the two books while words that are far from the line, for instance, “Catherine” and “John”, are more specific to one of the books.\n\nWe already saw in the analysis of Jane Eyre that we use term frequency to measure frequency of a word \\(t\\) within a text or document \\(d\\). The formal definition of term frequency is [ (t,d)=. ]\nSuppose that we now want to study the frequency of a word \\(t\\) across a corpus \\(D\\), that is, a set of documents. The inverse document frequency \\(\\mathrm{idf}(t,D)\\) measures how common or rare the word \\(t\\) is across \\(D\\). Formally, \\(\\mathrm{idf}(t,D)\\) is defined as\n[ (t,D) = () = (), ] where \\(|D|\\) denotes the cardinality of the set \\(D\\), i.e., the number of documents in \\(D\\).\nWe can also say that \\(\\mathrm{idf}(t,D)\\) describes how specific the word \\(t\\) is to a document within \\(D\\). If \\(t\\) is contained in all documents, we get \\(\\mathrm{idf}(t,D)=0\\), while \\(\\mathrm{idf}(t,D)=\\log |D|\\) if \\(t\\) only appears in a single document. Consequently, a smaller value for \\(\\mathrm{idf}(t,D)\\) corresponds to \\(t\\) being more common across the documents in \\(D\\).\nFinally, we define the term frequency-inverse document frequency (tf-idf) as [ (t,d,D)=(t,d) (t,D). ]\nLet’s consider some scenarios\n\nTo get a very high value for \\(\\mathrm{tf.idf}(t,d,D)\\), we need \\(t\\) to occur frequently within the document \\(d\\), and not in any other document in \\(D\\).\nIf \\(t\\) occurs across all documents in the set \\(D\\), we have \\(\\mathrm{tf.idf}(t,d,D)=0\\). For instance, very common words such as \\(t\\)=“the” or \\(t\\)=“and” highly likely yield \\(\\mathrm{tf.idf}(t,d,D)=0\\). This is a nice feature, because we are usually not interested in these stop words.\n\nSo the term frequency-inverse document frequency is useful to assess how important a word is to a document, in relation to the overall set of documents. This feature is the reason that tf-idf is widely applied in search engines and text-based recommender systems. For instance, if we want to search for a website on a specific subject, described by a word \\(t\\), the suggestions provided by the search engine may use \\(\\mathrm{tf.idf}(t,d,D)\\) to rank websites, where \\(D\\) comprises all the websites considered by the search engine.\n\nSuppose our corpus comprises four books by Charles Dickens: A Christmas Carol, A Tale of Two Cities, Great Expectations and Oliver Twist. The text for the books from Project Gutenberg is available as “Dickens.csv” on Moodle:\n\nDickens_raw &lt;- read.csv(\"data/dickens.csv\" )\n\nAs in the analysis of Jane Eyre and Wuthering Heights, we use the unnest_tokens() function to separate the text into individual words, and we use gsub() to remove any underscores:\n\nDickens &lt;- Dickens_raw %&gt;%\n    unnest_tokens(word, text) %&gt;%\n    mutate(word = gsub(\"_\", \"\", word))\n\nNext we count the number of appearances of each word separately for each of the books:\n\nDickensCount &lt;- Dickens %&gt;% count(title, word, sort = TRUE)\nhead(DickensCount)\n\n                 title word    n\n1         Oliver Twist  the 9633\n2   Great Expectations  the 8145\n3 A Tale of Two Cities  the 8053\n4   Great Expectations  and 7098\n5   Great Expectations    i 6667\n6         Oliver Twist  and 5428\n\n\nThe quantities \\(\\mathrm{tf}(t,d)\\), \\(\\mathrm{idf}(t,D)\\) and \\(\\mathrm{tf.idf}(t,d,D)\\) for all terms and books are then calculated from this table using the function bind_tf_idf() in the tidytext package:\n\nDickens_tf.idf &lt;- DickensCount %&gt;%\n    bind_tf_idf(word, title, n) %&gt;%\n    arrange(desc(tf_idf)) %&gt;%\n    mutate(idf = round(idf, 2))\nhead(Dickens_tf.idf)\n\n                 title    word   n          tf  idf      tf_idf\n1    A Christmas Carol scrooge 327 0.011047297 1.39 0.015314806\n2         Oliver Twist  oliver 876 0.005396980 1.39 0.007481803\n3 A Tale of Two Cities   lorry 369 0.002664549 1.39 0.003693849\n4         Oliver Twist  bumble 397 0.002445892 1.39 0.003390726\n5         Oliver Twist   sikes 354 0.002180971 1.39 0.003023468\n6 A Tale of Two Cities defarge 302 0.002180742 1.39 0.003023150\n\n\nWe see that the words with the highest tf-idf are the names of the protagonists in the four books, while the term “the”, that had the highest count, is not there. It’s also common to say that “scrooge” is the most specific term across the considered set of books. The names with the highest tf-idf are also specific to a book and that’s why they have an idf of \\(\\log 4 \\approx 1.39\\).",
    "crumbs": [
      "Lecture Notes",
      "Text Data Analysis"
    ]
  },
  {
    "objectID": "lecture_notes/03-Text-Data-Analysis.html#topic-modelling",
    "href": "lecture_notes/03-Text-Data-Analysis.html#topic-modelling",
    "title": "Text Data Analysis",
    "section": "",
    "text": "So far, we have extracted the most common words within a text, and analyzed its sentiment. However, this information may not be sufficient in applications, e.g.:\n\nWhen working with a large collection of blog posts, news articles or scientific papers, we often wish to label these documents based on their content. It may be difficult to do this just based on the most common words.\nSummarizing a long piece of text in a few sentences. We cannot assume that the most common words or the sentiment are necessarily the important pieces of information.\n\nThis has led to the development of several statistical and natural language processing methods. In this chapter we introduce topic modeling, which aims to to discover “natural” groups (topics) of words, even when we are not sure what we are looking for.\n\nWhile there exists several approaches for topic modelling, we only explore Latent Dirichlet allocation (LDA), which is a particularly popular method. The key idea is to treat each document as a mixture of topics, and each topic as a mixture of words. Let’s illustrate these principles:\n\nEvery document is a mixture of topics. We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”\nEvery topic is a mixture of words. Suppose we considered BBC news and had topics such as “politics”, “sports”, “entertainment”, etc. The most common words in the “politics” topic might be “prime minister”, “Commons”, and “government”, while the “sports” topic includes “football”, “cricket”, etc. Importantly, words can be shared between topics; a word like “bench” might appear in both topics with the same frequency.\n\nThis framework allows documents to “overlap” each other in terms of content, in a way that mirrors typical use of natural language, rather than being separated into discrete groups. Based on a set of documents, the aim is then to find the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document at the same time.\nRemark: The framework above is different from the clustering methods you may have seen before, e.g., such as \\(k\\)-means where each data point belongs to exactly one cluster. LDA belongs to the class of soft (or fuzzy) clustering methods, which allow for a data point (in this case a document) to belong to more than one cluster.\n\nLet’s dive briefly into the mathematics behind the LDA approach before focusing on exploring how to fit the model and interpret the results. We start by introducing the Dirichlet distribution:\nDefinition: The Dirichlet distribution is a family of continuous multivariate probability distributions on the d-dimensional unit simplex \\(\\{(x_1,\\ldots,x_d)\\in\\mathbb{R}_+^d: x_1+\\cdots+x_d=1\\}\\) with \\(d&gt;1\\). A Dirichlet distribution is parametrized by a \\(d\\)-dimensional vector of positive reals.\nNow suppose we have a corpus with \\(N\\) documents, with a total of \\(M\\) different words, and we specify that there are different \\(K\\) topics. The LDA approach then describes two Dirichlet distributions.\n\nDistribution of words within a topic: The term frequencies with which the \\(M\\) distinct words are expected to appear within a document from topic \\(k\\in\\{1,\\ldots,K\\}\\) is described by a M-dimensional Dirichlet distribution, where each topic has its own set of parameters.\nDistribution of topics within a document: The proportions \\((\\psi_{i,1},\\ldots,\\psi_{i,K})\\) with which the \\(K\\) topics feature in document \\(i\\in\\{1,\\ldots,N\\}\\) are modelled via a \\(K\\)-dimensional Dirichlet distribution. These proportions allow us to group the different documents.\n\nWe will stop considering the theoretical details of the LDA approach at this point, because the estimation of the parameter vectors is too complex to be covered in a Year 2 unit (we would require tools from Bayesian statistics). In the following, we study two examples to illustrate how to perform topic modelling in R.\n\nTo illustrate LDA we first consider an example where we know the ‘truth’. Here we will take two books by Charles Dickens and split each up into their individual chapters. We then want to see how well LDA performs at distinguishing between the two books.\nWe consider the books Great Expectations and A Tale of Two Cities we studied previously:\n\nDickens_raw &lt;- read.csv(\"data/dickens.csv\" )\nDickens &lt;- filter(\n    Dickens_raw,\n    title %in% c(\"Great Expectations\", \"A Tale of Two Cities\")\n)\n\n\nWe want to split the books into their chapters, with each chapter being handled as a separate document. As in Section 3.2, we extract the chapter the individual lines in the text document belong to:\n\nDickens_chapters &lt;- Dickens %&gt;%\n    group_by(title) %&gt;%\n    mutate(chapter = cumsum(\n        str_detect(text, regex(\"^chapter \", ignore_case = TRUE))\n    ))\n\nThe next step is to remove any text before the first chapter, and we further us the unite() function from the tidyr R package to create an identifier document from the book title and chapter number:\n\nlibrary(tidyr)\nDickens_chapters &lt;- Dickens_chapters %&gt;%\n    filter(chapter &gt; 0) %&gt;%\n    unite(col = document, title, chapter)\n\nFinally, we split the documents into words, remove any stop words, and count the frequency for each word within each document, as we did when analyzing word frequency:\n\nDickens_chapters &lt;- Dickens_chapters %&gt;%\n    unnest_tokens(word, text) %&gt;%\n    mutate(word = gsub(\"_\", \"\", word)) %&gt;%\n    anti_join(stop_words, by = \"word\") %&gt;%\n    count(document, word, sort = TRUE)\n\nRemark: Stop words are removed because they usually perform poorly at explaining differences in the underlying topics. Further, their high prevalence in all topics leads to topics appearing more similar than they are.\n\nThe R package we are using for topic modelling requires the data to be stored as a document term matrix which takes the following form:\n\nEach row represents one document (a book chapter in our case).\nEach column corresponds to one term / word.\nEach entry contains the number of appearances of a term in a document.\n\nThe tidytext package provides the cast_dtm() function to create such a matrix, but we need to install the tm R package to do this:\n\nDickens_dtm &lt;- Dickens_chapters %&gt;% cast_dtm(document, word, n)\n\n\nWith everything in place, we now use the LDA() function in the topicmodels R package. One key decision we have to make is specifying the number \\(K\\) of topics. For the purpose of this analysis, it seems logical to set the number of topics to \\(K=2\\):\n\nlibrary(topicmodels)\nDickens_LDA &lt;- LDA(Dickens_dtm, k = 2, method = \"Gibbs\", control = list(seed = 123))\n\nRemark: The estimates are sensitive to the value we specify in control=list(seed=…). In this course you may ignore the sensitivity and focus on analyzing the results obtained.\n\nHaving fitted the model, we want to explore the estimated model parameters. Recall that we have\n\n\\(K\\) parameters per document, which represent how much the \\(K\\) topics feature in the document\n\\(M\\) parameters per topic, which give insight on how frequent each word features within a topic\n\nTo extract the \\(K\\) proportions for each of the documents, we use the following R code:\n\nDickens_topics &lt;- tidy(Dickens_LDA, matrix = \"gamma\")\nDickens_topics %&gt;% slice_head(n = 4)\n\n# A tibble: 4 x 3\n  document              topic gamma\n  &lt;chr&gt;                 &lt;int&gt; &lt;dbl&gt;\n1 Great Expectations_57     1 0.310\n2 Great Expectations_7      1 0.190\n3 Great Expectations_38     1 0.370\n4 Great Expectations_17     1 0.237\n\n\nThe values in the column gamma correspond to the estimated proportions. We see that Chapter 7, 17, 38 and 57 from Great Expectations feature more of Topic 2 than Topic 1.\nGiven the high number of chapters, it’s better to visualize the estimates in order to discuss agreement between the estimated topics and the underlying truth. One option is to create box plots:\n\nDickens_topics %&gt;%\n    separate(document, c(\"title\", \"chapter\"), sep = \"_\", convert = TRUE) %&gt;%\n    ggplot(aes(x = factor(topic), y = gamma)) +\n    facet_wrap(~title) +\n    geom_boxplot() +\n    labs(x = \"Topic\", y = \"Proportion\")\n\n\n\nIllustration of the estimated proportions for each chapter within each book.\n\n\n\nWe see a clear difference in the plots for the two books. Most of the chapters from A Tale of Two Cities mostly consist of words that belong to Topic 1, while for Great Expectations the chapters are composed of words from Topic 2. As such, the topics could be used to classify the individual chapters.\nSuppose we would label a chapter as “A Tale of Two Cities” if the proportion of text estimated to be from Topic 1 exceeds 0.5. Such a classification would work well, with only one chapter per book being misclassified. This result may be due to some chapters rarely mentioning important characters and given that both books were written by Charles Dickens, correctly labeling these chapters is more challenging.\nLet’s look at the words most common in the different topics. We extract the top five words for each topic:\n\ntidy(Dickens_LDA, matrix = \"beta\") %&gt;%\n    group_by(topic) %&gt;%\n    slice_max(beta, n = 3)\n\n# A tibble: 6 x 3\n# Groups:   topic [2]\n  topic term     beta\n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1 lorry 0.00690\n2     1 hand  0.00634\n3     1 time  0.00600\n4     2 joe   0.0146 \n5     2 miss  0.00951\n6     2 don   0.00812\n\n\nIt seems that character names plan an important role when deciding whether a text belongs to Topic 1 or Topic 2, which is kind of what we expect. Let’s produce a plot to compare the term frequencies for the different words in more detail:\n\ntidy(Dickens_LDA, matrix = \"beta\") %&gt;%\n    mutate(topic = case_when(topic == 1 ~ \"Topic1\", topic == 2 ~ \"Topic2\")) %&gt;%\n    pivot_wider(names_from = topic, values_from = beta, values_fill = 0) %&gt;%\n    ggplot(aes(x = Topic1, y = Topic2)) +\n    geom_point() +\n    geom_text(aes(label = term), check_overlap = TRUE, vjust = 1) +\n    coord_trans(x = \"sqrt\", y = \"sqrt\") +\n    theme_bw() +\n    labs(x = \"Term Frequency in Topic 1\", y = \"Term Frequency in Topic 2\")\n\n\n\nComparison of term frequencies for the two topics.\n\n\n\nThe plot shows some very interesting patterns. In particular, the two topics seem to mainly differ in terms of the term frequency of a subset of words (the ones located close to the axes), while the term frequencies are very similar for the majority of words.\n\nLet’s look at an example where there is no ‘truth’ as such. The file “NYT.csv” provides metadata for 1289 articles published by the New York Times in their “Europe” subsection between 01/01/2023 and 01/11/2024:\n\nNYT &lt;- read.csv(\"data/nyt.csv\" )\n\nSpecifically, the data file gives for each article the lead paragraph and the date it was published. We want to explore the results we obtain when applying LDA.\n\nSince we take each article to be a separate document, we first assign a unique identifier to each article:\n\nNYT &lt;- NYT %&gt;% mutate(ID = 1:nrow(NYT))\n\nNow we follow the same procedure as in the previous analysis. As such, we start by splitting the sentences into individual words, remove any stop words and count the number of occurrences for each article individually:\n\nNYT_count &lt;- NYT %&gt;%\n    unnest_tokens(word, lead_paragraph) %&gt;%\n    anti_join(stop_words, by = \"word\") %&gt;%\n    count(ID, word, sort = TRUE)\n\nThe next step is to convert the counts into a document term matrix:\n\nNYT_dtm &lt;- NYT_count %&gt;% cast_dtm(ID, word, n)\n\nFinally, we are ready to run the LDA() function and we choose to set \\(K=2\\):\n\nNYT_LDA &lt;- LDA(NYT_dtm, k = 2, method = \"Gibbs\", control = list(seed = 2024))\n\n\nLet’s start by investigating the make-up of the articles. This requires us to explore with which proportions the two topics feature in the different articles. One option is to again create a box plot:\n\ntidy(NYT_LDA, matrix = \"gamma\") %&gt;%\n    ggplot(aes(y = gamma)) +\n    theme_bw() +\n    geom_boxplot() +\n    labs(y = \"Proportion\")\n\n\n\nBoxplot of the estimated proportions for the articles.\n\n\n\nThe result seems a bit unsatisfying as the make-up only varies between a 65-35 and 35-65 split. Consequently, the estimates suggests that there are very little differences in the language used within the topics.\nTo explore the topics in more detail, we plot the estimated term frequencies for the two topics against each other:\n\ntidy(NYT_LDA, matrix = \"beta\") %&gt;%\n    mutate(topic = case_when(topic == 1 ~ \"Topic1\", topic == 2 ~ \"Topic2\")) %&gt;%\n    pivot_wider(names_from = topic, values_from = beta, values_fill = 0) %&gt;%\n    ggplot(aes(x = Topic1, y = Topic2)) +\n    coord_trans(x = \"sqrt\", y = \"sqrt\") +\n    geom_point() +\n    theme_bw() +\n    geom_text(aes(label = term), check_overlap = TRUE, vjust = 1) +\n    labs(x = \"Term Frequency in Topic 1\", y = \"Term Frequency in Topic 2\")\n\n\n\nComparison of term frequencies for the two topics.\n\n\n\nThis plot provides some more insight. We see that Topic 1 seems to feature more of the words related to the War in Ukraine, but it’s not a perfect plot - we note that for instance “missile” features more in Topic 2 although this topic mainly occurred in the context of the war.\nTo summarize, we find that LDA produces two topics which seem quite sensible, when considering the term frequencies of the words. However, it’s also important to point out that LDA does not really split the documents into separate groups, due to each document only containing a few words. This is quite a common feature, as the flexibility of the LDA approach comes at the cost of it requiring a lot of data for model fitting.",
    "crumbs": [
      "Lecture Notes",
      "Text Data Analysis"
    ]
  },
  {
    "objectID": "lecture_notes/03-Text-Data-Analysis.html#summary",
    "href": "lecture_notes/03-Text-Data-Analysis.html#summary",
    "title": "Text Data Analysis",
    "section": "",
    "text": "We considered several aspects related to the analysis of text data in R:\n\nThe tidy text format is useful for the analysis of text data\nWord frequency can be visualized using bar plots or word clouds\nSentiment analysis can be used to study the emotional intent. We considered one approach based on sentiment lexicons, but also highlighted that we have to make a strong assumption\nDocuments can be compared by plotting term frequencies against each other and analyzing the term frequency - inverse document frequency\nLatent Dirichlet allocation is one method that may be used for topic modelling, but results need to be studied carefully\n\nSome other aspects one should be aware of:\n\nWe exclusively based our analyses on the individual words. However, in some applications it may be better to consider n-grams, which are compositions of \\(n\\) (usually consecutive) words. The unnest_tokens() provides options to extract n-grams from a text.\nThe principles introduced in this chapter can be applied to any type of text data, such as social media messages or song lyrics, and are not limited to short texts and books.\nYou should be mindful of copyright when scraping text data from the internet. That’s why we considered older books rather than recent bestsellers.",
    "crumbs": [
      "Lecture Notes",
      "Text Data Analysis"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA22019: Introduction to Data Science",
    "section": "",
    "text": "Welcome to the course website for MA22019.\n\n\nThis module introduces students to the statistical analysis of spatial data.\n\n\n\nWe will be using R and RStudio for all data analysis. Please see the Computing page for setup instructions.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "MA22019: Introduction to Data Science",
    "section": "",
    "text": "This module introduces students to the statistical analysis of spatial data.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#computing",
    "href": "index.html#computing",
    "title": "MA22019: Introduction to Data Science",
    "section": "",
    "text": "We will be using R and RStudio for all data analysis. Please see the Computing page for setup instructions.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "slides/week_10/semi_variogram_example.html",
    "href": "slides/week_10/semi_variogram_example.html",
    "title": "MA22019 2025 - Revision Class",
    "section": "",
    "text": "Loading packages\n\nlibrary( dplyr )\nlibrary( ggplot2 )\nlibrary( prettymapr )\nlibrary( ggspatial )\nlibrary( sp )\nlibrary( gstat )\n\nLoading the data\nThe file “Manchester.csv” contains some synthetic air pollution data for a day in Manchester:\n\nManchester &lt;- read.csv(\"manchester.csv\" )\n\nData visualization\nWe start by visualizing the data:\n\nggplot( Manchester, aes( x=Lon, y=Lat ) ) + \n  annotation_map_tile( zoom=11 ) + \n  geom_spatial_point( aes(color=Level), size=4 ) +\n  scale_color_distiller( palette=\"Reds\", trans=\"reverse\" ) +\n  labs( x=\"Longitude\", y=\"Latitude\", color=\"PM10 Levels\" )\n\n\n\n\n\n\n\nAnalysis of the spatial dependence\nWe estimate the semi-variogram using\n\ncoordinates( Manchester ) &lt;- ~Lon+Lat\ngamma_hat &lt;- variogram( Level~1, Manchester, cutoff=0.1 )\nggplot( gamma_hat, aes( x=dist, y=gamma/2 ) ) + geom_point( size=2 ) +\n  labs( x=\"Distance\", y=\"Semi-variogram\" )\n\n\n\n\n\n\n\n\nWhat do we conclude?\nIs it reasonable to assume that (i) the mean is constant and (ii) dependence is fully specified by spatial distance?"
  },
  {
    "objectID": "slides/week_2/analysis_nrfa_data_for_bathford.html",
    "href": "slides/week_2/analysis_nrfa_data_for_bathford.html",
    "title": "Analysis of NRFA Data",
    "section": "",
    "text": "library( lubridate )\nlibrary( dplyr )\n\n\nBathford &lt;- read.csv(\"bathford_river_flow.csv\", skip=20, header=FALSE,\n                      colClasses = c(\"character\",\"numeric\",\"NULL\") ) \n\nWarning in read.table(file = file, header = header, sep = sep, quote = quote, :\ncols = 2 != length(data) = 3"
  },
  {
    "objectID": "slides/week_2/analysis_nrfa_data_for_bathford.html#loading-the-data-and-r-packages",
    "href": "slides/week_2/analysis_nrfa_data_for_bathford.html#loading-the-data-and-r-packages",
    "title": "Analysis of NRFA Data",
    "section": "",
    "text": "library( lubridate )\nlibrary( dplyr )\n\n\nBathford &lt;- read.csv(\"bathford_river_flow.csv\", skip=20, header=FALSE,\n                      colClasses = c(\"character\",\"numeric\",\"NULL\") ) \n\nWarning in read.table(file = file, header = header, sep = sep, quote = quote, :\ncols = 2 != length(data) = 3"
  },
  {
    "objectID": "slides/week_2/analysis_nrfa_data_for_bathford.html#data-cleaning",
    "href": "slides/week_2/analysis_nrfa_data_for_bathford.html#data-cleaning",
    "title": "Analysis of NRFA Data",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nCheck which variables need to converted and renamed\n\nglimpse( Bathford )\n\nRows: 19,697\nColumns: 2\n$ V1 &lt;chr&gt; \"1969-10-27\", \"1969-10-28\", \"1969-10-29\", \"1969-10-30\", \"1969-10-31…\n$ V2 &lt;dbl&gt; 3.998, 3.958, 4.210, 4.480, 4.205, 3.830, 3.723, 3.986, 4.353, 4.52…\n\n\nChange variables names\n\nBathford &lt;- rename( Bathford, Date = V1, RiverFlow = V2 )\n\nConvert to correct type\n\nBathford$Date &lt;- as_date( Bathford$Date, format=\"%Y-%m-%d\" )"
  },
  {
    "objectID": "slides/week_2/analysis_nrfa_data_for_bathford.html#data-wrangling-and-visualization",
    "href": "slides/week_2/analysis_nrfa_data_for_bathford.html#data-wrangling-and-visualization",
    "title": "Analysis of NRFA Data",
    "section": "Data Wrangling and visualization",
    "text": "Data Wrangling and visualization\nProportion of missing data\n\nmean( is.na( Bathford$RiverFlow ) )\n\n[1] 0.006244606\n\n\nPlot river flow over time\n\nplot( Bathford$Date, Bathford$RiverFlow, type='l',\n      xlab=\"Date\", ylab=\"River Flow\", cex.lab = 1.5 )\n\n\n\n\n\n\n\nFilter observations based on date and observed river flow\n\nBathford_High &lt;- filter( Bathford, RiverFlow &gt; 100 )\nslice_head( Bathford_High, n=5 )\n\n        Date RiverFlow\n1 1970-11-19     104.3\n2 1971-01-21     128.4\n3 1971-01-22     114.8\n4 1971-01-23     115.2\n5 1971-01-24     133.1\n\n\n\nBathford_High &lt;- filter( Bathford, RiverFlow &gt; 100, year(Date) &gt; 1990 )\nslice_head( Bathford_High, n=5 )\n\n        Date RiverFlow\n1 1991-01-10     112.1\n2 1992-11-26     106.9\n3 1992-11-27     100.9\n4 1992-11-28     100.6\n5 1992-11-29     131.6\n\n\nLoading data for a second gauge at Bath\n\nBath &lt;- read.csv(\"bath_river_flow.csv\", skip=19, header=FALSE,\n                  colClasses = c(\"character\",\"numeric\",\"NULL\") )\nBath &lt;- Bath %&gt;% rename( Date = V1, RiverFlow = V2 ) %&gt;%\n  mutate( Date = as_date( Date, format=\"%Y-%m-%d\" ) )\nglimpse( Bath )\n\nRows: 17,196\nColumns: 2\n$ Date      &lt;date&gt; 1976-09-01, 1976-09-02, 1976-09-03, 1976-09-04, 1976-09-05,…\n$ RiverFlow &lt;dbl&gt; 3.39, 2.83, 2.97, 2.81, 2.90, 2.81, 2.59, 3.11, 2.78, 2.46, …\n\n\nWe note that the gauge at Bath started recording river flow a few years after the gauge at Bathford.\nThis prevents us from analyzing the dependence in the river flow of the two students, for instance, using the covariance (introduced in Year 1 Probability & Statistics):\n\ncov(Bath$RiverFlow, Bathford$RiverFlow)\n\nError in cov(Bath$RiverFlow, Bathford$RiverFlow): incompatible dimensions"
  },
  {
    "objectID": "slides/week_2/analysis_nrfa_data_for_bathford.html#merging-the-two-data-sets",
    "href": "slides/week_2/analysis_nrfa_data_for_bathford.html#merging-the-two-data-sets",
    "title": "Analysis of NRFA Data",
    "section": "Merging the two data sets",
    "text": "Merging the two data sets\nLet’s merge observations based on Date and rename the variables\n\nRF &lt;- full_join( Bathford, Bath, by=c(\"Date\" = \"Date\") ) %&gt;%\n  rename( Bathford = RiverFlow.x, Bath = RiverFlow.y )\nslice_head(RF, n=5)\n\n        Date Bathford Bath\n1 1969-10-27    3.998   NA\n2 1969-10-28    3.958   NA\n3 1969-10-29    4.210   NA\n4 1969-10-30    4.480   NA\n5 1969-10-31    4.205   NA\n\n\nFirst observations for Bath are missing, since the gauge was not operational at that time."
  },
  {
    "objectID": "slides/week_2/analysis_nrfa_data_for_bathford.html#data-analysis",
    "href": "slides/week_2/analysis_nrfa_data_for_bathford.html#data-analysis",
    "title": "Analysis of NRFA Data",
    "section": "Data analysis",
    "text": "Data analysis\nLet’s explore the dependence of the river flow levels for the two gauges:\n\nplot( RF$Bath, RF$Bathford, pch=19,\n      xlab=\"River Flow at Bath\", ylab=\"River Flow at Bathford\" )\n\n\n\n\n\n\n\nWe can now calculate the covariance and correlation\n\ncov( RF$Bathford, RF$Bath,  use = \"complete\" )\n\n[1] 582.4116\n\ncor( RF$Bathford, RF$Bath,  use = \"complete\" )\n\n[1] 0.9929988"
  },
  {
    "objectID": "slides/week_2/analysis_nrfa_data_for_bathford.html#comparison-with-inner_join",
    "href": "slides/week_2/analysis_nrfa_data_for_bathford.html#comparison-with-inner_join",
    "title": "Analysis of NRFA Data",
    "section": "Comparison with inner_join()",
    "text": "Comparison with inner_join()\nWhile full_join() keeps all observation, inner_join() only keeps the dates listed in both data sets\n\nRF &lt;- inner_join( Bathford, Bath, by=c(\"Date\" = \"Date\") )\nslice_head(RF, n=5)\n\n        Date RiverFlow.x RiverFlow.y\n1 1976-09-01       2.811        3.39\n2 1976-09-02       2.560        2.83\n3 1976-09-03       2.337        2.97\n4 1976-09-04       2.385        2.81\n5 1976-09-05       2.146        2.90"
  },
  {
    "objectID": "slides/week_2/analysis_australian_weather.html",
    "href": "slides/week_2/analysis_australian_weather.html",
    "title": "Analysis of Australian weather data",
    "section": "",
    "text": "library(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(patchwork)"
  },
  {
    "objectID": "slides/week_2/analysis_australian_weather.html#loading-and-cleaning-the-data",
    "href": "slides/week_2/analysis_australian_weather.html#loading-and-cleaning-the-data",
    "title": "Analysis of Australian weather data",
    "section": "Loading and cleaning the data",
    "text": "Loading and cleaning the data\n\nAUS &lt;- read.csv(\"weatheraustralia.csv\" )\nAUS$Date &lt;- as_date( AUS$Date, format=\"%d/%m/%Y\" )\nglimpse( AUS )\n\nRows: 15,781\nColumns: 6\n$ Date          &lt;date&gt; 2009-01-01, 2009-01-02, 2009-01-03, 2009-01-04, 2009-01…\n$ Location      &lt;chr&gt; \"Sydney\", \"Sydney\", \"Sydney\", \"Sydney\", \"Sydney\", \"Sydne…\n$ MinTemp       &lt;dbl&gt; 17.7, 18.5, 16.9, 18.7, 20.2, 19.9, 20.6, 20.2, 17.4, 16…\n$ MaxTemp       &lt;dbl&gt; 35.1, 23.0, 23.2, 27.1, 31.6, 29.7, 34.9, 21.7, 22.9, 24…\n$ Rainfall      &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0…\n$ WindGustSpeed &lt;int&gt; 72, 63, NA, 65, 63, 41, 59, 56, 50, 48, 54, 39, 59, 67, …\n\n\nWe can extract the list of locations using\n\nAUS %&gt;% distinct( Location )\n\n   Location\n1    Sydney\n2 Melbourne\n3  Adelaide\n4     Perth\n5    Darwin"
  },
  {
    "objectID": "slides/week_2/analysis_australian_weather.html#analysis-of-minimum-vs-maximum-temperature",
    "href": "slides/week_2/analysis_australian_weather.html#analysis-of-minimum-vs-maximum-temperature",
    "title": "Analysis of Australian weather data",
    "section": "Analysis of Minimum vs Maximum temperature",
    "text": "Analysis of Minimum vs Maximum temperature\nSetting up the canvas\n\nPlotAUS &lt;- ggplot( AUS, aes( x=MinTemp, y=MaxTemp ) )\nPlotAUS\n\n\n\n\n\n\n\nScatter plot\n\nPlotAUS + geom_point()\n\nWarning: Removed 489 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nScatter plot - Adding visual cues\n\nPlotAUS + geom_point( aes( shape=Location, color=Location ) )\n\n\n\n\n\n\n\nScatter plot - Facets\n\nPlotAUS + geom_point() +\n  facet_wrap( ~Location ) +\n  labs( x=\"Minimum daily temperature\", y=\"Maximum daily temperature\" )\n\n\n\n\n\n\n\nLine plot - Maximum daily temperature over time\nLet’s create a line plot for the city of Darwin:\n\nDarwin &lt;- AUS %&gt;% filter( Location == \"Darwin\" )\nggplot( Darwin, aes(x=Date, y=MaxTemp ) ) + geom_line() + \n  labs( x=\"Date\", y=\"Maximum daily temperature\", title = \"Darwin\" )\n\n\n\n\n\n\n\nWe can again use facets to create a subplot for each city:\n\nggplot( AUS, aes( x=Date, y=MaxTemp ) ) + facet_wrap( ~Location ) + \n  geom_line( aes( color=Location, linetype=Location ) ) + \n  labs( x=\"Date\", y=\"Maximum daily temperature\" )"
  },
  {
    "objectID": "slides/week_2/analysis_australian_weather.html#analysis-of-wind-gust-speed",
    "href": "slides/week_2/analysis_australian_weather.html#analysis-of-wind-gust-speed",
    "title": "Analysis of Australian weather data",
    "section": "Analysis of wind gust speed",
    "text": "Analysis of wind gust speed\nHistogram for Darwin\nWe use geom_histogram() to create a single histogram\n\nggplot( Darwin, aes( x=WindGustSpeed ) ) + geom_histogram( bins=20 ) + \n  labs( x=\"Speed of wind gust in km/h\", y=\"Count\", title = \"Darwin\" )\n\n\n\n\n\n\n\nDensity plot\nLet’s compare histogram and density plot\n\nggplot( Darwin, aes( x=WindGustSpeed ) ) +\n  geom_histogram( aes(y=..density..), alpha=0.5, bins=20 ) +\n  geom_density( size=1.2, color=\"red\" ) +\n  labs( x=\"Speed of wind gust in km/h\", y=\"Density\", title = \"Darwin\" )\n\n\n\n\n\n\n\n\nWeatherAD &lt;- filter( AUS, Location %in% c(\"Adelaide\",\"Darwin\") )\nggplot( WeatherAD, aes( x=WindGustSpeed ) ) +\n  geom_density( aes( linetype=Location, color=Location ), size=1.2 ) +\n  labs( x=\"Speed of wind gust in km/h\", y=\"Density\" )\n\n\n\n\n\n\n\nBox plots\n\nggplot( AUS, aes( x=Location, y=WindGustSpeed ) ) + \n  geom_boxplot( aes( fill=Location ) ) + \n  labs( y=\"Speed of wind gust in km/h\" )\n\n\n\n\n\n\n\nViolin plots\n\nggplot( AUS, aes( x=reorder(Location, WindGustSpeed, median, na.rm=TRUE), \n                  y=WindGustSpeed ) ) + geom_violin() + \n  labs( x=\"Location\", y=\"Wind gust speed in mph\", \n        title=\"Wind gust speed across five Australian cities\" )"
  },
  {
    "objectID": "slides/week_5/comparison_of_jane_eyre_and_wuthering_heights.html",
    "href": "slides/week_5/comparison_of_jane_eyre_and_wuthering_heights.html",
    "title": "Comparison of Jane Eyre and Wuthering Heights",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(tidytext)\n\nLoading and cleaning the text data\n\nJaneEyre_raw &lt;- readLines( \"jane_eyre.txt\", encoding = \"UTF-8\" )\nJaneEyre_raw &lt;- data.frame( text = JaneEyre_raw )\n\nJaneEyre &lt;- JaneEyre_raw %&gt;% \n  unnest_tokens(word, text) %&gt;%\n  mutate( word = gsub( \"_\", \"\", word ) ) %&gt;%\n  count( word, sort=TRUE ) %&gt;%\n  mutate( 'term frequency' = n / sum(n) ) %&gt;%\n  anti_join( stop_words )\n\nJoining with `by = join_by(word)`\n\n\n\nWutheringHeights_raw &lt;- readLines( \"wuthering_heights.txt\", encoding = \"UTF-8\" )\nWutheringHeights_raw &lt;- data.frame( text = WutheringHeights_raw )\n\nWutheringHeights &lt;- WutheringHeights_raw %&gt;% \n  unnest_tokens(word, text) %&gt;%\n  mutate( word = gsub( \"_\", \"\", word ) ) %&gt;%\n  count( word, sort=TRUE ) %&gt;%\n  mutate( 'term frequency' = n / sum(n) ) %&gt;%\n  anti_join( stop_words )\n\nJoining with `by = join_by(word)`\n\n\nCreate a single data frame with all the words and term frequencies\n\ntf &lt;- full_join( JaneEyre, WutheringHeights, by=\"word\" ) %&gt;%\n  rename( JaneEyre='term frequency.x', Heights='term frequency.y' )\n\n\nglimpse(tf)\n\nRows: 15,125\nColumns: 5\n$ word     &lt;chr&gt; \"jane\", \"rochester\", \"sir\", \"miss\", \"time\", \"day\", \"looked\", …\n$ n.x      &lt;int&gt; 341, 317, 316, 310, 244, 232, 221, 218, 187, 184, 182, 182, 1…\n$ JaneEyre &lt;dbl&gt; 0.0018099884, 0.0016825992, 0.0016772913, 0.0016454440, 0.001…\n$ n.y      &lt;int&gt; NA, NA, 43, 129, 128, 105, 75, 104, 117, 2, 133, 94, 142, 79,…\n$ Heights  &lt;dbl&gt; NA, NA, 3.672640e-04, 1.101792e-03, 1.093251e-03, 8.968074e-0…\n\n\nReplace NAs by 0\n\ntf &lt;- tf %&gt;%\n  mutate( JaneEyre = case_when( is.na(JaneEyre) == TRUE ~ 0, .default = JaneEyre),\n          Heights = case_when( is.na(Heights) == TRUE ~ 0, .default = Heights) )\n\nScatter plot to compare term frequencies\n\nggplot( tf, aes( x=JaneEyre, y=Heights ) ) +\n  geom_point() + \n  geom_text( aes(label=word), check_overlap = TRUE, vjust=1.5 ) + \n  coord_trans( x=\"sqrt\", y=\"sqrt\" ) + theme_bw() +\n  labs( x=\"Term Frequency in Jane Eyre\",\n        y=\"Term Frequency in Wuthering Heights\" )\n\nWarning: `coord_trans()` was deprecated in ggplot2 4.0.0.\nℹ Please use `coord_transform()` instead."
  },
  {
    "objectID": "slides/week_5/analysis_of_jane_eyre.html",
    "href": "slides/week_5/analysis_of_jane_eyre.html",
    "title": "Analysis of Jane Eyre",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(tidytext)\nlibrary(wordcloud)\nlibrary(stringr)"
  },
  {
    "objectID": "slides/week_5/analysis_of_jane_eyre.html#word-frequency-analysis",
    "href": "slides/week_5/analysis_of_jane_eyre.html#word-frequency-analysis",
    "title": "Analysis of Jane Eyre",
    "section": "Word frequency analysis",
    "text": "Word frequency analysis\nCalculating term frequency\nWe now count how often each word appears in the book:\n\nJaneEyre_Count &lt;- JaneEyre %&gt;% count( word, sort=TRUE )\n\n\nslice_head(JaneEyre_Count, n=10 )\n\n   word    n\n1   the 7856\n2     i 7169\n3   and 6632\n4    to 5238\n5     a 4470\n6    of 4368\n7   you 2970\n8    in 2769\n9   was 2526\n10   it 2406\n\n\nOne important measure is term frequency:\n\nJaneEyre_Count &lt;- JaneEyre_Count %&gt;%\n  mutate( 'term frequency'=n/sum(n), rank=row_number() ) %&gt;%\n  rename( Count = n )\n\n\nslice_head(JaneEyre_Count, n=10)\n\n   word Count term frequency rank\n1   the  7856     0.04169874    1\n2     i  7169     0.03805222    2\n3   and  6632     0.03520189    3\n4    to  5238     0.02780270    4\n5     a  4470     0.02372624    5\n6    of  4368     0.02318484    6\n7   you  2970     0.01576441    7\n8    in  2769     0.01469753    8\n9   was  2526     0.01340771    9\n10   it  2406     0.01277077   10\n\n\nDoes Zipf’s Law hold?\nPlot rank against term frequency (both on logarithmic scale):\n\nggplot( JaneEyre_Count, aes( x=rank, y=`term frequency` ) ) + \n  geom_line( linewidth=1.5 ) + \n  coord_trans( x=\"log10\", y=\"log10\" )\n\nWarning: `coord_trans()` was deprecated in ggplot2 4.0.0.\nℹ Please use `coord_transform()` instead.\n\n\n\n\n\n\n\n\nWe see a linear relationship in the plot, which supports Zipf’s Law that postulates an inverse proportionality between rank and term frequency.\nRemoving stop words\nWe use the data set stop_words in the tidytext package:\n\ndata(\"stop_words\")\nJaneEyre_Count &lt;- JaneEyre_Count %&gt;% \n  anti_join(stop_words, by=\"word\")\n\n\nslice_head(JaneEyre_Count, n=10)\n\n        word Count term frequency rank\n1       jane   341   0.0018099884   69\n2  rochester   317   0.0016825992   71\n3        sir   316   0.0016772913   72\n4       miss   310   0.0016454440   73\n5       time   244   0.0012951236   98\n6        day   232   0.0012314290  102\n7     looked   221   0.0011730423  106\n8      night   218   0.0011571187  109\n9       eyes   187   0.0009925743  120\n10      john   184   0.0009766506  125\n\n\nVisualization\nOption 1 Bar plot: we order the words based on frequency\n\nJaneEyre_Count %&gt;%\n  slice_max( Count, n=10 ) %&gt;% \n  mutate( word = reorder(word,Count) ) %&gt;%\n  ggplot( aes( x=Count, y=word ) ) + \n    geom_col() + labs( x=\"Count\", y=\"\" )\n\n\n\n\n\n\n\nOption 2 Word cloud:\n\nJaneEyre_Count %&gt;%\n  with( wordcloud( word, Count, max.words=40, colors=topo.colors(n=40) ) )"
  },
  {
    "objectID": "slides/week_5/analysis_of_jane_eyre.html#sentiment-analysis-for-jane-eyre",
    "href": "slides/week_5/analysis_of_jane_eyre.html#sentiment-analysis-for-jane-eyre",
    "title": "Analysis of Jane Eyre",
    "section": "\nSentiment Analysis for Jane Eyre\n",
    "text": "Sentiment Analysis for Jane Eyre\n\nStart with the raw data and split it into words\n\nJaneEyre &lt;- JaneEyre_raw %&gt;%\n  mutate( line = row_number() ) %&gt;%\n  unnest_tokens( word, text ) %&gt;%\n  mutate( word = gsub( \"_\", \"\", word ) )\n\nLoad the sentiment lexicon\n\nAFINN &lt;- read.csv(\"afinn_sentiment_lexicon.csv\" )\n\n\nslice_head(AFINN, n=5)\n\n       word value\n1   abandon    -2\n2 abandoned    -2\n3  abandons    -2\n4  abducted    -2\n5 abduction    -2\n\n\nOnly keep the words both in the book and the sentiment lexicon\n\nJaneEyre_AFINN &lt;- inner_join( JaneEyre, AFINN, by=\"word\" )\n\n\nslice_head(JaneEyre_AFINN, n=5)\n\n  line    word value\n1   33 demands    -1\n2   36  thanks     2\n3   41    fair     2\n4   41  honest     2\n5   48   vague    -2\n\n\nSentiment across lines\n\nJaneEyre_AFINN %&gt;% \n  filter( word != \"miss\" ) %&gt;%\n  mutate( sentiment = cumsum( value ) ) %&gt;%\n  ggplot( aes( x=line, y=sentiment ) ) +\n  geom_line( linewidth=1.2 ) + \n  labs( x=\"Line number\", y=\"Sentiment\" )\n\n\n\n\n\n\n\nSentiment for the individual chapters\nDetect which chapter each line belongs to:\n\nJaneEyre_chapters &lt;- JaneEyre_raw %&gt;%\n  mutate( chapter = cumsum( str_detect(\n    text, regex(\"^chapter \", ignore_case = TRUE)\n  ) ) )\n\nRemove any text before Chapter 1 and split the text into words:\n\nJaneEyre_chapters &lt;- JaneEyre_chapters %&gt;%\n  filter( chapter &gt; 0 ) %&gt;%\n  unnest_tokens( word, text ) %&gt;%\n  mutate( word = gsub( \"_\", \"\", word ) )\n\n\nslice_head(JaneEyre_chapters, n=5)\n\n  chapter    word\n1       1 chapter\n2       1       i\n3       1   there\n4       1     was\n5       1      no\n\n\nWe can now calculate and visualize the aggregated sentiment for each chapter\n\nJaneEyre_chapters %&gt;%\n  inner_join( AFINN, by=\"word\" ) %&gt;%\n  group_by( chapter ) %&gt;%\n  summarise( sentiment = mean(value) ) %&gt;%\n  ggplot( aes( x=chapter, y=sentiment ) ) + \n  geom_col() + labs( x=\"Chapter\", y=\"AFINN sentiment score\" )"
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html",
    "href": "slides/week_3/data_visualization_(part_2).html",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "",
    "text": "We have introduced six types of plot:\n\nScatter plot\nLine plot\nHistogram / Density plot\nBox / Violin plot\n\nWe create such plots in ggplot2 using the syntax\n\nggplot( data, aes(...) ) + \n  geom_...( aes(...) ) + \n  labs( x=..., y=... )"
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html#reminder-i",
    "href": "slides/week_3/data_visualization_(part_2).html#reminder-i",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "",
    "text": "We have introduced six types of plot:\n\nScatter plot\nLine plot\nHistogram / Density plot\nBox / Violin plot\n\nWe create such plots in ggplot2 using the syntax\n\nggplot( data, aes(...) ) + \n  geom_...( aes(...) ) + \n  labs( x=..., y=... )"
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html#reminder-ii",
    "href": "slides/week_3/data_visualization_(part_2).html#reminder-ii",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "Reminder II",
    "text": "Reminder II\n\n\nType of plot\ngeom_…\n\n\n\nScatter plot\ngeom_point\n\n\nLine plot\n\ngeom_line, geom_step\n\n\n\nBar plot\n\ngeom_bar, geom_col\n\n\n\nHistogram\ngeom_histogram\n\n\nBox plot\ngeom_boxplot\n\n\nDensity plot\ngeom_density\n\n\nViolin plot\ngeom_violin"
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html#plan-for-today",
    "href": "slides/week_3/data_visualization_(part_2).html#plan-for-today",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "Plan for Today",
    "text": "Plan for Today\nToday we will explore the remaining plot elements:\n\nCoordinate system (Sections 2.1.3 and 2.3.1)\nScale and context (Sections 2.1.5 and 2.3.2)\nLabelling and colours (Sections 2.3.3 and 2.3.4)\nChanging the format of a data frame (Section 2.4)\n\nThis will complete Chapter 2 on data visualization.\nAt the end, I will make some announcements regarding Coursework 1."
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html#overview",
    "href": "slides/week_3/data_visualization_(part_2).html#overview",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "Overview",
    "text": "Overview\nWe have to decide which coordinate system is the most suitable for our analysis.\nThe three types we consider in this course are\n\nCartesian coordinate system\nPolar coordinate system\nGeographical / spatial coordinate system\n\nFor now, we only focus on the fist two types.\nThe key function to switch between types is coord_polar()."
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html#example---wind-direction-at-bela-vista",
    "href": "slides/week_3/data_visualization_(part_2).html#example---wind-direction-at-bela-vista",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "Example - Wind direction at Bela Vista",
    "text": "Example - Wind direction at Bela Vista\nWe are working with:\n\nHourly data from Bela Vista, Brazil, for 2017 and 2018\nObservations on wind direction and speed\n\nLet’s explore the distribution of wind direction and its relation to wind speed.\n\nWhich types of plot should we be using?"
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html#pie-charts",
    "href": "slides/week_3/data_visualization_(part_2).html#pie-charts",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "Pie charts",
    "text": "Pie charts\nIn ggplot2, we have to create a bar plot with polar coordinates\n\ndf &lt;- data.frame( \"prob\" = c(0.3,0.4,0.3), \"group\" = c(\"A\",\"B\",\"C\") )\nggplot( df, aes( x=\"\", y=prob, fill=group ) ) + \n  geom_col() + coord_polar( theta=\"y\" ) + labs( x=\"\", y=\"\" )"
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html#summary",
    "href": "slides/week_3/data_visualization_(part_2).html#summary",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "Summary",
    "text": "Summary\nThere are two scenarios when a polar coordinate system may be useful to consider:\n\nPie charts  However, a bar plot may be better as it uses position and length as visual cues.\nTo visualize variables such as direction or time\n\nBoth these aspects can be achieved using coord_polar() in ggplot2.\nNote from ggplot2 developers: “Polar coordinates should be used with EXTREME caution because of the potential perceptual problems.”"
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html#types-of-scale",
    "href": "slides/week_3/data_visualization_(part_2).html#types-of-scale",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "Types of scale",
    "text": "Types of scale\nWe deal with three types of scale:\n\nNumerical: Speed, Age, etc.\nCategorical:  Ratings, Sites, etc.\nTime:  Months, Years, etc.\n\nWhen we create plots, knowing the type is important when deciding on the type of plot."
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html#changing-numerical-scales",
    "href": "slides/week_3/data_visualization_(part_2).html#changing-numerical-scales",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "Changing numerical scales",
    "text": "Changing numerical scales\n\nPlots may sometimes be dominated by a couple of data points or outliers\nChanging the scale may provide better insight\n\nThe function coord_trans() is one option to change it:\n\nggplot(..) + coord_trans( x=“log10” ) logarithmic scale\nggplot(..) + coord_trans( x=“sqrt” ) square root scale"
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html#examples",
    "href": "slides/week_3/data_visualization_(part_2).html#examples",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "Examples",
    "text": "Examples\nLet’s look at two examples:\n\nBody vs brain weight of mammals.\nNumber of followers and likes on Facebook\n\nWe will also demonstrate how to change font sizes of labels."
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html#overview-1",
    "href": "slides/week_3/data_visualization_(part_2).html#overview-1",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "Overview",
    "text": "Overview\nThe ggplot2 package has a default colour scheme. Here are some of the alternatives:"
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html#how-to-use-these-colour-schemes",
    "href": "slides/week_3/data_visualization_(part_2).html#how-to-use-these-colour-schemes",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "How to use these colour schemes",
    "text": "How to use these colour schemes\n\nAUS &lt;- read.csv(\"weatheraustralia.csv\" )\nggplot( AUS, aes( x=Location,y=WindGustSpeed ) ) +\n  geom_boxplot( aes(fill=Location) ) + \n  labs( y=\"Speed of wind gust in km/h\", fill=\"City\" ) + \n  scale_fill_brewer( palette=\"Oranges\" )"
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html#switching-data-formats",
    "href": "slides/week_3/data_visualization_(part_2).html#switching-data-formats",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "Switching data formats",
    "text": "Switching data formats\n\nggplot2 is based on one variable being plotted against another\nIn some cases, we have to alter the data structure to create the plot we want\n\nThe tidyr package provides two important functions\n\npivot_wider()\npivot_longer()"
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html#switching-data-formats---example",
    "href": "slides/week_3/data_visualization_(part_2).html#switching-data-formats---example",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "Switching data formats - Example",
    "text": "Switching data formats - Example\nWe have monthly average temperature data for Manaus, Brazil\n\nEach column refers to a month\nEach row refers to a year\n\nFor which plots is this format useful?\nFor which plots do we need to change the data format?"
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html#summary-1",
    "href": "slides/week_3/data_visualization_(part_2).html#summary-1",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "Summary",
    "text": "Summary\nToday we\n\nStudied the plot elements coordinate system, scale and context\nLearned how to change the coordinate system and scale\nSet axis labels, font size and colour scheme\nChanged the data structure using pivot_wider() and pivot_longer\nRemember: Plots should also help others to make meaningful comparisons"
  },
  {
    "objectID": "slides/week_3/data_visualization_(part_2).html#coursework-1",
    "href": "slides/week_3/data_visualization_(part_2).html#coursework-1",
    "title": "MA22019 - Data Visualization (Part 2)",
    "section": "Coursework 1",
    "text": "Coursework 1\n\nReleased at 15:00 on Friday 21 February on Moodle\nDue at 17:00 on Friday 28 February\nSubmission via Moodle only\n\nSupport:\n\nWork on it in the tutorial and in your own time\nOffice Hour Monday 9:00-11:00\nQ&A session 10:15-11:05 on Wednesday 26 February\nQuestions should be posted on the Padlet board"
  },
  {
    "objectID": "slides/week_3/change_in_data_structure___example.html",
    "href": "slides/week_3/change_in_data_structure___example.html",
    "title": "Switching Data Formats Example",
    "section": "",
    "text": "library(lubridate)\nlibrary(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "slides/week_3/change_in_data_structure___example.html#load-the-data",
    "href": "slides/week_3/change_in_data_structure___example.html#load-the-data",
    "title": "Switching Data Formats Example",
    "section": "Load the data",
    "text": "Load the data\n\nManaus &lt;- read.csv(\"manaus_temperature.csv\" )"
  },
  {
    "objectID": "slides/week_3/change_in_data_structure___example.html#case-1---comparison-months",
    "href": "slides/week_3/change_in_data_structure___example.html#case-1---comparison-months",
    "title": "Switching Data Formats Example",
    "section": "Case 1 - Comparison months",
    "text": "Case 1 - Comparison months\nData Cleaning\n\nManaus &lt;- Manaus %&gt;% \n  mutate( JAN = na_if( JAN, 999.9 ), FEB = na_if( FEB, 999.9 ) )\n\nPlotting the data\n\nManaus %&gt;% \n  filter( JAN &lt; 100, FEB &lt; 100 ) %&gt;%\n  ggplot( aes(x=JAN, y=FEB) ) + geom_point() + \n  labs(x=\"Temperature in January\", y=\"Temperature in February\") + \n  theme_bw() +\n  theme( axis.title=element_text(size=16), \n         axis.text=element_text(size=14) )"
  },
  {
    "objectID": "slides/week_3/change_in_data_structure___example.html#case-2---analyse-change-over-time",
    "href": "slides/week_3/change_in_data_structure___example.html#case-2---analyse-change-over-time",
    "title": "Switching Data Formats Example",
    "section": "Case 2 - Analyse change over time",
    "text": "Case 2 - Analyse change over time\nChanging the data structure\nThe data format is not good for creating a line plot of temperature over time. Sow e have to change the format:\n\nlibrary(tidyr)\nManaus_long &lt;- Manaus %&gt;%\n  pivot_longer(cols = JAN:DEC, names_to = \"Month\" )\n\n\nglimpse(Manaus_long)\n\nRows: 1,320\nColumns: 8\n$ YEAR   &lt;int&gt; 1910, 1910, 1910, 1910, 1910, 1910, 1910, 1910, 1910, 1910, 191…\n$ D.J.F  &lt;dbl&gt; 27.33, 27.33, 27.33, 27.33, 27.33, 27.33, 27.33, 27.33, 27.33, …\n$ M.A.M  &lt;dbl&gt; 26.62, 26.62, 26.62, 26.62, 26.62, 26.62, 26.62, 26.62, 26.62, …\n$ J.J.A  &lt;dbl&gt; 27.72, 27.72, 27.72, 27.72, 27.72, 27.72, 27.72, 27.72, 27.72, …\n$ S.O.N  &lt;dbl&gt; 28.52, 28.52, 28.52, 28.52, 28.52, 28.52, 28.52, 28.52, 28.52, …\n$ metANN &lt;dbl&gt; 27.55, 27.55, 27.55, 27.55, 27.55, 27.55, 27.55, 27.55, 27.55, …\n$ Month  &lt;chr&gt; \"JAN\", \"FEB\", \"MAR\", \"APR\", \"MAY\", \"JUN\", \"JUL\", \"AUG\", \"SEP\", …\n$ value  &lt;dbl&gt; 27.29, 26.99, 26.49, 26.19, 27.19, 27.49, 27.69, 27.99, 28.99, …\n\n\nTo reverse the process and convert the longer to the wider format, we use pivot_wider():\n\nManaus_short &lt;- Manaus_long %&gt;%\n  pivot_wider( names_from = Month, values_from = value)\n\nPlotting the data\n\n## Convert abbreviation for month into number\nManaus_long &lt;- \n  Manaus_long %&gt;% \n  mutate( Month = case_when( \n    Month == \"JAN\" ~ \"01\", Month == \"FEB\" ~ \"02\", Month == \"MAR\" ~ \"03\", \n    Month == \"APR\" ~ \"04\", Month == \"MAY\" ~ \"05\", Month == \"JUN\" ~ \"06\",\n    Month == \"JUL\" ~ \"07\", Month == \"AUG\" ~ \"08\", Month == \"SEP\" ~ \"09\", \n    Month == \"OCT\" ~ \"10\", Month == \"NOV\" ~ \"11\", Month == \"DEC\" ~ \"12\"\n    ) \n  )\n\n## Combine year and month and convert to date\nManaus_long &lt;- Manaus_long %&gt;%\n  mutate( Date = paste( Manaus_long$YEAR, Manaus_long$Month, sep=\"-\" ) ) %&gt;%\n  mutate( Date = ym( Date ) )\n\n\nManaus_long %&gt;%\n  mutate( value = na_if( value, 999.9 ) ) %&gt;%\n  ggplot( aes(\"x\"=Date, \"y\"=value) ) + \n  geom_line() + geom_smooth() +\n  labs( y=\"Average Monthly Temperature\") + theme_bw() +\n  theme( axis.title=element_text(size=16), axis.text=element_text(size=14) )\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\nAverage monthly temperature for Manaus, Brazil, between 1910 and 2019."
  },
  {
    "objectID": "slides/week_6/analysis_of_the_work_by_charles_dickens.html",
    "href": "slides/week_6/analysis_of_the_work_by_charles_dickens.html",
    "title": "Analysis of the work by Charles Dickens",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(tidytext)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(topicmodels)\n\n\nDickens_raw &lt;- read.csv(\"dickens.csv\" )\n\n\nhead(Dickens_raw)\n\n                              text                title\n1             A TALE OF TWO CITIES A Tale of Two Cities\n2                                  A Tale of Two Cities\n3 A STORY OF THE FRENCH REVOLUTION A Tale of Two Cities\n4                                  A Tale of Two Cities\n5               By Charles Dickens A Tale of Two Cities\n6                                  A Tale of Two Cities"
  },
  {
    "objectID": "slides/week_6/analysis_of_the_work_by_charles_dickens.html#loading-packages-and-data",
    "href": "slides/week_6/analysis_of_the_work_by_charles_dickens.html#loading-packages-and-data",
    "title": "Analysis of the work by Charles Dickens",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(tidytext)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(topicmodels)\n\n\nDickens_raw &lt;- read.csv(\"dickens.csv\" )\n\n\nhead(Dickens_raw)\n\n                              text                title\n1             A TALE OF TWO CITIES A Tale of Two Cities\n2                                  A Tale of Two Cities\n3 A STORY OF THE FRENCH REVOLUTION A Tale of Two Cities\n4                                  A Tale of Two Cities\n5               By Charles Dickens A Tale of Two Cities\n6                                  A Tale of Two Cities"
  },
  {
    "objectID": "slides/week_6/analysis_of_the_work_by_charles_dickens.html#prepare-the-data",
    "href": "slides/week_6/analysis_of_the_work_by_charles_dickens.html#prepare-the-data",
    "title": "Analysis of the work by Charles Dickens",
    "section": "Prepare the data",
    "text": "Prepare the data\n\nDickens &lt;- Dickens_raw %&gt;%\n  unnest_tokens( word, text ) %&gt;% \n  mutate( word = gsub( \"_\", \"\", word ) )"
  },
  {
    "objectID": "slides/week_6/analysis_of_the_work_by_charles_dickens.html#calculate-tf-idf-with-stop-words-kept-in-the-analysis",
    "href": "slides/week_6/analysis_of_the_work_by_charles_dickens.html#calculate-tf-idf-with-stop-words-kept-in-the-analysis",
    "title": "Analysis of the work by Charles Dickens",
    "section": "Calculate tf-idf with stop words kept in the analysis",
    "text": "Calculate tf-idf with stop words kept in the analysis\nExtract counts for each word\n\nDickensCount &lt;- Dickens %&gt;% count( title, word, sort=TRUE )\n\n\nslice_head(DickensCount, n=10)\n\n                  title word    n\n1          Oliver Twist  the 9633\n2    Great Expectations  the 8145\n3  A Tale of Two Cities  the 8053\n4    Great Expectations  and 7098\n5    Great Expectations    i 6667\n6          Oliver Twist  and 5428\n7    Great Expectations   to 5157\n8  A Tale of Two Cities  and 4998\n9    Great Expectations   of 4438\n10   Great Expectations    a 4053\n\n\nCalculate tf-idf using bind_tf_idf() from the tidytext package:\n\nDickens_tf.idf &lt;- DickensCount %&gt;% \n  bind_tf_idf( word, title, n ) %&gt;%\n  arrange( desc(tf_idf) ) %&gt;%\n  mutate( idf=round(idf,2) )\nslice_head( Dickens_tf.idf, n=10 )\n\n                  title     word   n          tf  idf      tf_idf\n1     A Christmas Carol  scrooge 327 0.011047671 1.39 0.015315323\n2          Oliver Twist   oliver 876 0.005397013 1.39 0.007481849\n3  A Tale of Two Cities    lorry 369 0.002664549 1.39 0.003693849\n4          Oliver Twist   bumble 397 0.002445907 1.39 0.003390747\n5          Oliver Twist    sikes 354 0.002180985 1.39 0.003023487\n6  A Tale of Two Cities  defarge 302 0.002180742 1.39 0.003023150\n7          Oliver Twist    fagin 309 0.001903741 1.39 0.002639145\n8    Great Expectations      pip 341 0.001803746 1.39 0.002500523\n9    Great Expectations havisham 318 0.001682086 1.39 0.002331866\n10   Great Expectations  herbert 313 0.001655638 1.39 0.002295201"
  },
  {
    "objectID": "slides/week_6/analysis_of_the_work_by_charles_dickens.html#calculate-tf-idf-with-stop-words-removed",
    "href": "slides/week_6/analysis_of_the_work_by_charles_dickens.html#calculate-tf-idf-with-stop-words-removed",
    "title": "Analysis of the work by Charles Dickens",
    "section": "Calculate tf-idf with stop words removed",
    "text": "Calculate tf-idf with stop words removed\nExtract counts for each word\n\nDickensCount &lt;- Dickens %&gt;% \n  anti_join( stop_words, by=\"word\" ) %&gt;%\n  count( title, word, sort=TRUE )\n\nCalculate tf-idf using bind_tf_idf() from the tidytext package:\n\nDickens_tf.idf &lt;- DickensCount %&gt;% \n  bind_tf_idf( word, title, n ) %&gt;%\n  arrange( desc(tf_idf) ) %&gt;%\n  mutate( idf=round(idf,2) )\nslice_head( Dickens_tf.idf, n=10 )\n\n                  title     word   n          tf  idf      tf_idf\n1     A Christmas Carol  scrooge 327 0.032175539 1.39 0.044604768\n2          Oliver Twist   oliver 876 0.015483324 1.39 0.021464444\n3  A Tale of Two Cities    lorry 369 0.007949331 1.39 0.011020113\n4          Oliver Twist   bumble 397 0.007016986 1.39 0.009727608\n5  A Tale of Two Cities  defarge 302 0.006505957 1.39 0.009019171\n6          Oliver Twist    sikes 354 0.006256960 1.39 0.008673988\n7    Great Expectations      pip 341 0.006045027 1.39 0.008380188\n8    Great Expectations havisham 318 0.005637298 1.39 0.007814955\n9    Great Expectations  herbert 313 0.005548662 1.39 0.007692078\n10         Oliver Twist    fagin 309 0.005461583 1.39 0.007571362"
  },
  {
    "objectID": "slides/week_6/analysis_of_the_work_by_charles_dickens.html#create-identifier-for-each-chapter",
    "href": "slides/week_6/analysis_of_the_work_by_charles_dickens.html#create-identifier-for-each-chapter",
    "title": "Analysis of the work by Charles Dickens",
    "section": "Create identifier for each chapter",
    "text": "Create identifier for each chapter\nExtract books and chapter\n\nDickens_chapters &lt;- Dickens_raw %&gt;%\n  filter( title %in% c( \"Great Expectations\",\"A Tale of Two Cities\" ) ) %&gt;%\n  group_by( title ) %&gt;%\n  mutate( chapter = cumsum( \n    str_detect( text, regex( \"^chapter \", ignore_case = TRUE ) ) \n  ) )\n\nCreate identifier\n\nDickens_chapters &lt;- Dickens_chapters %&gt;%\n  filter( chapter &gt; 0 ) %&gt;%\n  unite( col=document, title, chapter )\n\n\nhead(Dickens_chapters)\n\n# A tibble: 6 × 2\n  text                                                                  document\n  &lt;chr&gt;                                                                 &lt;chr&gt;   \n1 \"CHAPTER I.\"                                                          A Tale …\n2 \"The Period\"                                                          A Tale …\n3 \"\"                                                                    A Tale …\n4 \"\"                                                                    A Tale …\n5 \"It was the best of times, it was the worst of times, it was the age… A Tale …\n6 \"wisdom, it was the age of foolishness, it was the epoch of belief, … A Tale …"
  },
  {
    "objectID": "slides/week_6/analysis_of_the_work_by_charles_dickens.html#construct-the-document-term-matrix",
    "href": "slides/week_6/analysis_of_the_work_by_charles_dickens.html#construct-the-document-term-matrix",
    "title": "Analysis of the work by Charles Dickens",
    "section": "Construct the document term matrix",
    "text": "Construct the document term matrix\nSplit text into individual words and remove stop words:\n\nDickens_chapters &lt;- Dickens_chapters %&gt;%\n  unnest_tokens( word, text ) %&gt;%\n  mutate( word = gsub( \"_\", \"\", word ) ) %&gt;%\n  anti_join( stop_words, by=\"word\" )\n\nExtract counts for each word per chapter\n\nDickens_chapters &lt;- Dickens_chapters %&gt;% count( document, word, sort = TRUE )\n\nConstruct the document term matrix\n\nDickens_dtm &lt;- Dickens_chapters %&gt;% cast_dtm( document, word, n )\n\n\nDickens_dtm\n\n&lt;&lt;DocumentTermMatrix (documents: 104, terms: 13797)&gt;&gt;\nNon-/sparse entries: 65953/1368935\nSparsity           : 95%\nMaximal term length: 19\nWeighting          : term frequency (tf)"
  },
  {
    "objectID": "slides/week_6/analysis_of_the_work_by_charles_dickens.html#estimate-laten-dirichlet-allocation-model",
    "href": "slides/week_6/analysis_of_the_work_by_charles_dickens.html#estimate-laten-dirichlet-allocation-model",
    "title": "Analysis of the work by Charles Dickens",
    "section": "Estimate Laten Dirichlet Allocation model",
    "text": "Estimate Laten Dirichlet Allocation model\n\nDickens_LDA &lt;- LDA( Dickens_dtm, k=2, method = \"Gibbs\", control=list(seed=123) )"
  },
  {
    "objectID": "slides/week_6/analysis_of_the_work_by_charles_dickens.html#analysis-of-the-estimates",
    "href": "slides/week_6/analysis_of_the_work_by_charles_dickens.html#analysis-of-the-estimates",
    "title": "Analysis of the work by Charles Dickens",
    "section": "Analysis of the estimates",
    "text": "Analysis of the estimates\nMake up of the chapters\n\nDickens_topics &lt;- tidy( Dickens_LDA , matrix = \"gamma\" ) \nDickens_topics %&gt;% slice_head( n=4 )\n\n# A tibble: 4 × 3\n  document              topic gamma\n  &lt;chr&gt;                 &lt;int&gt; &lt;dbl&gt;\n1 Great Expectations_57     1 0.285\n2 Great Expectations_7      1 0.185\n3 Great Expectations_38     1 0.371\n4 Great Expectations_17     1 0.259\n\n\n\nDickens_topics %&gt;%\n  separate( document, c(\"title\", \"chapter\"), sep=\"_\", convert=TRUE ) %&gt;%\n  ggplot( aes( x=factor(topic), y=gamma ) ) + \n  facet_wrap( ~title ) + geom_boxplot() + \n  labs( x=\"Topic\", y=\"Proportion\" )\n\n\n\n\n\n\n\nMake up of the topics\n\ntidy( Dickens_LDA , matrix = \"beta\" ) %&gt;%\n  group_by( topic ) %&gt;%\n  slice_max( beta, n=3 )\n\n# A tibble: 6 × 3\n# Groups:   topic [2]\n  topic term       beta\n  &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;\n1     1 lorry   0.00702\n2     1 hand    0.00643\n3     1 defarge 0.00574\n4     2 joe     0.0143 \n5     2 miss    0.00941\n6     2 don     0.00765\n\n\n\ntidy( Dickens_LDA , matrix = \"beta\" ) %&gt;%\n  mutate( topic = case_when( topic==1 ~ \"Topic1\", topic==2 ~ \"Topic2\") ) %&gt;%\n  pivot_wider( names_from = topic, values_from = beta, values_fill = 0 ) %&gt;%\n  ggplot( aes(x=Topic1, y=Topic2) ) + geom_point() +  \n  geom_text( aes(label=term), check_overlap = TRUE, vjust=1 ) + \n  coord_trans( x=\"sqrt\", y=\"sqrt\" ) + theme_bw() +\n  labs( x=\"Term Frequency in Topic 1\", y=\"Term Frequency in Topic 2\" )\n\nWarning: `coord_trans()` was deprecated in ggplot2 4.0.0.\nℹ Please use `coord_transform()` instead."
  },
  {
    "objectID": "slides/week_6/text_data_analysis_(part_2).html",
    "href": "slides/week_6/text_data_analysis_(part_2).html",
    "title": "MA22019 - Text Data Analysis (Part 2)",
    "section": "",
    "text": "We complete the chapter on text data analysis:\n\nTerm frequency - inverse document frequency (Section 3.3.2)\nProblem Class 4\nTopic modelling (Section 3.4)"
  },
  {
    "objectID": "slides/week_6/text_data_analysis_(part_2).html#plan-for-today",
    "href": "slides/week_6/text_data_analysis_(part_2).html#plan-for-today",
    "title": "MA22019 - Text Data Analysis (Part 2)",
    "section": "",
    "text": "We complete the chapter on text data analysis:\n\nTerm frequency - inverse document frequency (Section 3.3.2)\nProblem Class 4\nTopic modelling (Section 3.4)"
  },
  {
    "objectID": "slides/week_6/text_data_analysis_(part_2).html#motivation",
    "href": "slides/week_6/text_data_analysis_(part_2).html#motivation",
    "title": "MA22019 - Text Data Analysis (Part 2)",
    "section": "Motivation",
    "text": "Motivation\nWe learned last week that a text can be analysed based on\n\nFrequency of words within it\nIts emotional intent\n\nThese aspects can also be considered when comparing texts.\nHowever: This may not be the best way to:\n\nIdentify the key words that best describe a text\nGroup a large number of texts into subgroups\n\nToday we will address these aspects."
  },
  {
    "objectID": "slides/week_6/text_data_analysis_(part_2).html#motivation-1",
    "href": "slides/week_6/text_data_analysis_(part_2).html#motivation-1",
    "title": "MA22019 - Text Data Analysis (Part 2)",
    "section": "Motivation",
    "text": "Motivation\nSuppose \\(D\\) is a large number of documents / texts / websites, also referred to as a corpus.\nHow do we decide which words are specific to a text?\nSuch information is useful for\n\nDesigning search engines\nDocument classification\nText summarization"
  },
  {
    "objectID": "slides/week_6/text_data_analysis_(part_2).html#tf-and-idf",
    "href": "slides/week_6/text_data_analysis_(part_2).html#tf-and-idf",
    "title": "MA22019 - Text Data Analysis (Part 2)",
    "section": "tf and idf",
    "text": "tf and idf\nWe have two components:\n\nterm frequency of the word \\(t\\) in the document \\(d\\) [ (t,d)= ]\ninverse document frequency of \\(t\\) across \\(D\\) [ (t,D) = (). ]"
  },
  {
    "objectID": "slides/week_6/text_data_analysis_(part_2).html#tf-idf",
    "href": "slides/week_6/text_data_analysis_(part_2).html#tf-idf",
    "title": "MA22019 - Text Data Analysis (Part 2)",
    "section": "tf-idf",
    "text": "tf-idf\nThe term frequency - inverse document frequency is then defined as [ (t,d,D) = (t,d) (t,D). ]\nWhat are the properties of tf.idf?\n\n\\(\\mathrm{tf.idf}(t,d,D)=0\\) if \\(t\\) occurs in all documents\n\\(\\mathrm{tf.idf}(t,d,D)\\) is large if \\(t\\) occurs very often and only in one document\n\nWhether stop words should be removed or not depends on the application."
  },
  {
    "objectID": "slides/week_6/text_data_analysis_(part_2).html#example---books-by-charles-dickens",
    "href": "slides/week_6/text_data_analysis_(part_2).html#example---books-by-charles-dickens",
    "title": "MA22019 - Text Data Analysis (Part 2)",
    "section": "Example - Books by Charles Dickens",
    "text": "Example - Books by Charles Dickens\nLet’s look at a corpus containing only four books\n\nA Christmas Carol\nA Tale of Two Cities\nGreat Expectations\nOliver Twist\n\nWhich words would we expect to have the highest tf-idf?\n-&gt; R Markdown file"
  },
  {
    "objectID": "slides/week_6/text_data_analysis_(part_2).html#overview",
    "href": "slides/week_6/text_data_analysis_(part_2).html#overview",
    "title": "MA22019 - Text Data Analysis (Part 2)",
    "section": "Overview",
    "text": "Overview\nWe want to illustrate the techniques covered so far by analyzing the books by Jane Austen:\n\nEmma\nMansfield Park\nNorthanger Abbey\nPersuasion\nPride and Prejudice\nSense and Sensibility\n\n-&gt; R Markdown file"
  },
  {
    "objectID": "slides/week_6/text_data_analysis_(part_2).html#motivation---how-do-write-a-text",
    "href": "slides/week_6/text_data_analysis_(part_2).html#motivation---how-do-write-a-text",
    "title": "MA22019 - Text Data Analysis (Part 2)",
    "section": "Motivation - How do write a text?",
    "text": "Motivation - How do write a text?\nConsider the following aspects:\n\nWill the words we use depend on the topic we want to write about?\nCan we have several topics in a (longer) piece of text?\n\nHow can we adjust for such aspects in a model?"
  },
  {
    "objectID": "slides/week_6/text_data_analysis_(part_2).html#framework-and-principles",
    "href": "slides/week_6/text_data_analysis_(part_2).html#framework-and-principles",
    "title": "MA22019 - Text Data Analysis (Part 2)",
    "section": "Framework and Principles",
    "text": "Framework and Principles\nWe now have a corpus \\(D\\)\n\n\\(N\\) documents and\n\\(M\\) unique words across all documents.\n\nThe topics nor the term frequency with which words appear within a topic are known to us.\nOur framework should allow for the following principles:\n\nEvery document is a mixture of topics.\nEvery topic is a mixture of words."
  },
  {
    "objectID": "slides/week_6/text_data_analysis_(part_2).html#latent-dirichlet-allocation-lda",
    "href": "slides/week_6/text_data_analysis_(part_2).html#latent-dirichlet-allocation-lda",
    "title": "MA22019 - Text Data Analysis (Part 2)",
    "section": "Latent Dirichlet Allocation (LDA)",
    "text": "Latent Dirichlet Allocation (LDA)\nIntroduced in the early 2000s, the key aspects are:\n\nWe specify the number \\(K\\) of topics.\nThe proportions \\((\\psi_{i,1},\\ldots,\\psi_{i,K})\\) describe how much each topic features in document \\(i\\).\nThe proportions \\(\\left(\\theta_{k,1},\\ldots,\\theta_{k,M}\\right)\\) describe the term frequency (distribution of words) for a text from topic \\(k\\).\n\nThe proportions are estimated using the topicmodels package, and we should remove the stop words."
  },
  {
    "objectID": "slides/week_6/text_data_analysis_(part_2).html#using-the-topicmodels-package",
    "href": "slides/week_6/text_data_analysis_(part_2).html#using-the-topicmodels-package",
    "title": "MA22019 - Text Data Analysis (Part 2)",
    "section": "Using the topicmodels package",
    "text": "Using the topicmodels package\nThere are few steps that need to be performed:\n\nConstruct the document term matrix \\(A\\in\\mathbb{R}^{N\\times M}\\), where \\(A_{i,j}\\) denotes the number of counts of word \\(j\\) in document \\(i\\)\nFit the LDA model with a fixed value for \\(K\\)\n\nAnalyse the proportions:\n\n\\((\\psi_{i,1},\\ldots,\\psi_{i,K}),~i=1,\\ldots,N\\)\n\\(\\left(\\theta_{k,1},\\ldots,\\theta_{k,M}\\right),~k=1,\\ldots,K\\)"
  },
  {
    "objectID": "slides/week_6/text_data_analysis_(part_2).html#examples",
    "href": "slides/week_6/text_data_analysis_(part_2).html#examples",
    "title": "MA22019 - Text Data Analysis (Part 2)",
    "section": "Examples",
    "text": "Examples\nLet’s look at two examples:\n\n\nRecovering the books by Charles Dickens\n-&gt; R Markdown file\n\n\nClassifying news articles by the New York Times\n-&gt; R Markdown file"
  },
  {
    "objectID": "slides/week_6/text_data_analysis_(part_2).html#summary",
    "href": "slides/week_6/text_data_analysis_(part_2).html#summary",
    "title": "MA22019 - Text Data Analysis (Part 2)",
    "section": "Summary",
    "text": "Summary\nWe learned how to:\n\nIdentify the most common words within a text\nAnalyse the sentiment of a text\n\nWhen dealing with several a corpus, we can:\n\nMake comparisons between texts\nIdentify the words that best represent each text (if-idf)\nEstimate a topic model using Latent Dirichlet allocation"
  },
  {
    "objectID": "slides/week_6/text_data_analysis_(part_2).html#remark",
    "href": "slides/week_6/text_data_analysis_(part_2).html#remark",
    "title": "MA22019 - Text Data Analysis (Part 2)",
    "section": "Remark",
    "text": "Remark\nThe methods we introduced are still popular, but they are slowly being replaced by deep learning and transformer-based models in practice:\n\nBidirectional Encoder Representations from Transformers (Bert)\nNeural Topic Models\nWord2Vec and Top2Vec\n\nNevertheless, for the purposes of an initial analysis, the methods we introduced do a good job."
  },
  {
    "objectID": "slides/week_1/Overview.html",
    "href": "slides/week_1/Overview.html",
    "title": "MA22019 - Overview",
    "section": "",
    "text": "Overview of the course\nData cleaning (Section 1.1)\nWorking with a single data frame (Section 1.2)"
  },
  {
    "objectID": "slides/week_1/Overview.html#plan-for-today",
    "href": "slides/week_1/Overview.html#plan-for-today",
    "title": "MA22019 - Overview",
    "section": "",
    "text": "Overview of the course\nData cleaning (Section 1.1)\nWorking with a single data frame (Section 1.2)"
  },
  {
    "objectID": "slides/week_1/Overview.html#welcome-to-the-world-of-data-science",
    "href": "slides/week_1/Overview.html#welcome-to-the-world-of-data-science",
    "title": "MA22019 - Overview",
    "section": "Welcome to the World of Data Science!",
    "text": "Welcome to the World of Data Science!"
  },
  {
    "objectID": "slides/week_1/Overview.html#content-and-schedule",
    "href": "slides/week_1/Overview.html#content-and-schedule",
    "title": "MA22019 - Overview",
    "section": "Content and Schedule",
    "text": "Content and Schedule\n\n\nWeek\nTopic\n\n\n\n1 - 2\nData Wrangling\n\n\n2 - 3\nData Visualization\n\n\n4\nCoursework 1\n\n\n5 - 6\nText Data Analysis\n\n\n7 - 9\nSpatial Data Analysis\n\n\n10 - 11\nCoursework 2"
  },
  {
    "objectID": "slides/week_1/Overview.html#outlook",
    "href": "slides/week_1/Overview.html#outlook",
    "title": "MA22019 - Overview",
    "section": "Outlook",
    "text": "Outlook\nAt the end of the course, you will be able to produce and interpret graphics such as"
  },
  {
    "objectID": "slides/week_1/Overview.html#r-and-rstudio",
    "href": "slides/week_1/Overview.html#r-and-rstudio",
    "title": "MA22019 - Overview",
    "section": "R and RStudio",
    "text": "R and RStudio\nWe will use R throughout this course.\nI recommend that you install R and RStudio on your own computer / laptop.\nYou should run the commands in “InstallPackages.R” to ensure you have all the relevant R packages.\nExcept for patchwork, the packages we use for Weeks 1-4 are already installed on the campus computers."
  },
  {
    "objectID": "slides/week_1/Overview.html#timetable",
    "href": "slides/week_1/Overview.html#timetable",
    "title": "MA22019 - Overview",
    "section": "Timetable",
    "text": "Timetable\nThe number of hours varies a bit across weeks\n\nIn-person lecture on Wednesday 10:15-12:05 (W 1-3, 5-9) / 10:15-11:05 (W 4,10,11)\nOnline problem class (on Zoom) on Friday 14:15-15:05 (W 1-3)\n\nTutorial: Take place on Tuesday. Please check your timetable for time and location. No tutorial in Week 5!\nGetting Help:\n\nUse the Padlet board or send me an email\nOffice Hour: Monday 9:00-11:00 (in-person)"
  },
  {
    "objectID": "slides/week_1/Overview.html#what-can-you-find-on-moodle",
    "href": "slides/week_1/Overview.html#what-can-you-find-on-moodle",
    "title": "MA22019 - Overview",
    "section": "What can you find on Moodle?",
    "text": "What can you find on Moodle?\n\nLecture notes in HTML and PDF format\nData sets and R files to reproduce the results in the lecture notes\nMaterial for the problem class\nProblem sheets and solutions\nSlides from the lecture\nCheat sheets for some of the R packages we will be using\nCoursework"
  },
  {
    "objectID": "slides/week_1/Overview.html#problem-sheets-and-coursework",
    "href": "slides/week_1/Overview.html#problem-sheets-and-coursework",
    "title": "MA22019 - Overview",
    "section": "Problem Sheets and Coursework",
    "text": "Problem Sheets and Coursework\nProblem sheets provide\n\nExercises for revision. Check answers on Moodle.\nTutorial questions: More advanced than exercises.\nHomework question: Similar to coursework questions.\n\nYour mark will be determined by two pieces of coursework:\n\nCW1 (40%): Set 15:00 on 21 Feb, due 17:00 on 28 Feb.\nCW2 (60%): Set at the start of Week 10, due in Week 11.\n\nYou will receive individual feedback and provisional marks for CW1 before the release of CW2."
  },
  {
    "objectID": "slides/week_1/Overview.html#changes-made-based-on-oue-feedback",
    "href": "slides/week_1/Overview.html#changes-made-based-on-oue-feedback",
    "title": "MA22019 - Overview",
    "section": "Changes made based on OUE feedback",
    "text": "Changes made based on OUE feedback\n\nAdditional problem classes in Weeks 1-3\nMore guidance on how coursework will be marked.\n\nEach coursework question will be assessed as a whole based on:\n\nCorrectness\nQuality\nPresentation and conciseness\n\nTo reflect this, your answer to the homework question will be marked as (i) &lt;50%, (ii) 50-69% or (iii) &gt;=70%."
  },
  {
    "objectID": "slides/week_1/Overview.html#example",
    "href": "slides/week_1/Overview.html#example",
    "title": "MA22019 - Overview",
    "section": "Example",
    "text": "Example\nSuppose we had ambulance response data which includes:\n\nWaiting time of the patient\nAmbulance response category (1=‘life-threatening’, 4=‘non-urgent’)\nTime patient spent in hospital\n\nYou are asked to consider the question:\n“Does the data suggest that the length of stay in hospital increases with the time until the ambulance arrives?”\nI will present three answers, and you have to decide a mark."
  },
  {
    "objectID": "slides/week_1/Overview.html#any-questions",
    "href": "slides/week_1/Overview.html#any-questions",
    "title": "MA22019 - Overview",
    "section": "Any questions?",
    "text": "Any questions?\nLet’s have a 5-minute break."
  },
  {
    "objectID": "slides/week_1/Example.html",
    "href": "slides/week_1/Example.html",
    "title": "Marking of homework questions - example",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nAmbulance &lt;- read.csv(\"ambulance.csv\" )\nAmbulance &lt;- Ambulance %&gt;% mutate( Call=ymd_hms(Call) )\nHospitals &lt;- read.csv(\"hospital.csv\" ) \nPatientsAmbulance &lt;- inner_join( Ambulance, Hospitals )\nPatientsAmbulance &lt;- PatientsAmbulance %&gt;% \n  mutate( Arrival=ymd_hms(Arrival), Wait=as.numeric(Arrival-Call) )\nPatientsAmbulance &lt;- PatientsAmbulance %&gt;%\n  mutate( Category2 = case_when( Category2==1 ~ \"Category 1\",\n                                 Category2==2 ~ \"Category 2\",\n                                 Category2==3 ~ \"Category 3\",\n                                 Category2==4 ~ \"Category 4\" ) )"
  },
  {
    "objectID": "slides/week_1/Example.html#answer-1",
    "href": "slides/week_1/Example.html#answer-1",
    "title": "Marking of homework questions - example",
    "section": "Answer 1",
    "text": "Answer 1\nWe create a plot of waiting time versus length of stay:\n\nggplot( PatientsAmbulance, aes(x=Wait,y=Length) ) + geom_point() + geom_smooth() + \n  labs( x=\"Waiting Time in Minutes\", y=\"Length of Hospiyal Stay in Days\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\nWe find that in fact the opposite is observed: The length of the hospital stay decrease as the time taken to reach the patient increases. This would imply that the more severe a patient’s case, the faster the ambulance is likely to get to them - that is, the ambulance service is prioritizing the care of the patients who need it more urgently."
  },
  {
    "objectID": "slides/week_1/Example.html#answer-2",
    "href": "slides/week_1/Example.html#answer-2",
    "title": "Marking of homework questions - example",
    "section": "Answer 2",
    "text": "Answer 2\nWe create a plot of waiting time versus length of stay:\n\nggplot( PatientsAmbulance, aes(x=Wait,y=Length) ) + geom_point() + geom_smooth() + \n  labs( x=\"Waiting Time in Minutes\", y=\"Length of Hospiyal Stay in Days\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\nThere is a fairly weak correlation between the length of stay in hospital and the time taken for the ambulance to arrive. From 0-100 minutes the length of stay decreases as the time for arrival increases and from 100-300 minutes the length of stay in hospital remains fairly constant."
  },
  {
    "objectID": "slides/week_1/Example.html#answer-3",
    "href": "slides/week_1/Example.html#answer-3",
    "title": "Marking of homework questions - example",
    "section": "Answer 3",
    "text": "Answer 3\nSince the effect may vary across the different ambulance categories, I create a plot of waiting time against length of stay for each ambulance response category:\n\nggplot( PatientsAmbulance, aes(x=Wait,y=Length) ) +\n  facet_wrap(~Category2, scales = \"free_x\") + geom_point() + geom_smooth() + \n  labs( x=\"Waiting Time in Minutes\", y=\"Length of Hospiyal Stay in Days\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\nThe plots show that for the most severe cases (Categories 1 and 2), we see a strong increase in the length of hospital stay for patients with a long waiting. For Category 1, the average length of stay starts to increase for patients that wait more than 15 minutes. For patients in Category 2, the average length of stay tends to increase if patients wait for more than 50 minutes. Finally, there is a small increase in the average for patients in Category 3 waiting more than 150 minutes, and no effect is visible for patients in Category 4. In summary, the data supports the statement that the length of stay tends to increase with the time the patient is waiting."
  },
  {
    "objectID": "slides/week_8/analysis_of_colorado_precipitation_data.html",
    "href": "slides/week_8/analysis_of_colorado_precipitation_data.html",
    "title": "Analysis of Monthly Precipitation in Colorado",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(ggspatial)\nlibrary(patchwork)\n\nColoradoPrecip &lt;- read.csv(\"precipitationcolorado.csv\" )\nColoradoCities &lt;- read.csv(\"cities_colorado.csv\" )"
  },
  {
    "objectID": "slides/week_8/analysis_of_colorado_precipitation_data.html#load-the-packages-and-data",
    "href": "slides/week_8/analysis_of_colorado_precipitation_data.html#load-the-packages-and-data",
    "title": "Analysis of Monthly Precipitation in Colorado",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(ggspatial)\nlibrary(patchwork)\n\nColoradoPrecip &lt;- read.csv(\"precipitationcolorado.csv\" )\nColoradoCities &lt;- read.csv(\"cities_colorado.csv\" )"
  },
  {
    "objectID": "slides/week_8/analysis_of_colorado_precipitation_data.html#check-linearity",
    "href": "slides/week_8/analysis_of_colorado_precipitation_data.html#check-linearity",
    "title": "Analysis of Monthly Precipitation in Colorado",
    "section": "Check linearity",
    "text": "Check linearity\n\nggplot( ColoradoPrecip, aes( x=VALDEZ, y=WETMORE) ) + geom_point() + theme_bw() + \nggplot( ColoradoPrecip, aes( x=TOWNER, y=STONINGTON) ) + geom_point() + theme_bw() + \nggplot( ColoradoPrecip, aes( x=CARDIFF, y=NORTHGLENN) ) + geom_point() + theme_bw()"
  },
  {
    "objectID": "slides/week_8/analysis_of_colorado_precipitation_data.html#derive-eigenvectors-and-eigenvalues",
    "href": "slides/week_8/analysis_of_colorado_precipitation_data.html#derive-eigenvectors-and-eigenvalues",
    "title": "Analysis of Monthly Precipitation in Colorado",
    "section": "Derive eigenvectors and eigenvalues",
    "text": "Derive eigenvectors and eigenvalues\n\nhead(ColoradoPrecip)\n\n        date VALDEZ WETMORE WATKINS MOUNTAINVIEW HIDDENVALLEY MCCOY STERLING\n1 2010-01-01   12.6     7.7     4.3          7.0          6.9  13.6      0.8\n2 2010-02-01   28.5    20.3    17.3         25.4         27.7  17.4     12.2\n3 2010-03-01   41.3    42.9    32.2         49.7         43.8  17.7     24.8\n4 2010-04-01   24.0    25.7    59.2         72.3         62.1  27.8     75.8\n5 2010-05-01   25.8    26.7    51.7         48.5         40.6  27.5     65.6\n6 2010-06-01   24.0    18.2    60.1         51.5         51.4  30.6    102.6\n  TOWNER BELLVUE SUPERIOR STONINGTON WALLSTREET CALHAN UTLEYVILLE GLENDALE\n1    7.9     3.5      7.2        7.8        8.2    6.3       12.8      6.5\n2    9.4    14.4     27.0       15.0       27.5   15.3       18.5     23.3\n3   29.8    39.2     55.1       31.5       38.4   34.1       42.7     43.5\n4   44.8    88.6     87.5       27.2       64.3   42.7       30.3     63.6\n5   49.4    63.2     53.2       43.1       46.8   50.3       33.1     48.2\n6   31.8    69.9     57.4       48.3       55.2   52.3       39.1     51.3\n  FORTMORGAN BONDAD NUNN CARDIFF NORTHGLENN OHIO LYCAN SEGUNDO EDWARDS CORNISH\n1        0.9   52.6  2.3    19.8        6.1 20.7   8.9    12.5    13.3     2.0\n2        6.9   36.3 13.4    31.4       22.2 32.0  13.6    28.1    18.9    10.6\n3       22.2   34.8 31.9    35.6       45.6 48.7  33.1    38.9    26.1    25.1\n4       64.6   20.4 76.3    38.2       74.3 32.0  33.2    25.6    31.5    64.8\n5       49.5    4.3 59.4    29.6       50.5 14.1  40.1    24.9    25.6    53.8\n6       78.3    5.8 76.0    28.3       55.9 20.6  38.3    22.5    29.0    78.6\n  HOYT GROVER STONEHAM PENROSE WILLARD\n1  2.3    1.2      1.0     6.4     0.9\n2 10.2    9.4      7.7    17.6     9.0\n3 26.8   20.7     20.0    40.2    23.1\n4 60.1   62.5     70.8    30.5    69.3\n5 52.1   59.2     61.8    29.5    61.4\n6 73.1   82.1     91.0    21.5    95.1\n\n\n\nPrecip &lt;- ColoradoPrecip %&gt;% select( -date )\nPCA &lt;- prcomp( Precip, scale. = TRUE ) ## Combines all 3 steps"
  },
  {
    "objectID": "slides/week_8/analysis_of_colorado_precipitation_data.html#plot-the-ratio-of-eigenvalues",
    "href": "slides/week_8/analysis_of_colorado_precipitation_data.html#plot-the-ratio-of-eigenvalues",
    "title": "Analysis of Monthly Precipitation in Colorado",
    "section": "Plot the ratio of eigenvalues",
    "text": "Plot the ratio of eigenvalues\nNote: prcomp() stores \\(\\sqrt{\\lambda_1},\\ldots,\\sqrt{\\lambda_n}\\):\n\nColoradoEV &lt;- data.frame( m=1:30, Ratio = cumsum(PCA$sdev^2) / sum(PCA$sdev^2) )\nggplot( ColoradoEV, aes(x=m, y=Ratio) ) + geom_point() + \n  geom_abline( intercept = 0.9, slope = 0, linewidth=1.5 ) + theme_bw() +\n  ggtitle(\"Proportion of variance explained\")\n\n\n\n\n\n\n\nWhat do we conclude?"
  },
  {
    "objectID": "slides/week_8/analysis_of_colorado_precipitation_data.html#plot-eigenvectors",
    "href": "slides/week_8/analysis_of_colorado_precipitation_data.html#plot-eigenvectors",
    "title": "Analysis of Monthly Precipitation in Colorado",
    "section": "Plot eigenvectors",
    "text": "Plot eigenvectors\nWe attach the eigenvectors to the spatial sites\n\nColoradoCities &lt;- ColoradoCities %&gt;%\n  mutate( PC1 = PCA$rotation[,1], PC2 = PCA$rotation[,2], PC3 = PCA$rotation[,3] ) %&gt;%\n  pivot_longer( cols=PC1:PC3, names_to=\"PC\" )\n\nVisualize the eigenvectors\n\nggplot( ColoradoCities, aes( x=LONG, y=LAT) ) + \n  facet_wrap( ~PC ) +\n  annotation_map_tile() + \n  geom_spatial_point( aes(color=value), size=5 ) + \n  scale_color_distiller( palette=\"RdYlBu\" ) +\n  labs( x=\"Longitude\", y=\"Latitude\", color=\"value\" )"
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "",
    "text": "We introduce two more techniques for analysing point-referenced data:\n\nSemi-variogram (Section 4.3)\nPrincipal component analysis (Section 4.4)\n\nThe two other types of spatial data will be considered in more detail in Week 9."
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#plan-for-today",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#plan-for-today",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "",
    "text": "We introduce two more techniques for analysing point-referenced data:\n\nSemi-variogram (Section 4.3)\nPrincipal component analysis (Section 4.4)\n\nThe two other types of spatial data will be considered in more detail in Week 9."
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#motivation",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#motivation",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Motivation",
    "text": "Motivation\nIn terms of analyzing point-referenced data, we explored\n\nVisualization\nInverse distance weighting\n\nThe latter makes certain assumptions to predict values at unobserved locations.\nWhich assumption may we make?"
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#sea-surface-temperature-anomalies",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#sea-surface-temperature-anomalies",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Sea Surface Temperature Anomalies",
    "text": "Sea Surface Temperature Anomalies\nLet’s consider a data set similar to Problem Class 5:\n\n\n\n\n\n\n\n\nWhat do we observe?"
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#spatial-dependence-1",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#spatial-dependence-1",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Spatial dependence",
    "text": "Spatial dependence\nLocations which are spatially close appear to have similar values.\nThis suggests that observations are dependent!\nToday we introduce two techniques for exploring this spatial dependence."
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#mathematical-definition",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#mathematical-definition",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Mathematical definition",
    "text": "Mathematical definition\nLet \\(X(\\mathbf{s})\\) denote random variable at \\(\\mathbf{s}\\in\\mathcal{S}\\).\nWe assume that \\(E[X(\\mathbf{s})]\\) is constant. Is this reasonable for our data?\nWe measure dependence between \\(X(\\mathbf{s})\\) and \\(X(\\tilde{\\mathbf{s}})\\) via the semi-variogram [ (,) = . ]"
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#properties",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#properties",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Properties",
    "text": "Properties\nThe semi-variogram satisfies\n\n\\(\\gamma(\\mathbf{s},\\tilde{\\mathbf{s}})\\geq 0\\) and \\(\\gamma(\\mathbf{s},\\mathbf{s})=0\\).\nFor \\(X(\\mathbf{s})\\) and \\(X(\\tilde{\\mathbf{s}})\\) i.i.d., \\(\\gamma(\\mathbf{s},\\tilde{\\mathbf{s}})=\\mathrm{Var}[X(\\mathbf{s})].\\)"
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#estimation-i",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#estimation-i",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Estimation I",
    "text": "Estimation I\nWhy can we not easily estimate \\(\\gamma(\\mathbf{s},\\tilde{\\mathbf{s}})\\)?\nTo estimate \\(\\gamma(\\mathbf{s},\\tilde{\\mathbf{s}})\\), we assume that the spatial random process is \\(X(\\mathbf{s})\\) stationary and isotropic.\nAs such there exists \\(\\tilde{\\gamma}:\\mathbb{R}_+\\to\\mathbb{R}_+\\) with [ ( , ) = (||-||), ] where \\(||\\mathbf{s}-\\tilde{\\mathbf{s}}||\\) denotes the distance between the sites."
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#estimation-ii",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#estimation-ii",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Estimation II",
    "text": "Estimation II\nIf our assumptions hold, we can estimate \\(\\tilde{\\gamma}(h)\\), \\(h&gt;0\\), as follows:\n\nFind all pairs of sites with a distance similar to \\(h\\). This gives the set \\(\\mathcal{N}_h =\\{(i,j):||\\mathbf{s}_i - \\mathbf{s}_j||\\approx h\\}\\).\nCalculate the estimate for \\(\\tilde\\gamma(h)\\) as [ (h) = _{i,j _h} (x_i-x_j)^2. ]\n\nWe use the function variogram() in the gstat R package for this.\nRemark: We have to convert the data into a specific format."
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#what-are-we-looking-for",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#what-are-we-looking-for",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "What are we looking for?",
    "text": "What are we looking for?\nIn many applications we find the following features:\n\n\\(\\hat\\gamma(h)\\) increases with increasing distance. This suggests that spatial dependence becomes weaker the further sites are apart.\n\\(\\hat\\gamma(h)\\) levels off at a certain distance.  This marks the point when sites are so far apart that the realizations are close to independent."
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#example",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#example",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Example",
    "text": "Example\nLet’s look at the example on sea surface temperature anomalies\n-&gt; R Markdown file"
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#summary",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#summary",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Summary",
    "text": "Summary\nTo analyse spatial dependence using the semi-variogram, we need to\n\nConsider whether it is reasonable to assume that dependence is fully described by spatial distance and that all sites have the same mean\nEstimate the semi-variogram using the sp and gstat package\nInterpret the estimated semi-variogram\n\nYou will learn more about the mathematical details in Year 3 Statistics."
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#motivation-1",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#motivation-1",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Motivation",
    "text": "Motivation\nLet’s assume we have multiple observations per spatial location.\nWe use \\(x_{i,t}\\) to denote the \\(t\\)-th observation for site \\(i\\).\nHow could we visualize such data?\nIn many applications we want to\n\nUnderstand the spatial structure of the data\nPerform dimension reduction"
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#motivation---data-for-two-sites",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#motivation---data-for-two-sites",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Motivation - Data for two sites",
    "text": "Motivation - Data for two sites"
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#can-we-reduce-it-to-a-single-variable",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#can-we-reduce-it-to-a-single-variable",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Can we reduce it to a single variable?",
    "text": "Can we reduce it to a single variable?"
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#how-does-pca-work",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#how-does-pca-work",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "How does PCA work?",
    "text": "How does PCA work?\n\nCalculate \\(\\tilde{x}_{i,t} = (x_{i,t} - \\bar{x}_i)/\\hat\\sigma_i\\).\nDerive the matrix [ = _{t=1}^T _t _t^{}. ] This matrix is also known as the empirical covariance matrix.\nDerive and study the eigenvalues and eigenvectors of \\(\\Sigma\\), \\(\\Sigma = \\mathbf{UDU}^{\\mathrm{T}}\\)."
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#analysis-of-the-eigenvectors",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#analysis-of-the-eigenvectors",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Analysis of the eigenvectors",
    "text": "Analysis of the eigenvectors\nEach eigenvector provides insight on the spatial structure in the data.\n-&gt; We create plots of the eigenvectors and interpret them sequentially\nDo we need to consider all eigenvectors?\nNo (usually), the eigenvalues will help us with this."
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#analysis-of-the-eigenvalues",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#analysis-of-the-eigenvalues",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Analysis of the eigenvalues",
    "text": "Analysis of the eigenvalues\nWe have eigenvalues \\(\\lambda_1 &gt; \\lambda_2 &gt; \\cdots &gt; \\lambda_n\\)\nConsider the ratio [ . ]\nThe smallest \\(m\\) for which the ratio exceeds 0.9 gives the number of eigenvectors we should plot.\nThis is one of several rules-of-thumb, but you only need to know this one."
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#example-1",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#example-1",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Example",
    "text": "Example\nLet’s analyse rainfall data for sites across the US state Colorado.\nWe have\n\n\\(n=30\\) cities\nMonthly amount of precipitation across 2010-2023. So we have \\(T=168\\) observations per site.\n\n-&gt; R Markdown file"
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#summary-1",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#summary-1",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Summary",
    "text": "Summary\nWhile PCA may seem like a black box, it is very powerful method and has many applications beyond spatial data analysis.\nThe key steps are\n\nApply the prcomp() function to the matrix of observations\nStudy the eigenvalues to determine how many eienvectors need to be considered\nVisualize the eigenvectors and make conclusions about the spatial structure in the data\n\nWhen does PCA work? -&gt; Linearity of variables"
  },
  {
    "objectID": "slides/week_8/spatial_data_analysis_(part_2).html#looking-ahead",
    "href": "slides/week_8/spatial_data_analysis_(part_2).html#looking-ahead",
    "title": "MA22019 - Spatial Data Analysis (Part 2)",
    "section": "Looking ahead",
    "text": "Looking ahead\n\nWe will conclude the content next week (Week 9).\nThere will be a problem sheet which will be considered in the tutorial after Easter (Tuesday in Week 10).\nThe lecture in Week 10 will be a revision lecture. Look out for an announcement on Moodle!\nCoursework 2 will be released on the Friday in Week 10"
  },
  {
    "objectID": "slides/week_9/Tornadoes.html",
    "href": "slides/week_9/Tornadoes.html",
    "title": "Analysis of Tornado Data",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(sf)\nlibrary(ggspatial)\nlibrary(prettymapr)\nlibrary(spatstat)\n\nTornadoes &lt;- read.csv(\"tornadoes.csv\" )"
  },
  {
    "objectID": "slides/week_9/Tornadoes.html#loading-the-packages-and-the-tornado-data-set",
    "href": "slides/week_9/Tornadoes.html#loading-the-packages-and-the-tornado-data-set",
    "title": "Analysis of Tornado Data",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(sf)\nlibrary(ggspatial)\nlibrary(prettymapr)\nlibrary(spatstat)\n\nTornadoes &lt;- read.csv(\"tornadoes.csv\" )"
  },
  {
    "objectID": "slides/week_9/Tornadoes.html#visualize-the-data",
    "href": "slides/week_9/Tornadoes.html#visualize-the-data",
    "title": "Analysis of Tornado Data",
    "section": "Visualize the data",
    "text": "Visualize the data\n\nggplot( Tornadoes, aes( Lon, y=Lat ) ) + annotation_map_tile() + \n  geom_spatial_point() + labs( x=\"Longitude\", y=\"Latitude\" )\n\nAssuming `crs = 4326` in stat_spatial_identity()\n\n\nZoom: 3\n\n\n\n\n\n\n\n\nLoad the shapefile for the United States\n\nUSA &lt;- read_sf( \"Shapefile USA/gadm41_USA_1.shp\" )\n\nExtract the shapefiles for Kansas and Texas\n\nKS &lt;- filter( USA, NAME_1==\"Kansas\" ) %&gt;% \n  st_simplify( dTolerance = 2000 )\n\nTX &lt;- filter( USA, NAME_1==\"Texas\" ) %&gt;% \n  st_simplify( dTolerance = 2000 ) \n\nTransform the points and shapefiles to a different projection\nWe transform the shapefiles and points to a projection different to WGS84 - this seems odd, but it is necessary to create the ppp object later:\n\nKS &lt;- KS %&gt;% st_transform( crs=3857 )\nTX &lt;- TX %&gt;% st_transform( crs=3857 )\n\nTornadoes_transformed &lt;- Tornadoes %&gt;%\n  st_as_sf( coords=c(\"Lon\",\"Lat\"), crs=4326 ) %&gt;%\n  st_transform( crs=3857 ) %&gt;% \n  st_coordinates( )\n\nCreate the ppp objects\nThis is the second possibility of creating a ppp objetc we will introduce - it’s more work due to the required change in projection, but it is more rigorous in return:\n\nTexas_ppp &lt;- ppp( Tornadoes_transformed[,1], Tornadoes_transformed[,2], \n                  window = as.owin(TX) )\nKansas_ppp &lt;- ppp( Tornadoes_transformed[,1], Tornadoes_transformed[,2],\n                   window = as.owin(KS) )\n\nEstimate and visualize the kernel smoothed intensity function\n\npar( mai=c(0.01,0.5,0.2,0.5), mfrow=c(1,2) )\nlambdaC_KS &lt;- density.ppp( Kansas_ppp, edge=TRUE )\nplot( lambdaC_KS, main=\"Kansas\" )\nlambdaC_TX &lt;- density.ppp( Texas_ppp, edge=TRUE )\nplot( lambdaC_TX, main=\"Texas\" )\n\n\n\nCorrected smoothed kernel intensity for the occurrence of tornadoes across Kansas and Texas."
  },
  {
    "objectID": "slides/week_9/Tornadoes.html#extract-number-of-tornadoes-per-state",
    "href": "slides/week_9/Tornadoes.html#extract-number-of-tornadoes-per-state",
    "title": "Analysis of Tornado Data",
    "section": "Extract number of tornadoes per state",
    "text": "Extract number of tornadoes per state\n\nTornadoesNumbers &lt;- Tornadoes %&gt;% count( State )"
  },
  {
    "objectID": "slides/week_9/Tornadoes.html#combine-the-number-with-the-data-frame-for-the-shapefile",
    "href": "slides/week_9/Tornadoes.html#combine-the-number-with-the-data-frame-for-the-shapefile",
    "title": "Analysis of Tornado Data",
    "section": "Combine the number with the data frame for the shapefile",
    "text": "Combine the number with the data frame for the shapefile\n\nUSA &lt;- st_simplify( USA, dTolerance = 5000, preserveTopology = TRUE )\nUSA$HASC_1 &lt;- gsub( \".*\\\\.\", \"\", USA$HASC_1)\nUSA &lt;- inner_join( USA, TornadoesNumbers, by=c(\"HASC_1\"=\"State\") )"
  },
  {
    "objectID": "slides/week_9/Tornadoes.html#remove-alaska-and-hawaii",
    "href": "slides/week_9/Tornadoes.html#remove-alaska-and-hawaii",
    "title": "Analysis of Tornado Data",
    "section": "Remove Alaska and Hawaii",
    "text": "Remove Alaska and Hawaii\n\nUSA &lt;- USA %&gt;% \n  filter( !NAME_1 %in% c(\"Alaska\", \"Hawaii\") ) %&gt;%\n  rename( State = NAME_1, Number=n )"
  },
  {
    "objectID": "slides/week_9/Tornadoes.html#create-the-plot",
    "href": "slides/week_9/Tornadoes.html#create-the-plot",
    "title": "Analysis of Tornado Data",
    "section": "Create the plot",
    "text": "Create the plot\n\nggplot( USA, aes(fill=Number) ) + theme_bw() + \n  annotation_map_tile( zoom=4 ) + geom_sf() +\n  scale_fill_distiller( palette=\"Reds\", trans=\"reverse\" ) + \n  labs( fill=\"Number\", x=\"Longitude\", y=\"Latitude\" ) +\n  theme( axis.title=element_text(size=15), axis.text=element_text(size=15) )"
  },
  {
    "objectID": "slides/week_9/WildFires.html",
    "href": "slides/week_9/WildFires.html",
    "title": "Analysis of Wildfires",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(sf)\nlibrary(ggspatial)\nlibrary(prettymapr)\nlibrary(spatstat)\n\nWildFires &lt;- read.csv(\"wildfires.csv\" )\n\n\nglimpse(WildFires)\n\nRows: 1,465\nColumns: 3\n$ AcresBurned &lt;int&gt; 257314, 30274, 27531, 27440, 22992, 20292, 14754, 12503, 1…\n$ Latitude    &lt;dbl&gt; 37.85700, 34.58559, 33.70950, 39.12000, 37.27900, 33.86157…\n$ Longitude   &lt;dbl&gt; -120.0860, -118.4232, -116.7288, -120.6500, -119.3180, -11…"
  },
  {
    "objectID": "slides/week_9/WildFires.html#loading-the-packages-and-the-californian-wildfire-data",
    "href": "slides/week_9/WildFires.html#loading-the-packages-and-the-californian-wildfire-data",
    "title": "Analysis of Wildfires",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(sf)\nlibrary(ggspatial)\nlibrary(prettymapr)\nlibrary(spatstat)\n\nWildFires &lt;- read.csv(\"wildfires.csv\" )\n\n\nglimpse(WildFires)\n\nRows: 1,465\nColumns: 3\n$ AcresBurned &lt;int&gt; 257314, 30274, 27531, 27440, 22992, 20292, 14754, 12503, 1…\n$ Latitude    &lt;dbl&gt; 37.85700, 34.58559, 33.70950, 39.12000, 37.27900, 33.86157…\n$ Longitude   &lt;dbl&gt; -120.0860, -118.4232, -116.7288, -120.6500, -119.3180, -11…"
  },
  {
    "objectID": "slides/week_9/WildFires.html#extracting-the-shapefile-for-california",
    "href": "slides/week_9/WildFires.html#extracting-the-shapefile-for-california",
    "title": "Analysis of Wildfires",
    "section": "Extracting the shapefile for California",
    "text": "Extracting the shapefile for California\nA shapefile is imported in the form of a data frame and thus we can apply data wrangling techniques to it:\n\nUSA &lt;- read_sf( \"Shapefile USA/gadm41_USA_1.shp\" )\nCalifornia &lt;- filter( USA, NAME_1==\"California\" ) %&gt;% \n  st_simplify(dTolerance = 2000)\n\n\nggplot(California) + geom_sf()"
  },
  {
    "objectID": "slides/week_9/WildFires.html#visualize-the-data",
    "href": "slides/week_9/WildFires.html#visualize-the-data",
    "title": "Analysis of Wildfires",
    "section": "Visualize the data",
    "text": "Visualize the data\nWe can use either of the two techniques we introduced for visualizing point-referenced data.\n\nshape &lt;- ggplot( data=California ) + theme_bw() + geom_sf() + \n  geom_point( data=WildFires, aes(x=Longitude,y=Latitude, color=log(AcresBurned) ) ) +\n  scale_color_distiller( palette=\"Reds\", trans=\"reverse\" ) +\n  theme( legend.position=\"none\" )\nmap &lt;- ggplot( WildFires, aes( Longitude, y=Latitude ) ) + \n  annotation_map_tile() + geom_spatial_point( aes(color=log(AcresBurned)) ) + \n  scale_color_distiller( palette=\"Reds\", trans=\"reverse\" ) +\n  labs( x=\"Longitude\", y=\"Latitude\", color=\"Log( Acres burned)\" )\nshape + map"
  },
  {
    "objectID": "slides/week_9/WildFires.html#quadrat-counting-estimate",
    "href": "slides/week_9/WildFires.html#quadrat-counting-estimate",
    "title": "Analysis of Wildfires",
    "section": "Quadrat counting estimate",
    "text": "Quadrat counting estimate\nWe need to generate a ppp object and one option to do this is:\n\nWildFires_ppp &lt;- ppp( WildFires$Longitude, WildFires$Latitude,\n                      poly = California$geometry[[1]][[1]] )\n\nWarning: 7 points were rejected as lying outside the specified window\n\n\nWarning: data contain duplicated points\n\n\nWe now use the quadratcount() function :\n\npar( mfrow=c(1,2), mai=c(0.01,0.01,0.5,0.01) )\nWildFireQuadrants &lt;- quadratcount( WildFires_ppp, nx=2, ny=2 )\nplot( WildFireQuadrants )\nWildFireQuadrants &lt;- quadratcount( WildFires_ppp, nx=6, ny=6 )\nplot( WildFireQuadrants )"
  },
  {
    "objectID": "slides/week_9/WildFires.html#kernel-smoothed-intensity-function",
    "href": "slides/week_9/WildFires.html#kernel-smoothed-intensity-function",
    "title": "Analysis of Wildfires",
    "section": "Kernel smoothed intensity function",
    "text": "Kernel smoothed intensity function\nLet’s compare the estimate with and without edge correction:\n\npar( mfrow=c(1,2), mai=c(0.01,0.5,0.2,0.5) )\nlambdaK &lt;- density.ppp( WildFires_ppp, edge=FALSE )\nplot( lambdaK, main=\"Uncorrected\" )\nlambdaC &lt;- density.ppp( WildFires_ppp, edge=TRUE )\nplot( lambdaC, main=\"Corrected\" )\n\n\n\nUncorrected (left) and corrected (right) kernel smoothed intensity for Californian wildfires."
  },
  {
    "objectID": "slides/week_9/WildFires.html#selection-of-the-bandwidth",
    "href": "slides/week_9/WildFires.html#selection-of-the-bandwidth",
    "title": "Analysis of Wildfires",
    "section": "Selection of the bandwidth",
    "text": "Selection of the bandwidth\nIn some cases we may to choose the parameter \\(\\sigma\\). Here are two poor choices which illustrate the effect of the parameter:\n\npar( mai=c(0.01,0.5,0.2,0.5), mfrow=c(1,2) )\nlambda_sigma0.1 &lt;- density.ppp( WildFires_ppp, edge=TRUE, sigma = 0.1 )\nplot( lambda_sigma0.1, main=\"sigma=0.1\" )\nlambda_sigma10 &lt;- density.ppp( WildFires_ppp, edge=TRUE, sigma = 10 )\nplot( lambda_sigma10, main=\"sigma=10\" )\n\n\n\nCorrected kernel smoothed intensity estimates for Californian wildfires with sigma=0.1 (left) and sigma=10 (right)."
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "",
    "text": "We start the final chapter on spatial data analysis:\n\nTypes of spatial data\nVisualization of point-referenced data (Section 4.1)\nProblem Class 5\nInverse Distance Weighting (Section 4.2)\nComments on Coursework 1"
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#plan-for-today",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#plan-for-today",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "",
    "text": "We start the final chapter on spatial data analysis:\n\nTypes of spatial data\nVisualization of point-referenced data (Section 4.1)\nProblem Class 5\nInverse Distance Weighting (Section 4.2)\nComments on Coursework 1"
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#motivation",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#motivation",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Motivation",
    "text": "Motivation\nIn many applications we have spatial information:\n\nAir pollution modelling\nDisease outbreaks\nEcological studies\nSocioeconomic studies\nWeather data\n\nHow can we use this information in a data analysis?"
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#types-of-spatial-data-i",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#types-of-spatial-data-i",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Types of Spatial Data I",
    "text": "Types of Spatial Data I\nIn spatial data analysis we consider three types of spatial data:\nPoint-referenced data\n\nData are observed across a number of fixed sites\nMeteorology and mining are text book examples for this type of data"
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#types-of-spatial-data-ii",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#types-of-spatial-data-ii",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Types of Spatial Data II",
    "text": "Types of Spatial Data II\nPoint-pattern data\n\nSpatial locations are random variables\nOften found when analyzing disease outbreaks and in ecological studies\n\nLattice / Areal unit data\n\nOur sites are fixed areas (countries, counties, etc.)\nExamples include insurance claims and image data"
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#motivation-1",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#motivation-1",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Motivation",
    "text": "Motivation\nLet’s look at some temperature data for Germany\n-&gt; R Markdown file\nJust using ggplot2 does not really provide a useful plot.\nWould a map provide additional information?"
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#shapefiles",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#shapefiles",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Shapefiles",
    "text": "Shapefiles\nOne option is to add the boundaries of Germany.\nSuch data is available via the website gadm.org.\nLet’s see how we can add such shapefile data to our plot \n-&gt; R Markdown file"
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#projections",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#projections",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Projections",
    "text": "Projections\nWe used latitude and longitude within a Cartesian coordinate system.\nHowever, the earth is not flat!\nConverting locations on earth to a two-dimensional representation is called a projection.\nWe can choose a projection using the coord_sf() function.\n-&gt; R Markdown file"
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#example",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#example",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#remarks-about-projections",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#remarks-about-projections",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Remarks about projections",
    "text": "Remarks about projections\n\nWGS84 coordinate reference system is the standard for GPS systems and Google Earth and has st_crs(4326)\nThe projection may have a influence on how the plot is perceived\nSome projections are better for some parts of the world than for others\nFor small areas we usually don’t need to consider projections.\nWhen using projections, we also need to project the points\n\n-&gt; R Markdown file"
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#creating-maps-with-ggspatial",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#creating-maps-with-ggspatial",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Creating maps with ggspatial",
    "text": "Creating maps with ggspatial\nIt may be better to place points on a (colourful) map.\nThis will be achieved by importing OpenStreetMap data using the package ggspatial()\n\nlibrary(ggspatial)\nlibrary(prettymapr)\n\nHow to make it work\n\nAdd the function “annotation_map_tile()”\nReplace geom_point() by geom_spatial_point()\n\n-&gt; R Markdown file"
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#exercise",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#exercise",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Exercise",
    "text": "Exercise\nPlease go to Moodle and download the materials for Problem Class 5."
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#motivation-2",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#motivation-2",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Motivation",
    "text": "Motivation\n\n\n\n\n\n\n\n\nWhat is the temperature at an unobserved location?"
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#notation",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#notation",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Notation",
    "text": "Notation\nWe introduce some notation to define our approach mathematically:\n\n\\(n\\) observations \\(x_1,\\ldots,x_n\\in\\mathbb{R}\\)\nlocations \\(\\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\in\\mathbb{R}^2\\)\n\\(\\mathbf{s}^*\\) is the unobserved location for which we want to derive a prediction \\(x^*\\)\n\nHow could we use the data and location information to get a prediction?"
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#inverse-distance-weighting-1",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#inverse-distance-weighting-1",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Inverse Distance Weighting",
    "text": "Inverse Distance Weighting\nLet \\(d: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}_+\\) be a distance metric, such as [ d(_1, _2) = ||_1-_2||_2= . ]\nInverse distance weighting defines [ x^* =\n\\[\\begin{cases}\n\\dfrac{\\sum_{i=1}^n w(\\mathbf{s}_i,\\mathbf{s}^*) x_i}{\\sum_{i=1}^n, w(\\mathbf{s}_i,\\mathbf{s}^*)} & \\mbox{if}~d(\\mathbf{s}^*, \\mathbf{s}_i)&gt;0,\\\\\nx_i & \\mathrm{if}~d(\\mathbf{s}^*, \\mathbf{s}_i)=0,\n\\end{cases}\\]\n] where \\(w(\\mathbf{s}_i,\\mathbf{s}^*) = \\left[d(\\mathbf{s}_i,\\mathbf{s}^*)\\right]^{-p}\\) and \\(p&gt;0\\) is called the power parameter."
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#effect-of-the-power-parameter",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#effect-of-the-power-parameter",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Effect of the power parameter",
    "text": "Effect of the power parameter\nWe have to select a suitable value for \\(p\\).\nExample: Suppose \\(\\mathbf{s}^* = (13.1, 51.0)\\) for the German temperature data.\nLet’s compare \\(p=0.5\\), \\(p=2\\) and \\(p=20\\).\n-&gt; R Markdown file\nIt may be useful to create a map of predicted values to:\n\nStudy spatial pattern of the values\nSelect a suitable value for \\(p\\)"
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#remarks",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#remarks",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Remarks",
    "text": "Remarks\n\nWe only need a reasonable \\(p\\), it does not have to optimal\n\nThe Euclidian distance for \\(d(\\mathbf{s}_1,\\mathbf{s}_2)\\) is not necessarily the best choice\n\n\nExample: We ignored the aspect of altitude in our calculations\n\n\nBe careful when making predictions for locations that are not close to at least one location in \\(\\{\\mathbf{s}_1, \\ldots,\\mathbf{s}_n\\}\\)"
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#summary",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#summary",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Summary",
    "text": "Summary\nToday we learned that\n\nSpatial data occurs in several applications\nThere are three types of spatial data\nWe can use shapefiles and maps to visualize the data\nWhen studying large areas, we have to consider the projection\nInverse distance weighting may be used for predicting values at unobserved locations"
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#summary-1",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#summary-1",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Summary",
    "text": "Summary\nAverage: 58.7%\nOffice Hour for questions on the coursework:\n\nThursday (tomorrow): 12:30-15:00\nMonday: 9:00-10:15 and 16:00-18:00\n\nAdvice for Coursework 2:\n\nDon’t suppress the R code for loading packages\nMake use of R Markdown options to modify the size of plots.\nClearly describe your approach."
  },
  {
    "objectID": "slides/week_7/spatial_data_analysis_(part_1).html#reminder---what-does-it-mean-to-be-a-data-scientist",
    "href": "slides/week_7/spatial_data_analysis_(part_1).html#reminder---what-does-it-mean-to-be-a-data-scientist",
    "title": "MA22019 - Spatial Data Analysis (Part 1)",
    "section": "Reminder - What does it mean to be a data scientist",
    "text": "Reminder - What does it mean to be a data scientist"
  },
  {
    "objectID": "computing_setup/why_quarto.html",
    "href": "computing_setup/why_quarto.html",
    "title": "Why we use Quarto",
    "section": "",
    "text": "In this course, we have transitioned from RMarkdown to Quarto. You might be wondering: “Why learn a new tool when RMarkdown already exists?” or “Is this relevant to my future career?”\nThe short answer is: Yes, Quarto is a massive upgrade for your data science toolkit. Here is why.",
    "crumbs": [
      "Computing",
      "Why Quarto?"
    ]
  },
  {
    "objectID": "computing_setup/why_quarto.html#it-is-language-agnostic-the-biggest-career-advantage",
    "href": "computing_setup/why_quarto.html#it-is-language-agnostic-the-biggest-career-advantage",
    "title": "Why we use Quarto",
    "section": "1. It is “Language Agnostic” (The Biggest Career Advantage)",
    "text": "1. It is “Language Agnostic” (The Biggest Career Advantage)\nRMarkdown is tied heavily to R. However, in the modern tech industry, data teams rarely use just one language. You will likely work on teams where:\n\nSome people use Python (pandas, scikit-learn).\nOthers use R (tidyverse, ggplot2).\nOthers use Julia or Observable JS.\n\nQuarto works natively with all of them.\nThis allows the entire team to use the same publishing system regardless of their preferred coding language. Being able to document Python projects with the same tool you use for R is a massive skills multiplier that makes you a more versatile and valuable asset to any data team.",
    "crumbs": [
      "Computing",
      "Why Quarto?"
    ]
  },
  {
    "objectID": "computing_setup/why_quarto.html#it-is-the-next-generation-standard",
    "href": "computing_setup/why_quarto.html#it-is-the-next-generation-standard",
    "title": "Why we use Quarto",
    "section": "2. It is the “Next Generation” Standard",
    "text": "2. It is the “Next Generation” Standard\nQuarto is built by Posit (formerly RStudio), the same team that created RMarkdown. They have explicitly stated that Quarto is the successor.\n\nWhile RMarkdown will be supported for maintenance, all new features and innovations are going into Quarto.\nLearning Quarto now puts you ahead of the curve.\nSticking with RMarkdown means learning “legacy” technology that will slowly fade away.",
    "crumbs": [
      "Computing",
      "Why Quarto?"
    ]
  },
  {
    "objectID": "computing_setup/why_quarto.html#better-tooling-ide-support",
    "href": "computing_setup/why_quarto.html#better-tooling-ide-support",
    "title": "Why we use Quarto",
    "section": "3. Better Tooling & IDE Support",
    "text": "3. Better Tooling & IDE Support\n\nRMarkdown is great in RStudio but can be clunky in other editors.\nQuarto is designed to work perfectly in VS Code, Jupyter Lab, and standard text editors.\n\nMany professional data scientists prefer VS Code over RStudio (especially for Python, cloud computing, or software engineering work). Knowing a publishing tool that works smoothly in the industry-standard editor (VS Code) is highly valuable.",
    "crumbs": [
      "Computing",
      "Why Quarto?"
    ]
  },
  {
    "objectID": "computing_setup/why_quarto.html#direct-skill-transfer",
    "href": "computing_setup/why_quarto.html#direct-skill-transfer",
    "title": "Why we use Quarto",
    "section": "4. Direct Skill Transfer",
    "text": "4. Direct Skill Transfer\nThe best news is that 95% of RMarkdown syntax works in Quarto. You aren’t “re-learning” from scratch; you are essentially upgrading to “RMarkdown 2.0”.\nThe transition is easy, but the ceiling for what you can build—websites, blogs, interactive dashboards, and books—is much higher with Quarto.\n\nVerdict: Teaching you Quarto is preparing you for the real-world, polyglot (multi-language) data science environment that is the standard in modern tech companies.",
    "crumbs": [
      "Computing",
      "Why Quarto?"
    ]
  },
  {
    "objectID": "computing_setup/project_structure.html",
    "href": "computing_setup/project_structure.html",
    "title": "Project Structure & Naming",
    "section": "",
    "text": "As a Data Scientist, you will generate hundreds of files. If you name them poorly, you will lose them.\n\n\n\n\n\n\nTipThe 3 Principles\n\n\n\n\nMachine Readable: No spaces, punctuation, or accents. Use _ or -.\nHuman Readable: Meaningful names. analysis.R is bad; 01_data_cleaning.R is good.\nSortable: Use ISO dates (YYYY-MM-DD) and numbered prefixes (01_, 02_).\n\n\n\n\n\n\n\n\n\n\n\n\nBad ❌\nGood ✅\n\n\n\n\nmy abstract.docx\n2025-02-15_abstract.docx\n\n\nJoe's Filenames Use Spaces and Punctuation.xlsx\njoes_filenames_are_constistent.csv\n\n\nfigure 1.png\nfig01_scatterplot_height_weight.png\n\n\nfinal_final_v2.R\n04_model_evaluation.R",
    "crumbs": [
      "Computing",
      "Project Structure"
    ]
  },
  {
    "objectID": "computing_setup/project_structure.html#the-golden-rules-of-naming",
    "href": "computing_setup/project_structure.html#the-golden-rules-of-naming",
    "title": "Project Structure & Naming",
    "section": "",
    "text": "As a Data Scientist, you will generate hundreds of files. If you name them poorly, you will lose them.\n\n\n\n\n\n\nTipThe 3 Principles\n\n\n\n\nMachine Readable: No spaces, punctuation, or accents. Use _ or -.\nHuman Readable: Meaningful names. analysis.R is bad; 01_data_cleaning.R is good.\nSortable: Use ISO dates (YYYY-MM-DD) and numbered prefixes (01_, 02_).\n\n\n\n\n\n\n\n\n\n\n\n\nBad ❌\nGood ✅\n\n\n\n\nmy abstract.docx\n2025-02-15_abstract.docx\n\n\nJoe's Filenames Use Spaces and Punctuation.xlsx\njoes_filenames_are_constistent.csv\n\n\nfigure 1.png\nfig01_scatterplot_height_weight.png\n\n\nfinal_final_v2.R\n04_model_evaluation.R",
    "crumbs": [
      "Computing",
      "Project Structure"
    ]
  },
  {
    "objectID": "computing_setup/project_structure.html#organizing-your-workspace",
    "href": "computing_setup/project_structure.html#organizing-your-workspace",
    "title": "Project Structure & Naming",
    "section": "2. Organizing Your Workspace",
    "text": "2. Organizing Your Workspace\nWhether you are using a university PC (H: Drive) or your own laptop, you need a dedicated home for this course.\nWhere should you put it? * Uni PC: H:/MA22019/ * Windows (Personal): Documents/MA22019/ * Mac/Linux: ~/Documents/MA22019/\n\n\n\n\n\n\nWarning⚠️ WARNING: Is your “Documents” folder synced?\n\n\n\nOn many Macs (iCloud) and Windows PCs (OneDrive), the Documents folder is automatically synced to the cloud. Git + Cloud (iCloud/OneDrive) = Data Loss.\nCheck this: 1. Mac: System Settings &gt; Apple ID &gt; iCloud &gt; iCloud Drive &gt; Options &gt; Desktop & Documents Folders. If this is ON, do not save your work in Documents. Create a folder in your User Home directory instead (e.g., /Users/yourname/MA22019). 2. Windows: Check if your Documents has a green tick (OneDrive). If so, make a folder directly on your C: drive (e.g., C:\\MA22019) to be safe.\n\n\n\nRecommended Folder Structure\nWe recommend the following structure to handle the three types of activities in this course:\nMA22019/\n├── 01_Labs/                  # Weekly Computer Labs (Formative)\n│   ├── Week_01_Lab/\n│   │   └── Lab_01.Rproj\n│   ├── Week_02_Lab/\n│   └── ...\n│\n├── 02_Homework/              # Weekly Homework Questions (Formative)\n│   ├── Week_01_HW/\n│   │   └── HW_01.Rproj\n│   └── ...\n│\n├── 03_Coursework/            # Assessed Summative Work (Graded)\n│   ├── CW_1_Wrangling/       # Coursework 1 Repository (Assessed)\n│   └── CW_2_Project/         # Coursework 2 Repository (Assessed)\n│\n└── 04_Resources/             # Lecture notes, cheat sheets\n    ├── ggplot2_cheatsheet.pdf\n    └── ...\n\n\nThe Three Activity Types\n\nLabs (Formative): These are the exercises we do together in the computer labs. Great for practice.\nHomework (Formative): Weekly open-ended questions. These help you verify your understanding.\nCoursework (Summative): These are the two major assessed pieces of work. These MUST be Git repositories (see Assignment Workflow). Keep them in 03_Coursework so you don’t accidentally mix them up with your weekly practice files.",
    "crumbs": [
      "Computing",
      "Project Structure"
    ]
  },
  {
    "objectID": "computing_setup/project_structure.html#inside-a-project-the-tidy-project",
    "href": "computing_setup/project_structure.html#inside-a-project-the-tidy-project",
    "title": "Project Structure & Naming",
    "section": "3. Inside a Project (The “Tidy” Project)",
    "text": "3. Inside a Project (The “Tidy” Project)\nEvery time you start a new RStudio Project (e.g., for a Lab or Coursework), use this internal structure:\nmy_project/\n├── my_project.Rproj    # RStudio Project file (NEVER delete this)\n├── README.md           # Explanation of what this project does\n├── data/               # Raw and processed data\n│   ├── raw/\n│   └── processed/\n├── R/                  # R scripts (if you have complex functions)\n├── output/             # Plots, tables, and saved results\n└── analysis.qmd        # Your main report/analysis\n\nWhy this matters?\n\nRelative Paths: You can load data with read.csv( \"data/raw/my_data.csv\" ). This works on your computer, your friend’s computer, and the marker’s computer.\nClean Workspace: output/ keeps your generated files (which you can delete and regenerate) separate from your source code.\nSanity: You know exactly where to find the raw data vs. the cleaned data.",
    "crumbs": [
      "Computing",
      "Project Structure"
    ]
  },
  {
    "objectID": "computing_setup/computing_assignments.html",
    "href": "computing_setup/computing_assignments.html",
    "title": "Assignment Workflow",
    "section": "",
    "text": "In this course, you will submit all assignments through GitHub. This mirrors real-world data science workflows where version control is essential.\nWe streamline this process using GitHub Classroom and RStudio’s built-in tools. You essentially never need to open the terminal.\n\n\n\n\n\nflowchart LR\n    subgraph Course GitHub Organization\n        A[📁 Assignment Template]\n    end\n    \n    subgraph Your GitHub Account\n        B[📁 Your Copy]\n    end\n    \n    subgraph Your Computer\n        C[📁 Local Clone]\n    end\n    \n    A --&gt;|1. Fork/Accept| B\n    B --&gt;|2. Clone| C\n    C --&gt;|3. Work & Commit| C\n    C --&gt;|4. Push| B\n    B --&gt;|5. Submission!| B\n    \n    style A fill:#202329,color:#fff\n    style B fill:#EC0836,color:#fff\n    style C fill:#14C8E1,color:#000",
    "crumbs": [
      "Computing",
      "Assignments Workflow"
    ]
  },
  {
    "objectID": "computing_setup/computing_assignments.html#how-assignment-submission-works",
    "href": "computing_setup/computing_assignments.html#how-assignment-submission-works",
    "title": "Assignment Workflow",
    "section": "",
    "text": "In this course, you will submit all assignments through GitHub. This mirrors real-world data science workflows where version control is essential.\nWe streamline this process using GitHub Classroom and RStudio’s built-in tools. You essentially never need to open the terminal.\n\n\n\n\n\nflowchart LR\n    subgraph Course GitHub Organization\n        A[📁 Assignment Template]\n    end\n    \n    subgraph Your GitHub Account\n        B[📁 Your Copy]\n    end\n    \n    subgraph Your Computer\n        C[📁 Local Clone]\n    end\n    \n    A --&gt;|1. Fork/Accept| B\n    B --&gt;|2. Clone| C\n    C --&gt;|3. Work & Commit| C\n    C --&gt;|4. Push| B\n    B --&gt;|5. Submission!| B\n    \n    style A fill:#202329,color:#fff\n    style B fill:#EC0836,color:#fff\n    style C fill:#14C8E1,color:#000",
    "crumbs": [
      "Computing",
      "Assignments Workflow"
    ]
  },
  {
    "objectID": "computing_setup/computing_assignments.html#step-by-step-guide",
    "href": "computing_setup/computing_assignments.html#step-by-step-guide",
    "title": "Assignment Workflow",
    "section": "Step-by-Step Guide",
    "text": "Step-by-Step Guide\n\nStep 1: Accept the Assignment\n\nClick the GitHub Classroom link provided on Moodle/Course Page.\nGitHub will automatically create a private repository for you (e.g., assignment-1-yourname).\nCopy the URL of this new repository from your browser address bar.\n\nIt should look like: https://github.com/MA22019/assignment-1-yourname\n\n\n\n\nStep 2: Clone into RStudio\nDo not download a ZIP file. We will connect RStudio directly to the cloud.\n\n\n\n\n\n\nImportantWHERE TO SAVE? (Critical)\n\n\n\nDo NOT save this project in OneDrive, Dropbox, or Google Drive. Cloud sync services corrupt Git repositories.\nRecommendation: Use your University H: Drive (Network mapped drive). 👉 Read the H: Drive Setup/Map Guide\n\n\n\nOpen RStudio.\nGo to File &gt; New Project...\nChoose Version Control (the 3rd option).\nChoose Git (the 1st option).\nPaste the Repository URL you copied in Step 1.\nClick Create Project.\n\nResult: RStudio will download your assignment files and open them automatically.\n\n\nStep 3: The “Edit - Commit” Loop\nYou should save your work to the “cloud” (GitHub) frequently. Think of this as a super-charged “Save” button.\nThe Workflow:\n\nEdit: Work on your .qmd file. Write code, analysis, and answers.\nRender: Click “Render” to ensure your code runs and produces the HTML output.\nStage (The Checkbox):\n\nGo to the Git Pane in RStudio (usually top-right).\nCheck the Staged box next to the files you changed (e.g., assignment.qmd).\n\nCommit (The Snapshot):\n\nClick the Commit button in the Git Pane.\nWrite a message in the box: “Completed Question 1 data cleaning”.\nClick Commit.\n\n\n\n\n\n\n\n\nImportantCommit at Least 3 Times!\n\n\n\nWe expect to see a progression of work in your commits. This helps us: - Verify the work is yours - Give partial credit if something breaks - Provide better feedback\n\n\n\n\nStep 4: Submit (Push)\nYour “Commits” are currently only saved on your local computer. To submit them to us, you must Push.\n\nIn the Git Pane, look for the Green Up Arrow (Push).\nClick it.\nWait for the message: “HEAD -&gt; main… set up to track remote branch…”\n\nVerification: Go back to your GitHub repository page in your browser and refresh. You should see your latest commit message and updated files.",
    "crumbs": [
      "Computing",
      "Assignments Workflow"
    ]
  },
  {
    "objectID": "computing_setup/computing_assignments.html#assignment-repository-structure",
    "href": "computing_setup/computing_assignments.html#assignment-repository-structure",
    "title": "Assignment Workflow",
    "section": "Assignment Repository Structure",
    "text": "Assignment Repository Structure\nEach assignment repository will contain:\nassignment-name/\n├── README.md           # Instructions overview\n├── assignment.qmd      # Main file you'll edit\n├── data/               # Datasets for the assignment\n│   └── dataset.csv\n└── .gitignore          # Files to ignore\n\n\n\n\n\n\nNoteLarge Datasets\n\n\n\nSome assignments require large data files (&gt; 25 MB). These will be available on Moodle. Download and place them in the data/ folder inside your project before running the code.",
    "crumbs": [
      "Computing",
      "Assignments Workflow"
    ]
  },
  {
    "objectID": "computing_setup/computing_assignments.html#what-gets-graded",
    "href": "computing_setup/computing_assignments.html#what-gets-graded",
    "title": "Assignment Workflow",
    "section": "What Gets Graded?",
    "text": "What Gets Graded?\n\n1. Content (80%)\n\n\n\nWhat We Look At\nWhy It Matters\n\n\n\n\nYour .qmd file\nYour code and written answers\n\n\nRendered output\nDoes your code run correctly?\n\n\nAccuracy\nAre your answers correct?\n\n\nCompleteness\nDid you attempt all questions?\n\n\n\n\n\n2. Workflow Points (20%) - “The 3-Commit Rule”\nWe grade your professional habits.\nRequirement: You must commit at least 3 times as you work through the assignment.\n\n\n\n\n\n\n\n\nRequirement\nPoints\nWhat We Check\n\n\n\n\nMinimum 3 commits\n10%\nWe look at your commit history\n\n\nFinal files in repo\n5%\nBoth .qmd and rendered output on GitHub\n\n\nOrganization\n5%\nClean repo, meaningful commit messages\n\n\n\nWhy do we require multiple commits? 1. Proves progression: Shows you worked incrementally, not all at once. 2. Enables recovery: If something breaks, you have earlier working versions. 3. Mirrors industry: Professional developers commit frequently.",
    "crumbs": [
      "Computing",
      "Assignments Workflow"
    ]
  },
  {
    "objectID": "computing_setup/computing_assignments.html#deadlines-and-late-policy",
    "href": "computing_setup/computing_assignments.html#deadlines-and-late-policy",
    "title": "Assignment Workflow",
    "section": "Deadlines and Late Policy",
    "text": "Deadlines and Late Policy\n\n\n\n\n\n\nWarningImportant!\n\n\n\n\nDeadline: The state of your GitHub repository at the deadline time\nLate submissions: 10% penalty per day, up to 3 days\nAfter 3 days: No submissions accepted without prior arrangement\n\n\n\n\nHow to Check Your Submission\n\nGo to github.com\nNavigate to your assignment repository\nVerify:\n\n✅ Your .qmd file is there and up to date\n✅ All files are committed\n✅ Last commit time is before the deadline",
    "crumbs": [
      "Computing",
      "Assignments Workflow"
    ]
  },
  {
    "objectID": "computing_setup/computing_assignments.html#getting-your-own-copy-started",
    "href": "computing_setup/computing_assignments.html#getting-your-own-copy-started",
    "title": "Assignment Workflow",
    "section": "Getting Your Own Copy Started",
    "text": "Getting Your Own Copy Started\nIf you want to work on something not from an assignment invite link:\n\n\n\n\n\nflowchart TD\n    A[Create new repository on GitHub] --&gt; B[Clone to your computer]\n    B --&gt; C[Add your files]\n    C --&gt; D[Commit and push]\n    \n    style A fill:#202329,color:#fff\n    style D fill:#EC0836,color:#fff\n\n\n\n\n\n\n\nCreating a New Repository\n\nGo to github.com/new\nName your repository\nSelect “Add a README file”\nClick “Create repository”\nFollow the Clone steps above to bring it into RStudio.",
    "crumbs": [
      "Computing",
      "Assignments Workflow"
    ]
  },
  {
    "objectID": "computing_setup/computing_assignments.html#collaboration-on-group-projects",
    "href": "computing_setup/computing_assignments.html#collaboration-on-group-projects",
    "title": "Assignment Workflow",
    "section": "Collaboration on Group Projects",
    "text": "Collaboration on Group Projects\nFor group projects, you’ll work together in a shared repository:\n\n\n\n\n\nflowchart TD\n    subgraph Shared Repository\n        A[Main Branch]\n    end\n    \n    subgraph Team Members\n        B[Alice's Computer]\n        C[Bob's Computer]\n        D[Charlie's Computer]\n    end\n    \n    A &lt;--&gt;|push/pull| B\n    A &lt;--&gt;|push/pull| C\n    A &lt;--&gt;|push/pull| D\n    \n    style A fill:#202329,color:#fff\n    style B fill:#EC0836,color:#fff\n    style C fill:#14C8E1,color:#000\n    style D fill:#673AB7,color:#fff\n\n\n\n\n\n\n\nRules for Group Work\n\nPull before you start working: Use the Blue Down Arrow (Pull) in the Git Pane.\nCommunicate: Tell teammates what you’re working on.\nCommit frequently: Smaller commits are easier to merge.\nPush when done: Don’t leave changes on your computer.",
    "crumbs": [
      "Computing",
      "Assignments Workflow"
    ]
  },
  {
    "objectID": "computing_setup/computing_assignments.html#troubleshooting",
    "href": "computing_setup/computing_assignments.html#troubleshooting",
    "title": "Assignment Workflow",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n“I don’t see the Git Pane in RStudio”\n\nDid you skip Step 2? You must create a New Project &gt; Version Control. You cannot just “Open File”.\nTry Tools &gt; Project Options &gt; Git/SVN and ensure “Version control system” is set to “Git”.\n\n\n\n“Push Failed” (Login Errors)\n\nGitHub no longer accepts regular passwords.\nIf RStudio asks for a username/password:\n\nUsername: Your GitHub username.\nPassword: You must use a Personal Access Token (PAT).\nSee GitHub’s official guide for creating a PAT.\n\n\n\n\n“Merge Conflict”\n\nThis rarely happens in individual assignments unless you edit files on two different computers.\nIf you see &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD, ask a TA for help immediately. Don’t panic!",
    "crumbs": [
      "Computing",
      "Assignments Workflow"
    ]
  },
  {
    "objectID": "practice/week_2/quiz_2.html",
    "href": "practice/week_2/quiz_2.html",
    "title": "MA22019 2025 - Quiz 2",
    "section": "",
    "text": "Overview\nExercises 1-3 will help you with revising the techniques covered in the lectures in Week 1 (Sections 1.1 and 1.2 of the lecture notes). You can check your solutions to these questions yourself by entering them on Moodle via the Quiz provided in the “Problem Sheets” section.\nBefore starting the exercises, make sure to load the dplyr and lubridate R packages:\n\nlibrary(dplyr)\nlibrary(lubridate)\n\nExercise 1 - Functions for data wrangling\nWe described that data may not be in the format required for the analysis because\n\nVariable names are uninformative;\nData types are incompatible with available R functions;\nWe are only interested in a subset of the data.\n\nFor each of the following R functions, decide which issue they may address and submit your answers via the Moodle quiz:\n\nas_date()\nas.numeric()\nfilter()\ngroup_by()\nrename()\nslice_head()\n\nIf you are unsure about a function, you can use the help() function in R, or look at the cheat sheets provided on Moodle.\nExercise 2 - Flights from New York City airports\nThe file “NYCFlights.csv” provides data for flights in 2013 that departed from any of the three main airports servicing New York City: John F Kennedy (JFK), LaGuardia (LGA) and Newark (EWR). To load the data into your R Workspace, you can use\n\nNYCFlights &lt;- read.csv(\"data/nycflights.csv\" )\n\nAnswer the following questions using the provided data and submit your answers via the Moodle quiz:\n\nHow many flights does the data set contain?\nFor which month were the most flights recorded?\nWhich plane (specified by the tailnum variable) departed most often from New York City airports in 2013?\nHow often did the plane identified in part c) depart from New York City airports in January?\nExercise 3 - Earthquakes around the world\nThe data set “Earthquakes.csv” contains the location and size of all significant earthquakes, as recorded by the National Earthquake Information Center (NEIC), which occurred worldwide between 1965 and 2016.\nWe can load the data using\n\nEarthquakes &lt;- read.csv(\"data/earthquakes.csv\" )\n\nAnswer the following questions and submit your answers via the Moodle quiz:\n\nIn which year between 1965 and 2016 were the most earthquakes recorded?\nWere more earthquakes recorded in the northern or the southern hemisphere between 1965 and 2016?"
  },
  {
    "objectID": "practice/week_2/lab_2.html",
    "href": "practice/week_2/lab_2.html",
    "title": "MA22019 2025 -Computer Lab 2",
    "section": "",
    "text": "Overview\nThe tutorial questions ask you to apply functions from the dplyr and lubridate R packages to analyse real-world data sets, and also introduce some important functions we did not use so far.\nBefore starting the exercises, make sure to load the dplyr and lubridate R packages:\n\nlibrary(dplyr)\nlibrary(lubridate)\n\nIf you are using your own laptop, you may have to first run the code in “InstallPackages.R” (available from Moodle) to install all the packages essential for this course.\nTutorial Question 1 - Honey production in the United States\nIn 2006, global concern was raised over the rapid decline in the U.S. honeybee population, an integral component to American honey agriculture. Large numbers of hives were lost to Colony Collapse Disorder, a phenomenon of disappearing worker bees causing the remaining hive colony to collapse. Speculation to the cause of this disorder points to hive diseases and pesticides harming the pollinators, though no overall consensus has been reached.\nThe data collected by the National Agricultural Statistics Service (NASS) for 1998-2012 is provided in the file “HoneyProductionUS.csv”. The following information is provided:\n\nstate - Initial of the U.S state\nnumcol - Number of honey producing colonies\nyieldpercol - Honey yield per colony in pounds\npriceperlb - Average price per pound in dollars based on expanded sales\nyear - Year to which the data relates\n\nWe are asked to extract some information on the amount of honey produced and its price.\n\nFor each state calculate the total amount of honey produced (in pounds) for the period 1998-2012. Identify the states which had the lowest and highest production.\nHow has the yield per colony of honey bees in the U.S. evolved from 1998 to 2012?\nBy which factor has the average price of honey increased in the state of Alabama between 1998 and 2012?\nTutorial Question 2 - Price of Bitcoin\nThe file “Bitcoin.csv” provides hourly data on the price of Bitcoin and the level of trade for the period 17 August 2017 - 19 October 2023. There are four variables:\n\ndate - Day and time of the observation\nclose - The value of one Bitcoin token at the end of the hour\nvolume.usdt - The trading volume during the hour in Tether (USDT). This provides a stable reference point for trading volume because the value of USDT is supposed to be roughly equivalent to $1 USD.\ntradecount - The total number of individual trades or transactions that have occurred within the hour. This variable counts the actual number of separate buy and sell transactions.\n\nUse the data to address the following questions:\n\nLoad the data and use the function as_datetime() in the lubridate package to convert the variable representing the date and time to its correct type. Explore how the value of one Bitcoin token has evolved between 17 August 2017 and 19 October 2023 (which is the period covered by the data).\nDoes the number of tokens traded per day follow the same pattern as that found in part a) for the price of one token?"
  },
  {
    "objectID": "practice/week_5/homework_5.html",
    "href": "practice/week_5/homework_5.html",
    "title": "MA22019 2025 - Homework 5",
    "section": "",
    "text": "Overview\nYour answer to the Homework Question can be submitted on Moodle to your tutor for feedback. The submission deadline is 17:00 on Thursday 13 March 2025. You should submit a single PDF or Word file that provides your R code, any created R output and all your comments.\nYou may want to load the following packages before starting the exercise:\n\nlibrary( dplyr )\nlibrary( ggplot2 )\nlibrary( tidytext )\nlibrary( wordcloud )\nlibrary( stringr )\n\nWhen working on a University PC, you have to first install the tidytext and wordcloud packages. The installation of the textdata package is quite cumbersome, and thus I provide the AFINN and Bing sentiment lexicons as .csv files for your convenience. You can load them using\n\nAFINN &lt;- read.csv( \"data/AFINN Sentiment Lexicon.csv\" )\nBing &lt;- read.csv( \"data/Bing Sentiment Lexicon.csv\" )\n\nHomework Question - The work of H.G. Wells\nWe want to analyze and compare the novels The Time Machine and War of the Worlds by the British author Herbert George Wells. The books are provided in the files “Time Machine.csv” and “War of Worlds.csv”.\n\nCompare the two books in terms of the words used within them.\nHow do “The Time Machine” and “The War of the Worlds” differ regarding their sentiment?"
  },
  {
    "objectID": "practice/week_4/quiz_4.html",
    "href": "practice/week_4/quiz_4.html",
    "title": "MA22019 2025 - Quiz 4",
    "section": "",
    "text": "Overview\nExercises 1-3 help you with revising some of the techniques covered in Week 3, and to also get some more pratice on data wrangling. There are no tutorial and homework questions as Coursework 1 will be released at 15:00 on Friday, 21 February 2025.\nBefore starting the questions, make sure to load the different packages we considered so far:\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(tidyr)\n\nExercise 1 - CO\\(_2\\) emissions across the globe\nThe file “CO2 Emissions.csv” provides the recorded daily CO\\(_2\\) emissions across six different sectors between 1 January 2019 and 31 May 2023 for 11 countries, the EU, the rest of the world (ROW) and across and the planet (WORLD). Enter your answers to the questions below via the quiz on Moodle.\n\nWhich two countries recorded the highest total CO\\(_2\\) emissions across all sectors for the considered time window?\nCreate a pie chart that illustrates the total emissions per sector for the UK between 1 January 2019 and 31 May 2023. Which sector of the UK economy produced the most CO\\(_2\\) emissions over the observation period?\nExercise 2 - Traffic between Minneapolis and St Paul\nThe file “Traffic Minnesota.csv” contains hourly data on the traffic volume for westbound I-94, a major interstate highway in the US that connects Minneapolis and Saint Paul, Minnesota. The data was collected by the Minnesota Department of Transportation from 2012 to 2018 at a station roughly midway between the two cities.\nThe variables are\n\ntraffic_volume: Hourly I-94 reported westbound traffic volume.\nholiday: Indicates whether the date is a US national holiday or a regional holiday (such as the Minnesota State Fair).\ndate_time: Shows the hour of the data collected in local CST time.\n\n\nUse the function dmy_hm() in the lubridate R package to convert the variable date_time to its correct type. Apply the function weekdays() to extract the day of the week and store this information as a separate variable within the data frame.\n\nAnswer the following two questions using the data and submit your solutions via the Moodle quiz on Moodle:\n\nWhat is the average traffic volume on a Saturday?\nWhat is the difference in average traffic volume between work days and the weekend/ a holiday?\n\n\nCreate a plot which visualizes how the traffic volume changes throughout the day on Mondays.\nExercise 3 - Weather in Brisbane and Gold Coast\nThe file “BrisbaneGoldCoast.csv” contains daily weather measurements for Brisbane and Gold Coast in Australia.\n\nExplore how the daily maximum temperature for Gold Coast varies throughout the year. Use the results from your analysis to answer the Moodle quiz.\nCreate a scatter plot of the daily maximum temperature for Brisbane and Gold Coast using the ggplot2 R package. What do you conclude?"
  },
  {
    "objectID": "practice/week_3/homework_3.html",
    "href": "practice/week_3/homework_3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Overview\nYour answer to the Homework Question can be submitted on Moodle to your tutor for feedback. The submission deadline is 17:00 on Thursday 20 February 2025. You should submit a single Word, PDF or HTML file that provides your R code, any created R output and all your comments.\nBefore starting the questions, please make sure to load the relevant R packages:\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nHomework Question - Optimizing growing conditions for orchids\nA research institute set up an experiment in 2024 to determine the best growing conditions for two types of Orchids: Cymbidium and Dendrobium. The design of the experiment was as follows:\n\nAll orchids were planted in March or April 2024. Each individual orchid was exposed to constant temperature and phosphate levels, but levels differed across orchids.\nThe height (in inches) and quality of each orchid were measured on 20 October 2024. Plant quality was assessed using a score between 1 and 10 (1=very poor, 10=excellent). Any orchid with a score above 6 is considered of “good” quality.\n\nThe research institute approached us to analyze their collected data which provides:\n\nType - Type of the orchid.\nHeight - Height of the orchid on 22 October 2024.\nQuality - Quality as measured on 22 October 2024.\nPhosphate - The level of phosphate in ppm (parts per million) the orchid was exposed to since it had been planted.\nTemperature - The temperature (in degree Celsius) the orchid was exposed to since it had been planted.\nPlanting - The date the orchid was planted.\n\nThe full data are provided in the file “Orchids.csv” and the research institute is interested in:\n\nHow do the two types of orchids compare in terms of the heights measured on 22 October 2024?\nWhat are the effects of phosphate and temperature on the height of the orchids?\nDid the date when the orchid was planted have any effect on whether their quality was at least “good” on 22 October 2024?\n\nPerform an analysis which considers the three aspects above. Make sure to clearly state your approach and conclusions."
  },
  {
    "objectID": "practice/week_6/quiz_6.html",
    "href": "practice/week_6/quiz_6.html",
    "title": "MA22019 2025 - Quiz 6",
    "section": "",
    "text": "Overview\nThis week’s problem sheet focuses on the text data analysis techniques covered in Sections 3.3.2 and 3.4 of the lecture notes. Exercises 1-2 help you with revising the content of the lecture in Week 6. You can check your solutions to these questions yourself by answering the Moodle quiz.\nYou may want to load the following packages before starting the exercise:\n\nlibrary( dplyr )\nlibrary( ggplot2 )\nlibrary( tidytext )\nlibrary( stringr )\nlibrary( tidyr )\nlibrary( topicmodels )\n\nWhen working on a University PC, you have to first install the tidytext package and any dependencies using\n\ninstall.packages( \"tidytext\", dependencies = TRUE )\n\nFor the sentiment analysis you can load the sentiment lexicons using\n\nAFINN &lt;- read.csv( \"data/AFINN Sentiment Lexicon.csv\" )\nBing &lt;- read.csv( \"data/Bing Sentiment Lexicon.csv\" )\n\nExercise 1 - Comparing Moby Dick and Robinson Crusoe\nWe want to consider the books Moby Dick and The Life and Adventures of Robinson Crusoe. The text for both books is provided in the file “AdventureBooks.csv” and we load it using\n\nBooks &lt;- read.csv(\"data/adventurebooks.csv\" )\n\nConsider the following two questions:\n\nWhich five words (excluding stop words) are the most common in Moby Dick?\nWhen considering a corpus which only includes Moby Dick and The Life and Adventures of Robinson Crusoe, which five words have the highest term frequency - inverse document frequency (tf-idf)?\nExercise 2 - Analysis of news articles\nIn Section 4.4 we applied topic modelling to articles published in the New York Times. We now study another example. The file “Articles.csv” on Moodle provides the text for 2692 news articles from 2015. The articles were published either in the “business” or the “sports” category. In this exercise you will first repeat the steps from Section 4.4, and then explore how well your fitted model performs at identifying whether an article belongs to the “business” or “sports” category.\n\nTreading each article as a separate document, derive the document term matrix for the set of articles and store it as Articles_dtm.\n\nWith the document term matrix having been derived, we estimate the parameters of an LDA model with \\(K=2\\) topics using:\n\nArticles_LDA &lt;- LDA( Articles_dtm, k = 2, method=\"Gibbs\", control = list(seed=2024) )\n\n\nFor each article, extract the proportions with which the different topics feature. Is there a difference in proportions between “business” and “sports” articles?\nWhich are the five most common words in each of the two topics?"
  },
  {
    "objectID": "practice/week_6/lab_6.html",
    "href": "practice/week_6/lab_6.html",
    "title": "MA22019 2025 - Computer Lab 6",
    "section": "",
    "text": "Overview\nTutorial Question 1 is quite extensive and will require you to apply most of the techniques from Chapter 3. Tutorial Question 2 focuses on the function grep(), which was used before, but we never explored its full potential. Should time permit, you may want to work on the Homework Question during the tutorial.\nYou may want to load the following packages before starting the exercise:\n\nlibrary( dplyr )\nlibrary( ggplot2 )\nlibrary( tidytext )\nlibrary( stringr )\nlibrary( tidyr )\nlibrary( topicmodels )\n\nWhen working on a University PC, you have to first install the tidytext package and any dependencies using\n\ninstall.packages( \"tidytext\", dependencies = TRUE )\n\nFor the sentiment analysis you can load the sentiment lexicons using\n\nAFINN &lt;- read.csv( \"data/AFINN Sentiment Lexicon.csv\" )\nBing &lt;- read.csv( \"data/Bing Sentiment Lexicon.csv\" )\n\nTutorial Question 1 - Comparing books\nWe are asked to explore and compare the books Anne of Green Gables by L.M. Montgomery and Rebecca of Sunnybrook Farm by Kate Douglas Wiggin. The two books are provided together in the file “Books Tutorial Question 1.csv” and the following information is provided:\n\ntext - Text as printed in the book\ntitle - Book the text comes from\nchapter - Chapter the text belongs to\n\nPerform the following tasks using the techniques described in Chapter 3 of the lecture notes.\n\nExtract the two words with the highest term frequency-inverse document frequency for each book, with the corpus only containing Anne of Green Gables and Rebecca of Sunnybrook Farm.\nUse sentiment analysis to explore how the emotional intent has evolved over the two books. How do the two books compare?\nSuppose each book chapter is considered as a separate document (as we did in Section 3.4.3 in the lecture notes). Use Latent Dirichlet Allocation to derive \\(K=2\\) topics, and then study the estimated proportions provided by the model. What do you conclude?\nSome scholars claim that Anne of Green Gables is patterned after Rebecca of Sunnybrook Farm. Discuss whether your results in parts a)-c) support this claim or not.\nTutorial Question 2 - The function grep()\nWe so far only used the function grep() when extracting the text data from the files provided by Project Gutenberg. Specifically, we used it to identify the lines containing the word “EBOOK”, signalling the beginning and end of the book. The following exercise will require you to use grep() to identify all lines which contain a specific phrase:\nThe Police of Utopia sent us data on burglaries which were reported between 2015 and 2021, including a short description providing information on the number of criminals and their victims. Victims are classified into six groups: “young single”, “young couple”, “middle-aged single”, “middle-aged couple”, “elderly single” and “elderly couple”. The data are available in the file “UtopiaCrimes.csv”. Extract the following information using the functions grep():\n\nFor which group of people did the police record the most burglaries?\nWhat is the proportion of burglaries that involved more than two criminals?"
  },
  {
    "objectID": "practice/week_1/lab_1.html",
    "href": "practice/week_1/lab_1.html",
    "title": "MA22019 2025 - Computer Lab 1",
    "section": "",
    "text": "Overview\nThis week’s tutorial questions revise some fundamentals of the R programming language. You will further learn how to produce a PDF/HTML file using R Markdown. Note, you will be required to use R Markdown for the coursework.\nIf you are using your own laptop, you want to first run the code in “InstallPackages.R” (available from Moodle) to install all the packages essential for this course.\nTutorial Question 1 - Creating and editing an R Markdown file\nR Markdown allows us to produce a document that includes all the R code, plots and accompanying text of our analysis. After completing the following tasks, you will have produced and edited your first R Markdown file:\n\n\nCreate an R Markdown file in RStudio:\n\nGo to File -&gt; New File and then click R Markdown.\nSelect any output option and provide a title and author (both can be changed later). Click Continue at the bottom of the window. This should load an R Markdown file with some material already added to it.\n\n\nSave the file in a directory on your OneDrive. Then click the Knit button (the ball of yarn and needle) to compile the document into a PDF/Word/HTML document. After a few seconds, you should see a pop-up window with the produced document. If you are using your own laptop, you may first need to install a version of LaTeX using the following code:\n\n\n    install.packages( \"tinytex\", dependencies = TRUE )\n    tinytex::install_tinytex()\n\n\n\nHave a look at the document “Editing the R Markdown file” on Moodle. Then edit, and ultimately knit, your R Markdown file such that it includes\n\nA chunk of R code which samples 10,000 observations from a \\(\\mathrm{Normal}(\\mu=5,~\\sigma^2=4)\\) distribution and stores the samples in a vector y.\nA second chunk of R code which extracts the minimum and maximum value in y. You may want to consider using the functions min() and max().\nA third chunk of R code which creates a histogram of the data in \\(y\\). Your plot should be centred and be 60% of the text width.\nSome text before each R chunk to describe what the different chunks of R code are doing.\n\n\nTutorial Question 2 - Tuition fees in the United States\nWe are interested in exploring how tuition fees for private colleges have evolved across the U.S. between 2013 and 2020. For the analysis we will consider data collected by the National Center of Education Statistics (NCES).\nThe file “Private College Costs.csv” gives per U.S. state (except Wyoming) the average annual tuition fee (in U.S. Dollars) charged to full-time students enrolled in private colleges located in the state for 2013–2020. We can load the data file into R using\n\nFees &lt;- read.csv( \"data/Private College Costs.csv\" )\n\nNote, the data file needs to be in the same folder as the R Markdown file. Use the data to address the following questions:\n\nCreate a plot which visualizes the recorded average fee for the years 2013–2020 for the state of Alabama. What can we conclude from the plot?\nFor each state, calculate the relative change in tuition fees when comparing the value for the years 2013 and 2020. Provide a histogram which visualizes the calculated values.\nWhich state has recorded the highest relative increase in average tuition fees for private universities when comparing 2013 and 2020?\nDiscuss whether the data may be used to derive the average increase in tuition fees across all private US colleges from 2013 to 2020."
  },
  {
    "objectID": "practice/week_8/homework_8.html",
    "href": "practice/week_8/homework_8.html",
    "title": "MA22019 2025 - Homework 8",
    "section": "",
    "text": "Overview\nYour answer to the Homework Question can be submitted on Moodle to your tutor for feedback. The submission deadline is 17:00 on Thursday 3 April 2025. You should submit a single PDF or Word file that provides your R code, any created R output and all your comments.\nYou may want to load the following packages before starting the exercise:\n\nlibrary( dplyr )\nlibrary( lubridate )\nlibrary( ggplot2 )\nlibrary( tidyr )\nlibrary( sf )\nlibrary( ggspatial )\nlibrary( prettymapr )\nlibrary( sp )\nlibrary( gstat )\n\nWhen working on a University PC, you will have to first install some of these packages.\nHomework Question - Spatial dependence in temperatures across Brazil\nLet’s again consider the temperature data from Brazil considered in Problem Sheet 6. The data are provided in the file “Brazil Temperature.csv”. In the following we want to explore the spatial dependence in the data.\n\nEstimate the semi-variogram for the data. What do you conclude about the spatial dependence in the observed temperatures?\nDiscuss whether it is reasonable to assume that the spatial random process has a constant mean and that the value of the semi-variogram is fully specified by the distance between spatial sites.\nBonus: One may argue that the constant mean assumption may not hold for the spatial process \\(\\{X(\\mathbf{s}) : \\mathbf{s}\\in\\mathcal{S}\\}\\). The variogram() function allows us to define a model of the form \\[\\begin{equation}\nX(\\mathbf{s}) = \\beta_0  + \\beta_1\\mathrm{Longitude}(\\mathbf{s}) + \\beta_2\\mathrm{Latitude}(\\mathbf{s}) + Z(\\mathbf{s}),\n\\end{equation}\\] where \\(\\{Z(\\mathbf{s}) : \\mathbf{s}\\in\\mathcal{S}\\}\\) is termed the residual random process. The semi-variogram estimated by the variogram() function then corresponds to that for the residual random process. In R, we derive and visualize this estimate using\n\n\nBrazil &lt;- read.csv( \"data/Brazil Temperature.csv\" )\ncoordinates( Brazil ) &lt;- ~Lon+Lat\ngamma_hat &lt;- variogram( MeanTemp ~Lon+Lat, data = Brazil )\nggplot( gamma_hat, aes( x=dist, y=gamma/2 ) ) + geom_point( size=2 ) + \n  theme_bw() + labs( x=\"Distance\", y=\"Semi-variogram\" )\n\n\n\n\n\n\n\nDescribe the similarities and differences between this estimate and yours in part a). Discuss whether the conclusions we draw regarding spatial dependence in the residual process are the same as for the process in part a).\nRemark: You may learn more about the type of models in Equation (1) in Statistical Modelling & Data Analytics 3B."
  },
  {
    "objectID": "practice/week_9/lab_9.html",
    "href": "practice/week_9/lab_9.html",
    "title": "MA22019 2025 - Computer Lab 9",
    "section": "",
    "text": "Overview\nTutorial Questions 1 and 2 provide some more advanced questions. You can check your answers for Exercises 1 and 2 using the quiz provided on Moodle.\nYou may want to load the following packages before starting the exercise:\n\nlibrary( dplyr )\nlibrary( ggplot2 )\nlibrary( tidyr )\nlibrary( sf )\nlibrary( ggspatial )\nlibrary( prettymapr )\nlibrary( spatstat )\n\nWhen working on a University PC, you will have to first install some of these packages.\nTutorial Question 1 - Crime across Utopia\nUtopia’s police department needs your help with analyzing their 2015-2021 data regarding certain crimes. The data is provided in the file “UtopiaCrimes.csv”. Note, longitude and latitude coordinates are only available for drug possession offences across District 44.\nUtopia consists of 59 districts and a shapefile of Utopia is provided as “UtopiaShapefile.shp”. To hide Utopia’s location, constants have been added to the latitude and longitude coordinates, but the shapes they define are correct. The population for each district is provided in the file “UtopiaPopulation.csv”.\n\nIdentify the three most common crimes in Utopia for the period 2015-2021.\nVisualize the rate per 1,000 population for the most common crime for 2015-2021 for the different districts. What do you conclude from your plot?\nYou are told that District 44 is notorious for drug possession. The police is planning to conduct a raid to tackle the issue, but they are unsure which areas of the district are the most seriously affected. Use spatial data analysis techniques to identify the parts of District 44 they should be focusing on. Hint: You may want to consider the function st_reverse() should you get an error message regarding orientation/direction.\nTutorial Question 2 - Cost of living crisis in Texas\nAs many other countries around the world, the United States have seen high inflation over the past years. In the following we want to analyze socioeconomic data for the state of Texas. We have access to two data files:\n\n“Cost_Texas.csv” gives the cost for essentials (food, rent, healthcare,etc.) in 2023 for a single household for each county in Texas based on the Family Budget Calculator by the Economic Policy Institute.\n“Income_Texas.csv” gives the median income per capita for each county in Texas as reported by the Bureau of Economic Analysis of the US government for 2021.\n\nPerform the following tasks and answer the research questions:\n\nLoad the two data files and calculate the difference between median income and cost of living for each county.\nThe file “Texas.Rdata” provides a shapefile of Texas, called Texas, with all county boundaries. Use the shapefile to create a map which visualizes the variable calculated in part a). What do you conclude?\nHow reliable are our results found in part b)? Hint: You may want to extract the two counties with the highest median income and see what you find out about them on Wikipedia."
  },
  {
    "objectID": "practice/week_9/quiz_9.html",
    "href": "practice/week_9/quiz_9.html",
    "title": "MA22019 2025 - Quiz 9",
    "section": "",
    "text": "Overview\nThis week’s problem sheet focuses on the methods for analyzing point pattern and lattice/areal unit data in Sections 4.5-4.7 in the lecture notes. Exercises 1-2 help you with revising the content of the lecture in Week 9\nYou may want to load the following packages before starting the exercise:\n\nlibrary( dplyr )\nlibrary( ggplot2 )\nlibrary( tidyr )\nlibrary( sf )\nlibrary( ggspatial )\nlibrary( prettymapr )\nlibrary( spatstat )\n\nWhen working on a University PC, you will have to first install some of these packages.\nExercise 1 - Swine fever outbreaks across Europe\nThe file “Outbreaks.csv” contains the data for animal disease outbreaks between September 2015 and August 2017 as provided by the EMPRES Global Animal Disease Information System. Each entry in the data set specifies the location, date and type of disease for an individual outbreak. In this question we want to focus on the African swine fever that has affected large parts of Europe in the past years. Address the following three research tasks / questions and then complete the Moodle quiz.\n\nWhich three countries were the worst affected in terms of the total number of outbreaks of African swine fever?\nVisualize the locations of outbreaks of African swine fever for the Baltic states (Estonia, Latvia and Lithuania). What do you conclude?\nLatvia has recorded a high number of outbreaks. Identify the parts of Latvia that observed a high frequency of swine fever outbreaks using quadrat counting or the kernel smoothed intensity function. Is it reasonable to conclude that the point process describing outbreaks of African swine fever across Latvia is homogeneous?\nExercise 2 - Crime rates across France\nThe file “Crimes France.csv” contains crime statistics from 2015 for all French départments (except Corsica). The file uses the UTF-8 encoding and you should ensure to specify this when loading the file. A shapefile for France with the départment boundaries is provided in the file “gadm41_FRA_2.shp”. Use the techniques for areal unit data to complete the following tasks and then go to Moodle and answer the quiz questions.\n\nImport the shapefile and create a map for France which includes the boundaries for the départments.\nVisualize the rate of violence per 1000 people per départment. Which areas should we avoid when we are concerned about violence?\nVisualize the rate of burglaries per 1000 people per départment. What do you conclude?"
  },
  {
    "objectID": "practice/week_7/homework_7.html",
    "href": "practice/week_7/homework_7.html",
    "title": "MA22019 2025 - Homework 7",
    "section": "",
    "text": "The Homework question is inspired by one of last year’s coursework questions. Besides the creation of plots, the question asks you to carefully discuss the assumptions we make when applying the tools covered in Sections 4.1 and 4.2.\nYour answer to the Homework Question can be submitted on Moodle to your tutor for feedback. The submission deadline is 17:00 on Thursday 27 March 2025. You should submit a single Word, PDF or HTML file that provides your R code, any created R output and all your comments.\nYou may want to load the following packages before starting the exercises:\n\nlibrary( dplyr )\nlibrary( ggplot2 )\nlibrary( sf )\nlibrary( ggspatial )\nlibrary( prettymapr )"
  },
  {
    "objectID": "practice/week_7/homework_7.html#homework-question---lead-concentration-across-amaurot",
    "href": "practice/week_7/homework_7.html#homework-question---lead-concentration-across-amaurot",
    "title": "MA22019 2025 - Homework 7",
    "section": "Homework Question - Lead concentration across Amaurot",
    "text": "Homework Question - Lead concentration across Amaurot\nThe local authorities in Amaurot, the capital of Utopia, have seen an alarming increase in lead concentration levels in the drinking water. They thus collected samples on lead concentration across the city. The data on the recorded lead levels and the spatial coordinates are provided in the file “Amaurot Lead.csv”.\nThe local authorities have now approached you to help them tackle the issue. They provided you with the collected data and a shapefile of Amaurot. To hide Amaurot’s location, the latitude and longitude coordinates have been manipulated, but the provided shapes are correct. You are asked to perform the following tasks:\n\nVisualize the measured lead concentrations. What do you conclude?\nPerform inverse distance weighting to predict lead levels for all points in the file “Amaurot Grid.csv”; the points form a regular grid over the whole city and there is also information in which district the point lies. Comment on the reliability of your predictions.\nThe authorities want to reduce the occurrences of lead concentrations exceeding a level of 10 parts per million. They have thus decided to deploy a team to one of the districts with the job of reducing lead levels to below this threshold for all households in that districts. Use your results in part b) to identify which district should be targeted."
  },
  {
    "objectID": "live_coding/week_2/analysis_of_pet_data_(complete).html",
    "href": "live_coding/week_2/analysis_of_pet_data_(complete).html",
    "title": "Problem Class 2 - Solution",
    "section": "",
    "text": "library(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(patchwork)"
  },
  {
    "objectID": "live_coding/week_2/analysis_of_pet_data_(complete).html#background",
    "href": "live_coding/week_2/analysis_of_pet_data_(complete).html#background",
    "title": "Problem Class 2 - Solution",
    "section": "Background",
    "text": "Background\nThe Utopian charity Respect for Pets has collected data on cats and dogs for 1990-2023. Utopia only allows three dog breeds: Beagle, Dachshund and Maltese. In Utopia all pets have to be registered. The charity would like to gain some insight regarding the following questions:\n\nHow have the numbers of dogs and cats changed over time?\nHow has the popularity of the different dog breeds evolved since 1990?\nMaltese are known to experience respiratory issues, such as wheezing or asthma. How does temperature affect the risk of a Maltese experiencing these issues?\n\nThis problem class is inspired by a question on last year’s MA20277 Coursework 1, which almost used the exact same data. We will explore how we can apply the techniques seen so far to solve the three questions."
  },
  {
    "objectID": "live_coding/week_2/analysis_of_pet_data_(complete).html#data-descriptions",
    "href": "live_coding/week_2/analysis_of_pet_data_(complete).html#data-descriptions",
    "title": "Problem Class 2 - Solution",
    "section": "Data Descriptions",
    "text": "Data Descriptions\nWe are provided with two data files for the analysis:\nData/Pets.csv: Information on the number of registered pets at the beginning of each month for January 1990 until September 2023:\n\nYear, Month: Year and month\nBeagles: Number of beagles\nDachshund: Number of dachshunds\nMaltese: Number of Maltese\nCats: Number of cats\n\nData/Cases.csv: Number of Maltese treated by a vet for respiratory issues for 1990-2022:\n\nDate: Date (day, month and year)\nTemperature: Daily maximum temperature in degree Celsius\nNumber: Number of Maltese admitted with respiratory issues"
  },
  {
    "objectID": "live_coding/week_2/analysis_of_pet_data_(complete).html#number-of-dogs-and-cats-over-time",
    "href": "live_coding/week_2/analysis_of_pet_data_(complete).html#number-of-dogs-and-cats-over-time",
    "title": "Problem Class 2 - Solution",
    "section": "Number of dogs and cats over time",
    "text": "Number of dogs and cats over time\nWe start by loading the data into our R Workspace using the read.csv() function and looking at the loaded data,\n\nPets &lt;- read.csv(\"data/pets.csv\" )\nglimpse( Pets )\n\nRows: 405\nColumns: 6\n$ Year      &lt;int&gt; 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, …\n$ Month     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, …\n$ Beagles   &lt;int&gt; 81817, 81975, 82169, 82064, 81965, 82169, 82036, 81923, 8209…\n$ Dachshund &lt;int&gt; 108739, 109039, 109362, 109221, 109081, 109298, 109163, 1090…\n$ Maltese   &lt;int&gt; 73322, 73498, 73632, 73529, 73414, 73640, 73551, 73440, 7362…\n$ Cats      &lt;dbl&gt; 283178, 283367, 283692, 284040, 283864, 284058, 284653, 2850…\n\n\nTask 1: Which type of plot may be useful to explore how the numbers of dogs and cats have changed over time?\n\nA line plot is the best choice here, as we study how a variable has changed over time.\n\nBefore we create the plot, we need to do some data cleaning and wrangling. We start by converting the information on year and month into a variable of type Date. We can use the function ym() from the lubridate package for this:\n\nPets &lt;- Pets %&gt;% \n  mutate( Date = paste( Year, Month, sep=\"-\" ) ) %&gt;%\n  mutate( Date = ym( Date ) )\n\nSince we want to plot the total number of dogs, we further create a variable Dogs which contains this information:\n\nPets &lt;- Pets %&gt;% mutate( Dogs = Beagles + Dachshund + Maltese )\n\nTask 2: Produce a plot which illustrates how the numbers of cats and dogs have changed over time.\n\nWe create two line plots with different shape and colour:\n\n\nggplot( Pets, aes(\"x\"=Date) ) + \n  geom_line( aes( \"y\"=Dogs, color=\"Dogs\" ) ) + \n  geom_line( aes( \"y\"=Cats, color=\"Cats\"), linetype=2 ) +\n  labs( x=\"Year\", y=\"Number\" ) +\n  scale_colour_manual( name=\"Pet\", values=c(\"Dogs\"=\"red\",\"Cats\"=\"blue\") )\n\n\n\n\n\n\n\n\nThe last line manipulates the legend such that it has a nice name (“Pets” insetad of “colour”).\n\nTask 3: What do we conclude from the plot produced in Task 2?\n\n\nThere were more cats than dogs until 2015, but since then there are more dogs than cats.\n\n\nThe number of dogs has steadily increased from around 265,000 to 315,000 in 2023.\n\n\nThe number of cats increased from about 283,000 in 1990 to around 305,000 in 2000-2005 and has since then been on the decline. For 2023, the number of registered cats is about 275,000."
  },
  {
    "objectID": "live_coding/week_2/analysis_of_pet_data_(complete).html#popularity-of-dog-breeds",
    "href": "live_coding/week_2/analysis_of_pet_data_(complete).html#popularity-of-dog-breeds",
    "title": "Problem Class 2 - Solution",
    "section": "Popularity of dog breeds",
    "text": "Popularity of dog breeds\nWe now turn our attention to the second research question.\nTask 4: How would you define popularity of a dog breed?\n\nFocusing on the number of dogs is not ideal, because the overall number of dogs has increased over time. As such, there may more dogs of one breed, but its popularity may be lower. Therefore, we explore the proportion of the three breeds (among the dog population):\n\n\nPets &lt;- Pets %&gt;% \n  mutate( BeaglesPopularity = Beagles / Dogs,\n          DachshundPopularity = Dachshund / Dogs,\n          MaltesePopularity = Maltese / Dogs )\n\nTask 5: Create a plot (or plots) to explore how popularity for the three different dog breeds has evolved over time.\n\nWe again create a line plot. We can either plot all three breeds in the same plot or create a separate plot for each breed. To create separate plots, we use the patchwork R package (it allows us to place plots next to each other using the “+” sign):\n\n\ng1 &lt;- ggplot( Pets, aes( \"x\"=Date, \"y\"=BeaglesPopularity ) ) +\n  geom_line() + labs( title=\"Beagle\", x=\"Year\", y=\"Proportion of Beagles\" )\ng2 &lt;- ggplot( Pets, aes( \"x\"=Date, \"y\"=DachshundPopularity ) ) +\n  geom_line() + labs( title=\"Dachshund\", x=\"Year\", y=\"Proportion of Dachshunds\" )\ng3 &lt;- ggplot( Pets, aes( \"x\"=Date, \"y\"=MaltesePopularity ) ) + \n  geom_line() + labs( title=\"Maltese\", x=\"Year\", y=\"Proportion of Maltese\" )\ng1 + g2 + g3\n\n\n\n\n\n\n\nTask 6: What do you conclude from your plot(s) created in Task 5?\n\n\nDachshunds have been the most popular breed throughout the period 1990-2023. The proportion of dachshunds has grown from about 41% in 1990 to 43% in 2023.\n\n\nBeagles have become slightly less popular over the period 1990-2023: the proportion was 31.5% in 1990 and is 29.5% today.\n\n\nPopularity of Maltese reached its peak between 2005 and 2015 and has been decreasing ever since - it’s even lower than in the 1990s."
  },
  {
    "objectID": "live_coding/week_2/analysis_of_pet_data_(complete).html#effect-of-temperature-on-respiratory-issues",
    "href": "live_coding/week_2/analysis_of_pet_data_(complete).html#effect-of-temperature-on-respiratory-issues",
    "title": "Problem Class 2 - Solution",
    "section": "Effect of temperature on respiratory issues",
    "text": "Effect of temperature on respiratory issues\nThe file “Data/Cases.csv” provides information on the daily temperature and number of Maltese with respiratory issues. So we start by loading this file:\n\nCases &lt;- read.csv(\"data/cases.csv\", header=TRUE )\n\nTask 7: Explain why plotting the daily temperature against the number of dogs admitted with respiratory issues is not a good approach. What may be a better approach to address this question?\n\nWhen plotting the reported number of Maltese with respiratory issues, we ignore that the number of Maltese in the population has changed over time. A better approach is to instead consider the proportion of Maltese treated by a vet for respiratory issues.\n\nOne possible approach is the following:\n\nMaltese &lt;- Pets %&gt;% select( Year, Month, Maltese )\n\nCases &lt;- Cases %&gt;% \n  mutate( Year=year(Date), Month=month(Date) ) %&gt;%\n  full_join( Maltese, by=c( \"Year\"=\"Year\", \"Month\"=\"Month\" ) ) %&gt;%\n  mutate( Proportion = Number / Maltese )\n\nggplot( Cases, aes(x=Temperature, y=Proportion) ) + \n  geom_point() + geom_smooth() + \n  labs( y=\"Proportion of Maltese with respiratory issues\" )\n\nWarning: Removed 9 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nTask 8: What do we conclude from the plot? Which assumptions did we make in our analysis?\n\nThe lowest risk for respiratory issues is observed around 5 degree Celsius. The risk slowly increases with increasing temperature over the range 5 to 30 degree Celsius, and then increases noticeably beyond 30 degree Celsius. For lower temperatures, the highest risk is observed for negative temperatures. Consequently, Maltese are at a higher risk on days with very low or very high maximum daily temperature.\n\n\nRegarding the assumptions underlying our conclusion, consider the following two aspects:\n\n\nIs the data comparable for all years, e.g., does the population of Maltese have the same characteristics all the time?\n\n\n\nHow quickly do we assume the issues to be reported to the vet?"
  },
  {
    "objectID": "live_coding/week_5/analysis_of_sea_surface_temperature_(complete).html",
    "href": "live_coding/week_5/analysis_of_sea_surface_temperature_(complete).html",
    "title": "Problem Class 5 - Solution",
    "section": "",
    "text": "Sea surface temperature is the temperature of the top millimeter of the ocean’s surface. Sea surface temperatures influence weather, including hurricanes, as well as plant and animal life in the ocean. Like Earth’s land surface, sea surface temperatures are warmer near the equator and colder near the poles. Currents like giant rivers move warm and cold water around the world’s oceans. Some of these currents flow on the surface, and they are obvious in sea surface temperature images.\nIn this short exercise we consider the average sea surface temperature for the North Atlantic in August 2022 as recorded by NASA. The data is produced by placing a fine grid over the globe and then deriving the average temperature for each pixel.\nLet’s import the data\n\nSea_raw &lt;- read.csv(\"data/seasurface.csv\" )\n\nOur aim is to visualize the spatial distribution, and we load the necessary packages\n\nlibrary(ggplot2)\nlibrary(sf)\n\nConsider the following tasks."
  },
  {
    "objectID": "live_coding/week_5/analysis_of_sea_surface_temperature_(complete).html#background",
    "href": "live_coding/week_5/analysis_of_sea_surface_temperature_(complete).html#background",
    "title": "Problem Class 5 - Solution",
    "section": "",
    "text": "Sea surface temperature is the temperature of the top millimeter of the ocean’s surface. Sea surface temperatures influence weather, including hurricanes, as well as plant and animal life in the ocean. Like Earth’s land surface, sea surface temperatures are warmer near the equator and colder near the poles. Currents like giant rivers move warm and cold water around the world’s oceans. Some of these currents flow on the surface, and they are obvious in sea surface temperature images.\nIn this short exercise we consider the average sea surface temperature for the North Atlantic in August 2022 as recorded by NASA. The data is produced by placing a fine grid over the globe and then deriving the average temperature for each pixel.\nLet’s import the data\n\nSea_raw &lt;- read.csv(\"data/seasurface.csv\" )\n\nOur aim is to visualize the spatial distribution, and we load the necessary packages\n\nlibrary(ggplot2)\nlibrary(sf)\n\nConsider the following tasks."
  },
  {
    "objectID": "live_coding/week_5/analysis_of_sea_surface_temperature_(complete).html#tasks",
    "href": "live_coding/week_5/analysis_of_sea_surface_temperature_(complete).html#tasks",
    "title": "Problem Class 5 - Solution",
    "section": "Tasks",
    "text": "Tasks\nTask 1: What is the type of spatial data we are working with?\n\nWhile this is strictly speaking lattice data, we will see that the techniques we introduced so far also work for this data due to its high resolution.\n\nTask 2: Convert the data into a spatial object using the st_as_sf() function:\n\nWe can use the same code as for the temperature data from Germany\n\n\nSea &lt;- st_as_sf( Sea_raw, coords=c(\"lon\", \"lat\"), crs=\"WGS84\" )\n\nTask 3: Visualize the data using the WGS84 coordinate system. What do you conclude?\n\nThis is an example where we don’t need a shapefile or map. We can directly plot the spatial object:\n\n\nggplot( data=Sea ) + theme_bw() + geom_sf( aes(color=Temperature) )\n\n\n\n\n\n\n\n\nWe see that north-south trend described in the background section, with temperatures tending to be be higher near to the equator. The plot also shows that the sea surface temperature changes slowly even over large distances. Finally, we identify the Labrador Current which is a cold current that flows along the east coast of Canada, and there are some indications of the Gulf Stream.\n\nTask 4: Visualize the data using coordinate system with CRS=3347. Which aspects are better represented by this projection?\n\nWe use the function coord_sf() to change the coordinate system:\n\n\nggplot( data=Sea ) + theme_bw() + geom_sf( aes(color=Temperature) ) + \n  coord_sf( crs=st_crs(3347) )\n\n\n\n\n\n\n\n\nThis new plot does a better job at visualizing that observations are located on a sphere. This will mean that distances between sites in the plot are more representative of the actual distances."
  },
  {
    "objectID": "live_coding/week_4/analysis_of_jane_austen.html",
    "href": "live_coding/week_4/analysis_of_jane_austen.html",
    "title": "Problem Class 4",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(tidytext)\nlibrary(stringr)"
  },
  {
    "objectID": "live_coding/week_4/analysis_of_jane_austen.html#background",
    "href": "live_coding/week_4/analysis_of_jane_austen.html#background",
    "title": "Problem Class 4",
    "section": "Background",
    "text": "Background\nJane Austen wrote seven novels, and we consider six of these: Emma, Mansfield Park, Northanger Abbey, Persuasion, Pride and Prejudice and Sense and Sensibility.\nAll novels are available via Project Gutenberg and we will analyze similarities and differences of the different books in the following. We start by loading the text data which is available in the file “Data/JaneAusten.csv” on Moodle:\n\nJaneAusten_raw &lt;- read.csv(\"data/janeausten.csv\" )\n\nWe will analyze the six books and\n\nIdentify the most common words (except stop words) for each book\nExtract the words with the highest term frequency - inverse document frequency\nCompare the six books in terms of their sentiment"
  },
  {
    "objectID": "live_coding/week_4/analysis_of_jane_austen.html#word-frequency-analysis",
    "href": "live_coding/week_4/analysis_of_jane_austen.html#word-frequency-analysis",
    "title": "Problem Class 4",
    "section": "Word frequency analysis",
    "text": "Word frequency analysis\nAs before, we first have to bring the data into a usable format and remove stop words.\nTask 1: Split the lines of text into individual words and remove all stop words and underscores.\n\nJaneAusten &lt;- JaneAusten_raw %&gt;%\n  unnest_tokens( word, text ) %&gt;%\n  mutate( word = gsub(\"_\", \"\", word) ) %&gt;%\n  anti_join( stop_words, by=\"word\" )\n\nWith the data in the desired format, we are ready to identify the most common words:\nTask 2: Extract the ten most common words (excluding stop words) for each book.\n\nJaneAusten_Count &lt;- JaneAusten %&gt;%\n  group_by( title ) %&gt;%\n  count( word, name = \"Count\", sort = TRUE ) %&gt;%\n  slice_head( n=10 )\n\nAfter identifying the most frequent words, let’s visualize them. The following piece of code produces for each book a bar plot which visualizes the number of occurrences of the words identified in Task 2, i.e., the bar plot for a book provides information on the ten most common words (excluding stop words) contained in that book.\n\nggplot( JaneAusten_Count, \n        aes( x=Count, y=reorder_within(word,Count,title), fill=title ) ) + \n  facet_wrap( ~title, scales = \"free\" ) + \n  geom_col( show.legend = FALSE ) + \n  scale_y_reordered() + theme_bw() + \n  labs( x=\"Count\", y=\"Word\" )\n\n\n\n\n\n\n\nTask 3: What is the benefit of using the functions reorder_within() and scale_y_reordered()? What do we conclude from the plot?\nTo conclude our analysis on the words used within the books, we want to calculate the tf-idf values for each term.\nTask 4: Calculate the tf-idf value for each word and book. What do you conclude when considering the words with the highest tf-idf?\n\nJaneAusten_Count &lt;- JaneAusten %&gt;% count( title, word, sort=TRUE )\n\n\nJaneAusten_Count %&gt;% \n  bind_tf_idf( word, title, n ) %&gt;%\n  arrange( desc(tf_idf) ) %&gt;%\n  slice_head(n=20)\n\n                   title       word   n          tf       idf     tf_idf\n1  Sense and Sensibility     elinor 685 0.018812996 1.7917595 0.03370836\n2  Sense and Sensibility   marianne 566 0.015544753 1.7917595 0.02785246\n3             Persuasion     elliot 289 0.011328003 1.7917595 0.02029706\n4    Pride and Prejudice      darcy 432 0.011019284 1.7917595 0.01974391\n5                   Emma     weston 440 0.009446114 1.7917595 0.01692516\n6    Pride and Prejudice     bennet 339 0.008647077 1.7917595 0.01549348\n7             Persuasion  wentworth 218 0.008544998 1.7917595 0.01531058\n8                   Emma  knightley 389 0.008351224 1.7917595 0.01496338\n9                   Emma      elton 387 0.008308287 1.7917595 0.01488645\n10   Pride and Prejudice    bingley 310 0.007907356 1.7917595 0.01416808\n11      Northanger Abbey  catherine 487 0.020431280 0.6931472 0.01416188\n12        Mansfield Park   crawford 605 0.012738988 1.0986123 0.01399521\n13                  Emma       emma 865 0.018570202 0.6931472 0.01287188\n14                  Emma  woodhouse 315 0.006762559 1.7917595 0.01211688\n15 Sense and Sensibility   jennings 235 0.006454094 1.7917595 0.01156418\n16      Northanger Abbey    morland 148 0.006209095 1.7917595 0.01112521\n17 Sense and Sensibility willoughby 216 0.005932273 1.7917595 0.01062921\n18            Persuasion    russell 148 0.005801192 1.7917595 0.01039434\n19        Mansfield Park    bertram 272 0.005727280 1.7917595 0.01026191\n20      Northanger Abbey     tilney 221 0.009271690 1.0986123 0.01018599"
  },
  {
    "objectID": "live_coding/week_4/analysis_of_jane_austen.html#sentiment-analysis",
    "href": "live_coding/week_4/analysis_of_jane_austen.html#sentiment-analysis",
    "title": "Problem Class 4",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\nLet’s study the sentiment of the books using the AFINN sentiment lexicon:\n\nAFINN &lt;- read.csv( \"Data/AFINN Sentiment Lexicon.csv\" )\n\nWe want to derive the sentiment score for each chapter in each book. To do this, we first need to identify which chapter each line belongs to. We can do this by adapting the code from Section 3.2 in the lecture notes:\n\nJaneAusten_chapters &lt;- JaneAusten_raw %&gt;%\n  group_by( title ) %&gt;%\n  mutate( chapter = cumsum( str_detect(\n    text, regex(\"^chapter \", ignore_case = TRUE)\n  ) ) ) %&gt;%\n  filter( chapter &gt; 0 ) %&gt;%\n  ungroup()\n\nThe next step is to split the lines of text into individual words and to match the words in the AFINN sentiment lexicon with the words in the books. As for Jane Eyre, we remove the word “miss” from the analysis:\n\nJaneAusten_AFINN &lt;- JaneAusten_chapters %&gt;%\n  filter( chapter &gt; 0 ) %&gt;%\n  unnest_tokens( word, text ) %&gt;%\n  mutate( word = gsub( \"_\", \"\", word ) ) %&gt;%\n  inner_join( AFINN, by = \"word\" ) %&gt;% \n  filter( word != \"miss\" )\n\nTask 5: Derive the aggregated sentiment score for each chapter using the AFINN sentiment lexicon.\n\nJaneAusten_AFINN &lt;- JaneAusten_AFINN %&gt;%\n  group_by( title, chapter ) %&gt;%\n  summarise( sentiment = sum(value) )\n\n`summarise()` has grouped output by 'title'. You can override using the\n`.groups` argument.\n\n\nWe produce plots which illustrate the sentiment for the different chapters and books as follows:\n\nggplot( JaneAusten_AFINN, aes( x=chapter, y=sentiment ) ) +\n  facet_wrap( ~title, scales=\"free_x\" ) +\n  geom_col( aes( fill=title ), show.legend = FALSE ) + \n  theme_bw() + labs( x=\"Chapter\", y=\"AFINN sentiment score\" ) \n\n\n\n\n\n\n\nTask 6: What do we conclude?"
  },
  {
    "objectID": "live_coding/week_3/analysis_of_fire_data_(complete).html",
    "href": "live_coding/week_3/analysis_of_fire_data_(complete).html",
    "title": "Problem Class 3 - Solution",
    "section": "",
    "text": "library(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(tidyr)"
  },
  {
    "objectID": "live_coding/week_3/analysis_of_fire_data_(complete).html#background",
    "href": "live_coding/week_3/analysis_of_fire_data_(complete).html#background",
    "title": "Problem Class 3 - Solution",
    "section": "Background",
    "text": "Background\nThe Utopian Fire Department has gathered data on their activities for 2022. They also managed to provide us with access to some data for the houses in Utopia. We are asked to use the data to address the following questions:\n\nFor each cause, explore how the frequency of fires varied across time of day.\nAre there any differences in the risk of fire for the different types of property?\nWhat is the relation between the year a property was built and the risk of fire?\n\nWe will explore how we can apply the techniques seen so far to address the three questions."
  },
  {
    "objectID": "live_coding/week_3/analysis_of_fire_data_(complete).html#data-descriptions",
    "href": "live_coding/week_3/analysis_of_fire_data_(complete).html#data-descriptions",
    "title": "Problem Class 3 - Solution",
    "section": "Data Descriptions",
    "text": "Data Descriptions\nWe are provided with two data files for the analysis:\nData/Fires.csv: List of fires recorded by the Utopian Fire Department for 2022\n\nDate: Day and time the fire was reported\nCause: Cause the fire was attributed to (“Cooking”, “Electrical Fault”, “Heating” or “Other”)\nRegisterNumber: Identification number of the property as listed in the register of houses/properties\n\nData/Housing Register.csv: Register of houses/properties for Utopia at the beginning of 2022\n\nID: Identification number of the property\nYear: Year the property was built\nType: Type of property (“Flat”, “Terraced”, “Semi-detached” or “Detached”)\nBedroom: Number of bedrooms (studio apartments are counted as having 1 bedroom)"
  },
  {
    "objectID": "live_coding/week_3/analysis_of_fire_data_(complete).html#number-of-fires-over-time",
    "href": "live_coding/week_3/analysis_of_fire_data_(complete).html#number-of-fires-over-time",
    "title": "Problem Class 3 - Solution",
    "section": "Number of fires over time",
    "text": "Number of fires over time\nWe start by loading the data set “Data/Fires.csv” and converting the variable Date to the correct type:\n\nFires &lt;- read.csv(\"data/fires.csv\" )\nFires &lt;- Fires %&gt;% mutate( Date = ymd_hm( Date ) )\n\nTask 1: Create a facet plot, where each subplot provides a histogram of the number of fires per hour of the day for a different cause.\n\nOne aspect we should consider is whether we use Cartesian or polar coordinates. Both options are reasonable in this case, and we can generate plots with polar coordinates using coord_polar(), as seen in this week’s lecture:\n\n\nggplot( Fires, aes(x=hour(Date) ) ) + facet_wrap(~Cause) + \n  geom_histogram(bins=24) + coord_polar() +\n  labs( x=\"Hour of the Day\", y=\"Number of Fires\" )\n\n\n\n\n\n\n\nTask 2: What do you conclude from your plot produced in Task 1?\n\nWe conclude that\n\n\n\nFires related to cooking are more likely in the morning, around lunch and in the evening, which seems intuitive given that these are the times when people are likely to cook.\n\n\nFor fires caused by electrical faults we see a slightly higher number during daytime than during nighttime, but the differences are relatively small.\n\n\nFires caused by heating have a higher frequency in the morning and evening than for the rest of the day.\n\n\nFinally, for the category “Other”, the highest number of fires is recorded between 9 and 15 o’clock."
  },
  {
    "objectID": "live_coding/week_3/analysis_of_fire_data_(complete).html#fire-risk-across-type-of-property",
    "href": "live_coding/week_3/analysis_of_fire_data_(complete).html#fire-risk-across-type-of-property",
    "title": "Problem Class 3 - Solution",
    "section": "Fire risk across type of property",
    "text": "Fire risk across type of property\nWe now move onto analyzing the risk of fire.\nTask 3: How would you define “risk of fire” in this context?\n\nOne sensible definition is to say that it refers to the probability of a property reporting a fire.\n\nLet’s load the data from the housing register and combine the two data sets\n\nHouses    &lt;- read.csv( \"Data/Housing Register.csv\" )\nFire_Risk &lt;- full_join( Fires, Houses, by=c(\"RegisterNumber\"=\"ID\") )\n\nTask 4: Create a variable to indicate whether a house was affected by a fire in 2022 or not.\n\nAfter joing the data sets, the properties which did not record a fire have a value of \\(\\mathrm{\\texttt{NA}}\\) for the date of the fire. So we can use the Date column and the case_when() function to create this new variable:\n\n\nFire_Risk &lt;- Fire_Risk %&gt;%\n  mutate( Fire = case_when( is.na(Date) == TRUE ~ 0,\n                            is.na(Date) == FALSE ~ 1 )  )\n\nTask 5: For each type of property, calculate the proportion of properties which reported a fire in 2022. What do you conclude?\n\nHaving defined the new variable, we can directly apply the functions group_by() and summarize() we have seen so often:\n\n\nFire_Risk %&gt;% \n  group_by( Type ) %&gt;%\n  summarize( \"Fire Risk\" = round( mean(Fire), 4 ) )\n\n# A tibble: 4 × 2\n  Type          `Fire Risk`\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Detached           0.0079\n2 Flat               0.009 \n3 Semi-detached      0.0079\n4 Terraced           0.0107\n\n\n\nWe find that terraced houses are at the highest risk, followed by flats, with detached and semi-detached houses exhibiting the lowest risk. Let’s consider whether there are differences when we consider the causes of fire for the different types of properties.\n\nTask 6: Explore whether some causes of fire are more frequent for certain types of property.\n\nThis is a tricky question. To answer it, we need to estimate the distribution of causes for each type of property - so we need to focus on the which actually reported a fire. Let’s start by calculating the number of fires per type of property and cause:\n\n\nFire_Type_Cause &lt;- Fire_Risk %&gt;%\n  filter( Fire == 1 ) %&gt;%\n  count( Type, Cause )\nslice_head(Fire_Type_Cause, n=5)\n\n      Type            Cause    n\n1 Detached          Cooking  684\n2 Detached Electrical Fault  499\n3 Detached          Heating 1996\n4 Detached            Other 1115\n5     Flat          Cooking 1204\n\n\n\nTo calculate the proportions, we may use the pivot_wider() function to have all the values for one type of property in a separate row. This then allows us to calculate the proportion. One piece of code which does this is\n\n\nFire_Type_Cause %&gt;%\n  pivot_wider( names_from = Type, values_from = n ) %&gt;%\n  mutate( Detached = round( Detached / sum(Detached), 2), \n          Flat = round(Flat / sum(Flat), 2),\n          `Semi-detached` = round( `Semi-detached` / sum(`Semi-detached`), 2), \n          Terraced = round(Terraced / sum(Terraced),2) )\n\n# A tibble: 4 × 5\n  Cause            Detached  Flat `Semi-detached` Terraced\n  &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Cooking              0.16  0.19            0.17     0.18\n2 Electrical Fault     0.12  0.14            0.12     0.18\n3 Heating              0.46  0.35            0.45     0.35\n4 Other                0.26  0.31            0.26     0.29\n\n\n\nWe see that fires caused by heating are more frequent in detached and semi-detached houses, while electrical faults pose a higher fire risk for terraced houses."
  },
  {
    "objectID": "live_coding/week_3/analysis_of_fire_data_(complete).html#relation-between-risk-of-fire-and-year-built",
    "href": "live_coding/week_3/analysis_of_fire_data_(complete).html#relation-between-risk-of-fire-and-year-built",
    "title": "Problem Class 3 - Solution",
    "section": "Relation between risk of fire and year built",
    "text": "Relation between risk of fire and year built\nTask 7: Create a data graphic to explore the relation between risk of fire and the year a property was built. What do you conclude?\n\nThis is a similar problem to that from the first lecture. While one may be tempted to just plot year built against risk of fire, a much better solution is to also include the type of property in the analysis. So we again create a facet plot:\n\n\nggplot( Fire_Risk, aes(x=Year, y=Fire) ) + \n  facet_wrap(~Type) + geom_smooth() +\n  labs( x=\"Year Built\", y=\"Fire Risk\" )\n\n\n\n\n\n\n\n\nWe see that newer flats, detached and semi-detached properties are at a higher risk than their older counterparts, with the risk being twice as high when considering detached and semi-detached houses. For terraced houses, recently build houses are at about half the risk of the very old terraced houses."
  },
  {
    "objectID": "live_coding/week_1/analysis_of_airbnb_data_(complete).html",
    "href": "live_coding/week_1/analysis_of_airbnb_data_(complete).html",
    "title": "Problem Class 1 - Solution",
    "section": "",
    "text": "Overview and Aims\nWe have seen a few short examples in this week’s lectures that illustrated the application of some functions in the dplyr R package. In this problem class we now study a more complex data set. We will also consider a few functions we did not cover so far, but which are useful in a wide range of applications.\nIn April 2018, all the information for properties listed on the online platform Airbnb for Rio de Janeiro, Brazil, was extracted. The data set is stored in the file “Data/Airbnb Rio April 2018.csv” on Moodle and contains six variables:\n\nhost_id - ID of the host on Airbnb\nhost_since - Date the host started to list their property on Airbnb\nneighbourhood - Neighbourhood where the property is located\nguests_included - Number of people included in the price\nprice - Price per night\nreview_scores_rating - Average rating for the property\n\nOur aim is to address two research questions using the methods we covered so far:\n\nWhich neighbourhoods have a high number of listed properties?\nHow does the price one would expect to pay vary across neighbourhoods?\n\nBefore starting the exercise, let’s load the dplyr R package:\n\nlibrary( dplyr )\n\nData Cleaning\nWe start by loading the data into our R Workspace using the read.csv() function and looking at the loaded data,\n\nAirbnb_raw &lt;- read.csv( \"Data/Airbnb Rio April 2018.csv\", header=TRUE )\nglimpse( Airbnb_raw )\n\nRows: 39,743\nColumns: 6\n$ host_id              &lt;int&gt; 53598, 68997, 99249, 102840, 135635, 153232, 1536…\n$ host_since           &lt;chr&gt; \"12/11/2009\", \"08/01/2010\", \"26/03/2010\", \"03/04/…\n$ neighbourhood        &lt;chr&gt; \"Botafogo\", \"Copacabana\", \"Ipanema\", \"Copacabana\"…\n$ guests_included      &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 1, 1, 7, 1, 8, 2, 4, 4, 1, 2…\n$ price                &lt;chr&gt; \"$133.00\", \"$270.00\", \"$222.00\", \"$161.00\", \"$222…\n$ review_scores_rating &lt;int&gt; 91, 93, 95, 94, 96, 94, 97, NA, 80, 87, 97, 80, 9…\n\n\nTask 1: Which variables are required for addressing the research questions? Define a data frame Airbnb which only contains the necessary variables.\n\nThe two research questions ask us to analyse the number of listed properties and their prices. As such, we require at least the information on the neighbourhood where a property is located, as well as, the price, that is the variables neighbourhood and price. We also keep the number of guests included as it may be more informative to consider price per guest rather than price per property. The other three variables are less important and we thus remove them:\n\n\nAirbnb &lt;- Airbnb_raw %&gt;% \n  select( neighbourhood, guests_included, price )\n\nTask 2: Convert the remaining variables to the correct type. Hint: You may want to consider the function gsub() to remove special symbols from a string of characters.\n\nThe only variable we need to convert is price. However, this conversion is tricky, because we have to first remove the dollar sign and comma. This is where we use gsub(), which allows us to remove these special symbols from the strings before we use as.numeric() to convert them to numerical values:\n\n\n\n\nAirbnb &lt;- Airbnb %&gt;%\n  mutate( price = gsub(pattern=\"\\\\$\", replacement=\"\", price) ) %&gt;%\n  mutate( price = gsub(pattern=\",\", replacement=\"\", price) ) %&gt;%\n  mutate( price = as.numeric(price) )\n\nResearch Question 1\nWith the data now being ready, we can turn to our first research question:\nTask 3: Use the functions in the dplyr package to extract the number of properties per neighbourhood. Hint: You may want to consider the function n() which returns the number of rows in a data frame when combined with summarise(). Alternatively, you can also look up the function count().\n\nOne approach is to group observations using the variable neighbourhood and to then count the number of properties in each group. This requires us to use the functions group_by() and summarise(), with the new function n() being used to extract the number of properties:\n\n\n\n\nAirbnb_Numbers &lt;- Airbnb %&gt;% \n  group_by( neighbourhood ) %&gt;%\n  summarise( Number = n() )\n\n\nThe same result can be achieved with count():\n\n\n\n\nAirbnb_Numbers &lt;- Airbnb %&gt;% \n  count( neighbourhood ) %&gt;%\n  rename( Number = n )\n\n\nThe final line with rename() just changes the name of the variable representing the counts per neighbourhood - count() stores the counts as a variable named n (which is arguably an uninformative variable name we may want to change).\n\n\n\nTask 4: Sort the neighbourhoods based on the number of listed properties. For all neighbourhoods with more than 1000 listed properties, print the name of the neighbourhood and the number of listed properties. What do we conclude?\n\nWe now need the function arrange() to sort observations. Further, we require the function filter() to only extract the neighbourhoods with more than 1000 listed properties. We combine these operations using the pipe:\n\n\n\n\nAirbnb_Numbers %&gt;% \n  arrange( desc(Number) ) %&gt;%\n  filter( Number &gt; 1000 )\n\n              neighbourhood Number\n1                Copacabana   9806\n2           Barra da Tijuca   4528\n3                   Ipanema   3351\n4               Jacarepaguá   2382\n5  Recreio dos Bandeirantes   2096\n6                  Botafogo   1930\n7                    Leblon   1816\n8              Santa Teresa   1312\n9                    Centro   1049\n10                 Flamengo   1035\n\n\n\nThe R output shows that there were 10 neighbourhoods in Rio de Janeiro in April 2018 with more than 1,000 listed properties on Airbnb. The most properties, by quite a distance, are located in the Copacabana neighbourhood, followed by Barra da Tijuca and Ipanema. A look at the map of Rio de Janeiro reveals that these three neighbourhoods are located in the south/south-east of Rio and at the coast (which presumably makes them attractive to tourists).\n\n\nResearch Question 2\nThis research question is a bit more open-ended than Research Question 1. So we should first consider which information may be best suited for addressing the research question.\nOne possible answer is to study price per guest, as properties that can house a larger number of guests are likely to be more expensive. Summaries that may be helpful to extract are the average and median price per guest for a listed property within the neighbourhood.\nTask 5: Why may it be good to also consider the median, instead of just the mean?\n\nProperty prices tend to be skewed, where we have a small number of properties with a very high price. The median is more robust to these outliers than the mean and thus provides a better picture of the price guests likely have to pay.\n\n\n\n\nWe can also illustrate this skewness by creating a histogram:\n\n\n\n\nhist( Airbnb$price, xlab=\"Price in US Dollars\", \n      main=\"Histogram of price per property\",\n      xlim=c(0,3000), breaks = 300 )\n\n\n\n\n\n\n\n\nWe see that the majority of prices lie below 500 US Dollars, but we also have a few listed properties with a price in excess of 1000 US Dollars. There is even a property with a price of $40000, which is not shown in the plot.\n\n\n\nTask 6: Create a data frame which provides the number of listed properties, and the average and median price per guest, for each neighbourhood with more than 1000 listed properties. What do we conclude?\n\nWe have to combine multiple operations. The first is to calculate the price per guest. Next, we again group observations based on their neighbourhood and calculate the summaries for each neighbourhood. Finally, we have to output the values for the neighbourhoods with more than 1000 listed properties. All these steps can be combined using the pipe: \n\n\n\n\nAirbnb %&gt;%\n  mutate( Price_per_Guest = price / guests_included ) %&gt;%\n  group_by( neighbourhood ) %&gt;%\n  summarise( Number=n(),\n             Mean_Price_per_Guest = mean(Price_per_Guest),\n             Median_Price_per_Guest = quantile(Price_per_Guest, 0.5)\n             ) %&gt;%\n  filter( Number &gt; 1000 ) %&gt;%\n  arrange( desc(Median_Price_per_Guest) )\n\n# A tibble: 10 × 4\n   neighbourhood            Number Mean_Price_per_Guest Median_Price_per_Guest\n   &lt;chr&gt;                     &lt;int&gt;                &lt;dbl&gt;                  &lt;dbl&gt;\n 1 Jacarepaguá                2382                 725.                   360 \n 2 Barra da Tijuca            4528                 897.                   349 \n 3 Recreio dos Bandeirantes   2096                 668.                   332 \n 4 Leblon                     1816                 599.                   301 \n 5 Ipanema                    3351                 496.                   249 \n 6 Copacabana                 9806                 360.                   180.\n 7 Botafogo                   1930                 331.                   167.\n 8 Flamengo                   1035                 356.                   151 \n 9 Santa Teresa               1312                 344.                   130 \n10 Centro                     1049                 252.                   120 \n\n\n\nThe R output shows that Jacarepaguá and Barra da Tijuca command the highest median and average price per guest respectively. We also see that there is quite a range in the median value, with the median value for Jacarepaguá being three times that of Centro. Finally, we note the marked differences between median and mean price per guest. While the ranking does not change too much when we consider the average price, these values would be less representative of the price a visitor may be expected to pay."
  }
]