% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  letterpaper,
]{book}
\usepackage{xcolor}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 
\usepackage[]{natbib}
\bibliographystyle{plainnat}


\usepackage{booktabs}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={MA22019 Introduction to Data Science - Lecture Notes},
  pdfauthor={Dr Christian Rohrbeck},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}


\title{MA22019 Introduction to Data Science - Lecture Notes}
\author{Dr Christian Rohrbeck}
\date{2025-02-01}
\begin{document}
\frontmatter
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}

\mainmatter
\bookmarksetup{startatroot}

\chapter{MA22019 Introduction to Data Science - Lecture
Notes}\label{ma22019-introduction-to-data-science---lecture-notes}

\bookmarksetup{startatroot}

\chapter*{Overview}\label{overview}
\addcontentsline{toc}{chapter}{Overview}

\markboth{Overview}{Overview}

\subsection*{Content}\label{content}
\addcontentsline{toc}{subsection}{Content}

In practice we often have to deal with large and complex data sets
(\textbf{``big data''}) - many organisations and companies have
collected vast amounts of data over the last decade. Analyzing such data
and extracting valuable insights from it lies at the heart of data
science.

Although there exists no unique definition, the consensus is that data
science is interdisciplinary and requires combining expertise across
mathematics, computer science and the area of application. As such, we
as mathematicians need to have at least some understanding of the
context in which the data were collected / are analyzed in order to
extract the important information and to communicate effectively with
other disciplines.

In this course we will focus on four key areas of data science:

\begin{itemize}
\item
  Data wrangling
\item
  Data visualization
\item
  Text data analysis
\item
  Spatial data analysis
\end{itemize}

Throughout this unit we will usually work with real-world data sets
which you can download from the MA22019 Moodle page.

\subsection*{Aims and Objectives}\label{aims-and-objectives}
\addcontentsline{toc}{subsection}{Aims and Objectives}

After taking this unit, you should be able to:

\begin{itemize}
\item
  Demonstrate knowledge of data science techniques used for data
  wrangling and visualization.
\item
  Use R for data visualization and wrangling.
\item
  Show awareness of the applications of these methods.
\item
  Understand some of the mathematical models underlying spatial and text
  data analysis.
\item
  Apply these methods in R to analyse large and complex data sets. This
  includes demonstrating a practical and critical approach to data
  analysis, including the ability to select a suitable data science
  method, explaining your reason for choosing it, and correctly
  interpreting the results.
\end{itemize}

\subsection*{Prerequisites}\label{prerequisites}
\addcontentsline{toc}{subsection}{Prerequisites}

You should be familiar with the basics of R introduced in the
Probability and Statistics component of MA12003. On Moodle, you can find
the ``Brief introduction to R'' provided by Professor Jennison in case
you are unsure about certain aspects.

This course will also use some of the fundamentals introduced in Year 1,
including the various probability distributions, and the concepts of
mean, variance and correlation.

\subsection*{Assessment}\label{assessment}
\addcontentsline{toc}{subsection}{Assessment}

\subsubsection*{Summative Assessment}\label{summative-assessment}
\addcontentsline{toc}{subsubsection}{Summative Assessment}

Your mark for MA22019 is based on two individual pieces of coursework:

\begin{itemize}
\item
  \textbf{Coursework 1: 40\% of unit mark.} Set at the end of Week 3 and
  due at the end of Week 4. This coursework will focus on data wrangling
  and data visualization.
\item
  \textbf{Coursework 2: 60\% of unit mark.} Set at the end of Week 10
  and due at the start of Revision Week. This coursework will focus on
  text and spatial data analysis, but you will also have to demonstrate
  your ability to use data wrangling and data visualization techniques.
\end{itemize}

You have to submit your solutions to the coursework via Moodle.

\subsubsection*{Formative Assessment}\label{formative-assessment}
\addcontentsline{toc}{subsubsection}{Formative Assessment}

Problem sheets will be set at the beginning of the week (except for
Weeks 4, 10 and 11) and include:

\begin{description}
\item[\textbf{Exercises:}]
Focus more on the programming aspects of the course and help you to
revise the content covered in that week's lectures. You can submit your
solutions to most questions via a Moodle quiz and you will receive
direct feedback.
\item[\textbf{Tutorial questions:}]
To be be attempted in the weekly tutorials. Solutions will be made
available on Tuesday evening after the last tutorial has finished.
\item[\textbf{Homework question:}]
Open-ended question, similar in style to the coursework questions. Your
answer can be submitted via Moodle to your tutor for feedback. After the
submission deadline, I will provide a solution which demonstrates the
aspects one may consider to address the question (and achieve a
first-class mark), but I won't provide a fully formulated answer - I
want you to practice discussing your results in your own words.
\end{description}

\subsection*{Organisation}\label{organisation}
\addcontentsline{toc}{subsection}{Organisation}

\subsubsection*{Lecture Notes}\label{lecture-notes}
\addcontentsline{toc}{subsubsection}{Lecture Notes}

Lecture notes are available from the MA22019 Moodle page. The notes are
available in two formats (HTML and PDF) with identical content but
different layouts.

\textbf{Important:} While the lecture notes and problem class notes
provide most of the relevant information on the mathematics/statistics
and programming aspects of the course, they do not fully explore (1) how
to decide which plots are best suited for addressing a research question
and (2) how to interpret these plots in the context of the application.
These skills are best trained by looking at a range of data
applications, and we will do so in the lectures and problem classes. The
open-ended question on the problem sheets is designed to support your
learning and to provide additional practice on this topic. As such, you
are strongly encouraged to actively engage with the lectures and problem
sheets.

\subsubsection*{Lectures and Tutorials}\label{lectures-and-tutorials}
\addcontentsline{toc}{subsubsection}{Lectures and Tutorials}

\textbf{In person lecture}: Wednesday 10:15-12:05 (Weeks 1-3,5-9) or
10:15-11:05 (Weeks 4,10,11) in 3WN 2.1

\textbf{Live online learning session (LOIL)}: Friday 14:15-15:05 (Weeks
1-3) on Zoom (link on Moodle)

\textbf{Computer Lab:} You will be assigned to a small group which will
meet weekly to go over the tutorial questions on the problem sheet.
Please check your timetable to identify the time and location of your
tutorial.

\subsection*{Schedule}\label{schedule}
\addcontentsline{toc}{subsection}{Schedule}

The rough outline for the individual weeks is as follows:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Week & Topic & Remarks \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Data Wrangling & Tutorial on RMarkdown \\
2 & Data Wrangling + Data Visualization & Problem Sheet 1 \\
3 & Data Visualization & Problem Sheet 2 \\
& & Release of Coursework 1 \\
4 & Q\&A Coursework 1 & Deadline Coursework 1 \\
5 & Text Data Analysis 1 & No tutorials \\
6 & Text Data Analysis 2 & Problem Sheet 3 \\
7 & Spatial Data Analysis 1 & Problem Sheet 4 \\
8 & Spatial Data Analysis 2 & Problem Sheet 5 \\
9 & Spatial Data Analysis 3 & Problem Sheet 6 \\
10 & Revision Class & Release of Coursework 2 \\
11 & Q\&A Coursework 2 & \\
12 & & Deadline Coursework 2 \\
\end{longtable}

We will finish the core content by the end of Week 9. In Weeks 4, 10 and
11, we will only have Revision and Coursework Q\&A sessions.

\subsection*{R and RStudio}\label{r-and-rstudio}
\addcontentsline{toc}{subsection}{R and RStudio}

We will use R and RStudio throughout this course. You can access RStudio
via the UniDesk, but I would recommend that you install R and RStudio on
your own computer / laptop. If you already have R installed, please
check that you have at least version 4.4.0.

To install all the relevant packages, run the R code in the file
``InstallPackages.R'', which you can find on Moodle. The individual
packages are then loaded using the \textbf{library()} function, e.g.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr) }
\end{Highlighting}
\end{Shaded}

This has to be done every time you start R/RStudio.

To load data from external files, we will use the following two
functions

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{load}\NormalTok{( }\StringTok{"..."}\NormalTok{ )     }\CommentTok{\# To load .RData files}
\FunctionTok{read.csv}\NormalTok{( }\StringTok{"..."}\NormalTok{ ) }\CommentTok{\# To load .csv files}
\end{Highlighting}
\end{Shaded}

\subsection*{Resources}\label{resources}
\addcontentsline{toc}{subsection}{Resources}

This unit is self-contained in the sense that you will not need to read
text books. However, you may wish to consult the following books,
available as ebooks via the University Library, to support your learning
and understanding:

Boehmke, Bradley C. \emph{Data Wrangling with R.} 1st Edition. 2016.
Springer.

Mailund, Thomas. \emph{Beginning Data Science in R : Data Analysis,
Visualization, and Modelling for the Data Scientist.} 1st Edition 2017.
Apress.

Wickham, Hadley. \emph{ggplot2 : Elegant Graphics for Data Analysis.}
2nd Edition. 2016. Springer.

Silge, Julia and Robinson, David. \emph{Text Mining with R: A Tidy
Approach.} 2015. O'Reilly.

\textbf{Remark:} Some examples in these books use R functions that have
been superseded, i.e., it is advised to use an alternative, more recent
function (which may have a different syntax). I will use the most recent
functions in this course.

\bookmarksetup{startatroot}

\chapter{Data Wrangling}\label{data-wrangling}

In practice, \textbf{raw data} (measurements, etc.) may be provided in a
format that hampers its analysis. Common problems include:

\begin{itemize}
\item
  Data types are incompatible with available R functions - data may, for
  instance, be imported as a set of characters, but we require numerical
  values for most R functions;
\item
  Variable names are uninformative - we do not want to always go back
  and look up what the individual variables represent;
\item
  We are only interested in a subset of the data - a study may have
  produced a lot of data, but we want to focus on a specific aspect.
\end{itemize}

\textbf{Data wrangling} is concerned with restructuring the raw data
into a format more useful for the analysis. We also want to extract key
information from the data, i.e, performing some \textbf{data
exploration}.

In this chapter we will explore how we can perform such tasks in R. Most
of the functions we will use are provided by the \textbf{dplyr} R
package (and other R packages where appropriate). Let's load the dplyr
package,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( dplyr )}
\end{Highlighting}
\end{Shaded}

When we import data into R, the data will usually be stored as a
\textbf{data frame}, which corresponds to a matrix, where each column
has a name. The dplyr package is particularly effective at working with
this data format. To access the individual columns for the different
variables in the data frame, we can use the \$ sign, followed by the
name of the column (you will see this syntax at multiple points).

\section{Data cleaning}\label{data-cleaning}

After loading the data, the first step is to check that variables have
the correct data type and decide whether the variable names are
suitable/informative. If the data type is incorrect, we should convert
it, in particular if we want to apply R functions to the data. The most
common conversion we will have to make is from the type
\textbf{character} to the type \textbf{numeric} or \textbf{date}. In
this section, we highlight some of the available R functions for
converting and renaming variables, and illustrate their application
using publicly available river flow data.

\subsection{Converting characters into numerical
values}\label{converting-characters-into-numerical-values}

When loading the data for a numerical variable into R, the individual
values may be stored as strings/words. Possible reasons include, for
instance, that the value 2345.34 is stored as ``2,345.34'' in the data
file (see Problem Class 1) or that missing values are represented via a
letter - once R fails to convert a single entry to a numerical value,
the whole column (variable) is converted to type \textbf{character}.
However, this data type is often not useful because only a few functions
can work with it.

We will introduce the functions \textbf{as.numeric()} and
\textbf{case\_when()} that may be used to convert a character to a
numerical value.

\textbf{Example 1:} Suppose we work with a data set where the letter
``M'' is used to indicate that an observation is missing. A toy example
is provided in the data file ``DataCleaningExample1.csv''. We load this
data and use the function \textbf{glimpse()} in the dplyr package to
print the data type and values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Example1 }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/DataCleaningExample1.csv"}\NormalTok{ )}
\FunctionTok{glimpse}\NormalTok{( Example1 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 8
Columns: 1
$ Value <chr> "1.02", "0.98", "0.79", "M", "2.1", "15.1", "M", "4.2"
\end{verbatim}

We find that the values are stored as characters/words, as indicated by
the data type being \(\mathrm{\texttt{<chr>}}\). In such a situation, we
cannot use the \textbf{mean()} function to derive the average value -
there is no average of a set of words:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{( Example1}\SpecialCharTok{$}\NormalTok{Value )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in mean.default(Example1$Value): argument is not numeric or logical:
returning NA
\end{verbatim}

\begin{verbatim}
[1] NA
\end{verbatim}

Instead, we have to first use the function \textbf{as.numeric()} to
convert the words to numerical values

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Example1}\SpecialCharTok{$}\NormalTok{Value }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{( Example1}\SpecialCharTok{$}\NormalTok{Value )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: NAs introduced by coercion
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{( Example1 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 8
Columns: 1
$ Value <dbl> 1.02, 0.98, 0.79, NA, 2.10, 15.10, NA, 4.20
\end{verbatim}

We see that the data type has changed to \(\mathrm{\texttt{<dbl>}}\),
which is one data type for numerical values. Further, the R output shows
that all entries with the letter ``M'' were converted to
\(\mathrm{\texttt{NA}}\) (not available) - this is R's way to tell us
that the conversion did not work or that a value is missing (which is
exactly what we want here).

With the values converted to the correct type, we can now calculate
their mean:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{( Example1}\SpecialCharTok{$}\NormalTok{Value, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{ ) }\CommentTok{\# na.rm=TRUE to ignore the entries with NA}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4.031667
\end{verbatim}

\textbf{Example 2:} Suppose responses to a survey question were encoded
as ``Y'' (``Yes'') or ``N'' (``No'') and we received the following data
vector:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{responses }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Y"}\NormalTok{,}\StringTok{"Y"}\NormalTok{,}\StringTok{"N"}\NormalTok{,}\StringTok{"Y"}\NormalTok{,}\StringTok{"N"}\NormalTok{,}\StringTok{"Y"}\NormalTok{,}\StringTok{"N"}\NormalTok{,}\StringTok{"N"}\NormalTok{,}\StringTok{"N"}\NormalTok{,}\StringTok{"Y"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Again, we cannot use \(\mathrm{\texttt{mean(responses)}}\) to derive the
proportion of participants who answered with ``Yes'', because the mean()
function requires numerical or logical values.

One common approach to derive the proportion in practice is to encode
the outcomes as numerical values to which the mean() function is then
applied. The dplyr package provides the function \textbf{case\_when()}
which allows us to replace ``Y'' and ``N'' by 1 and 0 respectively:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{responses }\OtherTok{\textless{}{-}} \FunctionTok{case\_when}\NormalTok{( responses }\SpecialCharTok{==} \StringTok{"Y"} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, responses }\SpecialCharTok{==} \StringTok{"N"} \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{ )}
\FunctionTok{mean}\NormalTok{( responses )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5
\end{verbatim}

We find that 50\% of the participants answered the question with
``Yes''. One strength of case\_when() is that we can define as many
cases as we need, there is no limit. The function can also be used to
convert other types of data, and it is not limited to converting a
character into a numerical value.

\textbf{Remark 1:} If you forget to specify a case in case\_when(), the
converted value for any unspecified case will be
\(\mathrm{\texttt{NA}}\) by default. The default option can be changed
and for our our example we could have also used

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{responses }\OtherTok{\textless{}{-}} \FunctionTok{case\_when}\NormalTok{( responses }\SpecialCharTok{==} \StringTok{"Y"} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }\AttributeTok{.default =} \DecValTok{0}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\textbf{Remark 2:} Be aware that case\_when() considers the expressions
sequentially (just as when you are using if, else if and else
statements). The following pieces of code show an example where the
result depends on the order of the conditions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{( }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{40}\NormalTok{ )}
\FunctionTok{case\_when}\NormalTok{( x }\SpecialCharTok{\%\%} \DecValTok{10} \SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, x }\SpecialCharTok{\%\%} \DecValTok{20} \SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \DecValTok{2}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1 1 1
\end{verbatim}

We see that all converted values are all equal to 1. Let's see what
happens when we change the order of the conditions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{case\_when}\NormalTok{( x }\SpecialCharTok{\%\%} \DecValTok{20} \SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \DecValTok{2}\NormalTok{, x }\SpecialCharTok{\%\%} \DecValTok{10} \SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1 2 2
\end{verbatim}

This second result seems more intuitive. Consequently, we should proceed
from the most specific to the most general condition when using
case\_when().

\subsection{Converting characters into
dates}\label{converting-characters-into-dates}

In many studies we are provided with the time the data were observed.
This information is often important in applications and we cannot simply
ignore it. When loading variables representing dates into R, their
values are often stored as strings, such as ``01/10/2022''.

The R package \textbf{lubridate} provides a range of nice functions to
convert data of type \textbf{character} into the data type \textbf{date}
or \textbf{date-time}. For instance, to convert the character
expressions ``01/10/2022'' and ``15/10/2023'', we use the
\textbf{as\_date()} function,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lubridate)}
\NormalTok{date\_observed  }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{( }\StringTok{"01/10/2022"}\NormalTok{, }\StringTok{"15/10/2023"}\NormalTok{ )}
\NormalTok{date\_converted }\OtherTok{\textless{}{-}} \FunctionTok{as\_date}\NormalTok{( date\_observed, }\AttributeTok{format=}\StringTok{"\%d/\%m/\%Y"}\NormalTok{ )}
\FunctionTok{glimpse}\NormalTok{( date\_converted )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 Date[1:2], format: "2022-10-01" "2023-10-15"
\end{verbatim}

We see that the default output format for dates is
\textbf{year-month-day}.

\textbf{Remark 1:} After converting values to \textbf{date}, we can
extract the year and month using the functions \textbf{year()} and
\textbf{month()} respectively. Let's extract the year from the dates in
\textbf{date\_converted}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{year}\NormalTok{( date\_converted )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2022 2023
\end{verbatim}

\textbf{Remark 2:} We can also calculate the difference between dates.
For instance, if we consider the two entries in the vector of converted
dates, we find

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{date\_converted[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ date\_converted[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Time difference of 379 days
\end{verbatim}

So you can now use R to quickly calculate how many days there are left
until the Easter break.

\subsection{Changing variable names}\label{changing-variable-names}

We should avoid using uninformative (or very long) variable names. Let's
generate a data frame with two columns, where each column contains five
samples from a standard normal distribution

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{( }\DecValTok{2025}\NormalTok{ )}
\NormalTok{obs }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{( }\StringTok{"x"}\OtherTok{=}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{5}\NormalTok{, }\AttributeTok{mean=}\DecValTok{0}\NormalTok{, }\AttributeTok{sd=}\DecValTok{1}\NormalTok{), }\StringTok{"y"}\OtherTok{=}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{5}\NormalTok{, }\AttributeTok{mean=}\DecValTok{0}\NormalTok{, }\AttributeTok{sd=}\DecValTok{1}\NormalTok{) )}
\NormalTok{obs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
          x           y
1 0.6207567 -0.16285434
2 0.0356414  0.39711189
3 0.7731545 -0.07998932
4 1.2724891 -0.34496518
5 0.3709754  0.70215136
\end{verbatim}

We may argue that the variable names \textbf{x} and \textbf{y} are
uninformative and should be changed to \textbf{Sample1} and
\textbf{Sample2} respectively. The function \textbf{rename()} in the
dplyr R package allows us to do this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{obs }\OtherTok{\textless{}{-}} \FunctionTok{rename}\NormalTok{( obs, }\StringTok{"Sample1"}\OtherTok{=}\NormalTok{x, }\StringTok{"Sample2"}\OtherTok{=}\NormalTok{y )}
\NormalTok{obs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    Sample1     Sample2
1 0.6207567 -0.16285434
2 0.0356414  0.39711189
3 0.7731545 -0.07998932
4 1.2724891 -0.34496518
5 0.3709754  0.70215136
\end{verbatim}

\subsection{Example: Loading and cleaning NRFA river flow
data}\label{example-loading-and-cleaning-nrfa-river-flow-data}

The National River Flow Archive (www.nrfa.ceh.ac.uk) provides data for
hundreds of sites (gauges) across the UK. We want to analyze daily river
flow data for the River Avon at Bathford. The data are available in the
file ``Bathford River Flow.csv''.

When looking at the data file, we identify two aspects that need to be
taken into account when loading the data into R

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  The first 20 lines are data descriptors (so called \textbf{meta
  data}), while the remaining lines contain the actual data: dates and
  river flow measurements.
\item
  The letter ``M'' appears in the third column whenever the river flow
  measurement is missing in later years.
\end{enumerate}

To ignore the first 20 lines and avoid importing the data file in a
wrong format, we have to use three of the options provided by the
read.csv() function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Bathford\_RF }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/Bathford River Flow.csv"}\NormalTok{, }\AttributeTok{skip=}\DecValTok{20}\NormalTok{, }\AttributeTok{header=}\ConstantTok{FALSE}\NormalTok{,}
                         \AttributeTok{colClasses =} \FunctionTok{c}\NormalTok{(}\StringTok{"character"}\NormalTok{,}\StringTok{"numeric"}\NormalTok{,}\StringTok{"NULL"}\NormalTok{) ) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in read.table(file = file, header = header, sep = sep, quote = quote, :
cols = 2 != length(data) = 3
\end{verbatim}

The option \(\mathrm{\texttt{skip=20}}\) means we ignore the first 20
lines, while
\(\mathrm{\texttt{colClasses= c("character","numeric","NULL")}}\) leads
to the third column being ignored when loading the data (you can ignore
the warning message in this case). Finally, we set
\(\mathrm{\texttt{header=FALSE}}\), because the file does not provide
variable names.

\textbf{Tip:} Have a look at the data set in the data file before trying
to load it. R (in particular recent versions) may load the data into an
incorrect format instead of giving an error. As an example, remove the
option \(\mathrm{\texttt{colClasses=..}}\). You will find that the
number of observations increases, but some of the dates are now listed
as ``M''.

Let's have a look at the imported data using the glimpse() function,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{( Bathford\_RF )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 19,697
Columns: 2
$ V1 <chr> "1969-10-27", "1969-10-28", "1969-10-29", "1969-10-30", "1969-10-31~
$ V2 <dbl> 3.998, 3.958, 4.210, 4.480, 4.205, 3.830, 3.723, 3.986, 4.353, 4.52~
\end{verbatim}

We see that there are 19,697 measurements in the data set. However, the
variable names have to be changed - \textbf{V1} and \textbf{V2} are just
not sensible. Further, we have to convert the dates into the
\textbf{date} format. The river flow measurements are already stored as
numeric values, so no conversion is required.

Let's start by changing the variable names to \textbf{Date} and
\textbf{RiverFlow}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Bathford\_RF }\OtherTok{\textless{}{-}} \FunctionTok{rename}\NormalTok{( Bathford\_RF, }\AttributeTok{Date =}\NormalTok{ V1, }\AttributeTok{RiverFlow =}\NormalTok{ V2 )}
\end{Highlighting}
\end{Shaded}

before converting the variable \textbf{Date} to the correct type,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Bathford\_RF}\SpecialCharTok{$}\NormalTok{Date }\OtherTok{\textless{}{-}} \FunctionTok{as\_date}\NormalTok{( Bathford\_RF}\SpecialCharTok{$}\NormalTok{Date, }\AttributeTok{format=}\StringTok{"\%Y{-}\%m{-}\%d"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

With the data having been \textbf{cleaned}, i.e., they have the correct
type and informative names, we can start the analysis. As a first step,
it is good practice to report the proportion of missing data of a
variable. We can extract the proportion of missing river flow
measurements using the functions mean() and \textbf{is.na()}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{( }\FunctionTok{is.na}\NormalTok{( Bathford\_RF}\SpecialCharTok{$}\NormalTok{RiverFlow ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.006244606
\end{verbatim}

We find that river flow measurements are missing on about 0.62\% of
dates and this should be reported.

\textbf{Remark:} Missing data is important when building models. In this
course, you are only expected to state the proportion of missing data.
The handling of missing data will be considered in more detail in the
Year 3 unit MA32022 Statistical Modelling and Data Analytics 3A.

Now that the data frame is in a much better format, we can plot it using
the function \textbf{plot()} covered in Year 1 Probability \&
Statistics:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{( Bathford\_RF}\SpecialCharTok{$}\NormalTok{Date, Bathford\_RF}\SpecialCharTok{$}\NormalTok{RiverFlow, }\AttributeTok{type=}\StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{,}
      \AttributeTok{xlab=}\StringTok{"Date"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"River Flow"}\NormalTok{, }\AttributeTok{cex.lab =} \FloatTok{1.5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.62\linewidth,height=\textheight,keepaspectratio]{01-DataWrangling_files/figure-pdf/Bathford-1.pdf}

}

\caption{River flow measurements at Bathford for 27 October 1969 to 30
September 2023.}

\end{figure}%

\textbf{Which of the following conclusions should we report when asked
to comment on the frequency of river flow levels above 100m\(^3\)/s and
the magnitude of river flow levels?}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Recorded river flow levels were as high as approximately 250m\(^3\)/s.
\item
  The data exhibits seasonality, with river flow levels being higher in
  winter than in summer.
\item
  The data covers the years 1969 to 2023.
\item
  There is at least one day with river flow levels exceeding
  100m\(^3\)/s for most years.
\end{enumerate}

\section{Working with a single data
frame}\label{working-with-a-single-data-frame}

We now study a range of aspects that frequently come up during the data
wrangling process:

\begin{description}
\item[\textbf{Selecting subsets of observations and variables:}]
When working with a large data set, not all variables and observations
may be relevant. So we may want to reduce the size of our data set and
only keep the observations required for our analysis. This process can
happen before or after the data cleaning.
\item[\textbf{Deriving new variables from existing data:}]
It may be useful to create new variables which we believe to be
interesting to explore in our analysis. These new variables should be
stored in the same data frame as the other variables.
\item[\textbf{Summarizing the data:}]
As one of the first steps in the data exploration, we should derive
summaries of the different variables to gain a better understanding of
the data. For instance, as highlighted in Section
\hyperref[example-loading-and-cleaning-nrfa-river-flow-data]{1.1.4}, the
proportion of missing data is one useful summary.
\item[\textbf{Sorting the data:}]
In applications, interest may lie in extracting the smallest/largest
observations and/or providing a ranking. As such, we need to be able to
sort observations based on one, or more, criteria.
\end{description}

In this section we explore how to perform these operations using the
dplyr package. In Problem Class 1 we will use the considered techniques
to analyze a relatively large data set from Brazil. Other aspects of
data exploration will be discussed in the next chapters.

\textbf{Tip:} All steps of the data wrangling / exploration process
should be placed in an R (or R Markdown) script, so that we can make
modifications quickly if something needs to be changed. It's also good
practice to keep the raw data available in your R Workspace. In the
following examples we never replace the raw data.

\subsection{Filtering observations}\label{filtering-observations}

In an analysis we may only want to focus on a subset of the data. For
instance, when modelling the risk of flooding, we are mostly interested
in the extremely high river flow measurements.

The function \textbf{filter()} is useful in such cases. Suppose we
classified a river flow exceeding 100 m\(^3\)/s at the gauge of Bathford
in Section
\hyperref[example-loading-and-cleaning-nrfa-river-flow-data]{1.1.4} as
extremely high. We can then extract the subset of observations exceeding
100m\(^3\)/s using filter(), and we use \textbf{slice\_head()} to print
the first five observations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Bathford\_RF\_High }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{( Bathford\_RF, RiverFlow }\SpecialCharTok{\textgreater{}} \DecValTok{100}\NormalTok{ )}
\FunctionTok{slice\_head}\NormalTok{( Bathford\_RF\_High, }\AttributeTok{n=}\DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Date RiverFlow
1 1970-11-19     104.3
2 1971-01-21     128.4
3 1971-01-22     114.8
4 1971-01-23     115.2
5 1971-01-24     133.1
\end{verbatim}

The function filter() can also handle multiple conditions. For instance,
we can extract the days across the period 1991-2023 when the river flow
exceeded 100m\(^3\)/s using

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Bathford\_RF\_High }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{( Bathford\_RF, RiverFlow }\SpecialCharTok{\textgreater{}} \DecValTok{100}\NormalTok{, }\FunctionTok{year}\NormalTok{(Date) }\SpecialCharTok{\textgreater{}} \DecValTok{1990}\NormalTok{ )}
\FunctionTok{slice\_head}\NormalTok{( Bathford\_RF\_High, }\AttributeTok{n=}\DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Date RiverFlow
1 1991-01-10     112.1
2 1992-11-26     106.9
3 1992-11-27     100.9
4 1992-11-28     100.6
5 1992-11-29     131.6
\end{verbatim}

\subsection{Selecting variables}\label{selecting-variables}

Not all variables in a data set may be of interest to us. For instance,
meteorological data sets often provide measurements for multiple weather
variables, but we may only need to analyze precipitation and
temperature.

\textbf{Example:} Let's consider the data set ``Tuscany.csv'' which
provides information on the population in Tuscany, Italy, for 2020:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tuscany\_raw }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/Tuscany.csv"}\NormalTok{ )}
\FunctionTok{slice\_head}\NormalTok{( Tuscany\_raw, }\AttributeTok{n=}\DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Year Postal_Code  Town Province Age Men Women
1 2020       45001 Aulla       MS   0  32    30
2 2020       45001 Aulla       MS   1  30    34
3 2020       45001 Aulla       MS   2  43    39
4 2020       45001 Aulla       MS   3  50    35
5 2020       45001 Aulla       MS   4  38    29
\end{verbatim}

Suppose we only want to compare the population data for the different
provinces and towns. As such, we don't need the variables \textbf{Year},
since all data are from 2020, and \textbf{Postal\_Code}. Let's look at
two possible options to achieve this using the \textbf{select()}
function in dplyr.

The first option is to specify the variables we want to keep

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tuscany }\OtherTok{\textless{}{-}} \FunctionTok{select}\NormalTok{( Tuscany\_raw, Town}\SpecialCharTok{:}\NormalTok{Women )}
\FunctionTok{slice\_head}\NormalTok{( Tuscany, }\AttributeTok{n=}\DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Town Province Age Men Women
1 Aulla       MS   0  32    30
2 Aulla       MS   1  30    34
3 Aulla       MS   2  43    39
4 Aulla       MS   3  50    35
5 Aulla       MS   4  38    29
\end{verbatim}

Here, the colon sign indicates that we want to keep all columns from
\textbf{Town} to \textbf{Women}.

The second option is to specify the variables to be excluded using the
minus sign,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tuscany }\OtherTok{\textless{}{-}} \FunctionTok{select}\NormalTok{( Tuscany\_raw, }\SpecialCharTok{{-}}\NormalTok{Year, }\SpecialCharTok{{-}}\NormalTok{Postal\_Code )}
\end{Highlighting}
\end{Shaded}

Whether we specify the variables to be kept, or the variables to be
removed, really depends on the number of variables to be included (or
excluded) - we want to write as little code as possible.

\subsection{Creating and attaching new
variables}\label{creating-and-attaching-new-variables}

When analyzing real-world data, it may be useful to create new variables
which we believe to be interesting to explore. For instance, for the
population from Tuscany, we may want to calculate the total population
for each age group and town, and attach this information as a new
variable to the data frame.

The \textbf{mutate()} function in the dplyr package is really useful in
such situations, as we can produce and directly attach a new variable
\textbf{Population} using

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tuscany }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{( Tuscany, }\AttributeTok{Population =}\NormalTok{ Men }\SpecialCharTok{+}\NormalTok{ Women )}
\FunctionTok{slice\_head}\NormalTok{( Tuscany, }\AttributeTok{n=}\DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Town Province Age Men Women Population
1 Aulla       MS   0  32    30         62
2 Aulla       MS   1  30    34         64
3 Aulla       MS   2  43    39         82
4 Aulla       MS   3  50    35         85
5 Aulla       MS   4  38    29         67
\end{verbatim}

We see that mutate() requires us to provide a new variable name and to
define how the values of this new variable are to be derived. Note, the
function mutate() can also be used to attach values stored in another R
object to the data frame.

\textbf{Important:} If you use a variable name that already exists
within the data frame, mutate() will overwrite this column with the new
values - so we can also use mutate() to modify the columns in your data
frame.

\subsection{Combining multiple operations - the
pipe}\label{combining-multiple-operations---the-pipe}

We have already introduced quite a few useful functions for data
cleaning and wrangling. Let's now consider the case that we want to
combine these functions. For instance, we may want to derive the
population per age group and town, and then remove the variables
\textbf{Year} and \textbf{Postal\_Code} from the original data frame.

\textbf{How can we do this?}

The first option is to manipulate the data step by step and to always
store the R object after finishing one operation (similar to what we
have done so far). This would be implemented as

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tuscany }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{( Tuscany\_raw, }\AttributeTok{Population =}\NormalTok{ Men }\SpecialCharTok{+}\NormalTok{ Women )}
\NormalTok{Tuscany }\OtherTok{\textless{}{-}} \FunctionTok{select}\NormalTok{( Tuscany, }\SpecialCharTok{{-}}\NormalTok{Year, }\SpecialCharTok{{-}}\NormalTok{Postal\_Code )}
\end{Highlighting}
\end{Shaded}

This is quite a bit of code, because we have to type
\(\mathrm{\texttt{Tuscany}}\) in each line.

\textbf{Can we do better?}

Well, we could place all the operations into a single line

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tuscany }\OtherTok{\textless{}{-}} \FunctionTok{select}\NormalTok{( }\FunctionTok{mutate}\NormalTok{( Tuscany\_raw, }\AttributeTok{Population =}\NormalTok{ Men }\SpecialCharTok{+}\NormalTok{ Women ), Town}\SpecialCharTok{:}\NormalTok{Population )}
\end{Highlighting}
\end{Shaded}

However, such an approach may quickly lead to a large number of
brackets, which increases the risk of frustrating syntax errors -
remember this may only be the start of our analysis.

Luckily, we can avoid both these two options by using the \textbf{pipe
command} \textbf{\%\textgreater\%} in the dplyr R package. The same
commands as above would be implemented as

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tuscany }\OtherTok{\textless{}{-}}\NormalTok{ Tuscany\_raw }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{Population =}\NormalTok{ Men }\SpecialCharTok{+}\NormalTok{ Women ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{( }\SpecialCharTok{{-}}\NormalTok{Year, }\SpecialCharTok{{-}}\NormalTok{Postal\_Code )}
\FunctionTok{slice\_head}\NormalTok{( Tuscany, }\AttributeTok{n=}\DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Town Province Age Men Women Population
1 Aulla       MS   0  32    30         62
2 Aulla       MS   1  30    34         64
3 Aulla       MS   2  43    39         82
4 Aulla       MS   3  50    35         85
5 Aulla       MS   4  38    29         67
\end{verbatim}

The operations are executed from top to bottom: We take the data frame
\textbf{Tuscany\_raw}, then apply the mutate() function to create the
column \textbf{Population}, and conclude by removing the columns
\textbf{Year} and \textbf{Postal\_Code} from the created data frame
using the select() function.

\textbf{Tip:} Combining multiple R commands can be tricky at first. If
you are unsure, try to outline the way you want to manipulate the data
before starting to implement it in R.

\subsection{Summarizing the data}\label{summarizing-the-data}

For large data sets, we usually want to provide data summaries. For
instance, one important summary for the Tuscany data set may be the
total number of people within the data. In such situations, we can apply
functions such as \textbf{sum()} directly

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{( Tuscany}\SpecialCharTok{$}\NormalTok{Population )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3691409
\end{verbatim}

If we want to extract several such summaries, we can either derive each
summary individually, or use the \textbf{summarize()} function in the
dplyr R package. Let's also extract the proportion of men and women

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tuscany }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{( }\StringTok{"Population\_Tuscany\_2020"} \OtherTok{=} \FunctionTok{sum}\NormalTok{( Population ),}
             \StringTok{"Men\_Tuscany\_2020"} \OtherTok{=} \FunctionTok{sum}\NormalTok{( Men ),}
             \StringTok{"Women\_Tuscany\_2020"} \OtherTok{=} \FunctionTok{sum}\NormalTok{( Women ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Population_Tuscany_2020 Men_Tuscany_2020 Women_Tuscany_2020
1                 3691409          1787649            1903760
\end{verbatim}

The summarize() function really starts to shine when we combine it with
the \textbf{group\_by()} function.

Suppose we wanted the population numbers for each of the provinces,
which requires us to sum up the numbers across towns and age groups
while accounting for the variable \textbf{Province}. We can do this
using group\_by() and summarize():

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tuscany\_Province }\OtherTok{\textless{}{-}}\NormalTok{ Tuscany }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( Province ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{( }\AttributeTok{Total =} \FunctionTok{sum}\NormalTok{(Population) )}
\NormalTok{Tuscany\_Province}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 2
   Province  Total
   <chr>     <int>
 1 AR       336450
 2 FI       997940
 3 GR       217803
 4 LI       328855
 5 LU       383688
 6 MS       189786
 7 PI       417799
 8 PO       265153
 9 PT       290177
10 SI       263758
\end{verbatim}

The group\_by() function splits the data subject to the specified
variable (\textbf{Province} in this case) and, for each subset, the
summarize() function then derives the population total.

\textbf{Remark:} We can specify multiple variables in group\_by() to
define the subgroups based on several criteria.

Let's consider a slightly more complicated task. Suppose we were asked
to study the age profile of women within the population. To extract the
proportion of women of a certain age, we need to group women by
\textbf{Age}, but also keep track of the total number of women within
the population. One possible way to extract the proportions is as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tuscany\_Women\_Age }\OtherTok{\textless{}{-}}\NormalTok{ Tuscany }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( Age ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{( }\AttributeTok{Number =} \FunctionTok{sum}\NormalTok{(Women) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{Proportion =}\NormalTok{ Number }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(Number) )}
\end{Highlighting}
\end{Shaded}

Note, we used the fact that the summarize() function returns a data
frame, and thus we can perform further operations. Finally, let's
illustrate the calculated proportions using a bar plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{barplot}\NormalTok{( Proportion}\SpecialCharTok{\textasciitilde{}}\NormalTok{Age, }\AttributeTok{data=}\NormalTok{Tuscany\_Women\_Age )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{01-DataWrangling_files/figure-pdf/unnamed-chunk-34-1.pdf}

}

\caption{Age profile of living women in Tuscany for the year 2020.}

\end{figure}%

We see that the highest proportions are observed for ages 40-70. The
lower proportions for younger ages reflect the decrease in birth rates
recorded for many countries over the past years. The decreasing
proportion beyond 70 is presumably due to an increased rate of mortality
for these age groups.

\subsection{Sorting the data frame based on a
variable}\label{sorting-the-data-frame-based-on-a-variable}

You may have already seen the \textbf{sort()} command, which allows you
to order the values within a vector. When we consider a data frame, we
may want to sort its rows subject to the values in one of the columns.
For instance, we may want to sort provinces based on their population.

The function \textbf{arrange()} in the dplyr R package does exactly this
job,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{arrange}\NormalTok{( Tuscany\_Province, Total )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 2
   Province  Total
   <chr>     <int>
 1 MS       189786
 2 GR       217803
 3 SI       263758
 4 PO       265153
 5 PT       290177
 6 LI       328855
 7 AR       336450
 8 LU       383688
 9 PI       417799
10 FI       997940
\end{verbatim}

We see that ``FI'' (Firenze) has the highest population among the
provinces in Tuscany. Further, the output demonstrates that the default
setting for arrange() is to sort the values in ascending order. Should
we want to sort values in descending order, we have to use the
additional command \textbf{desc()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tuscany\_Province }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{arrange}\NormalTok{( }\FunctionTok{desc}\NormalTok{(Total) )}
\end{Highlighting}
\end{Shaded}

\textbf{Remark:} If two observations have the same value, they are
listed in their original order, regardless of whether we sort in
ascending or descending order. If we want to change this (which we
sometimes want), we can specify a second variable in arrange(), just as
for group\_by().

\section{Working with multiple data
sets}\label{working-with-multiple-data-sets}

So far we have focused on analyzing a single data file. In many
applications, however, data is stored across multiple data files. For
instance, we may have one data file containing weather data and another
data file providing insurance data related to weather-related damages.
In these cases, we want to combine the different data files into a
single data frame for our analysis.

The dplyr R package provides the functions \textbf{inner\_join()},
\textbf{left\_joint()}, \textbf{right\_join()} and \textbf{full\_join()}
to combine data frames based on a ``key''. All these functions combine
two data frames and their application is illustrated via an example in
Section \href{Merging\%20two\%20data\%20sets}{1.3.1}.

When working with multiple data sets, we may also want to automate the
process. Imagine you had weather measurements for over 100 sites - you
do not really want to spend hours just to merge the data frames. This
aspect is considered in Section
\href{Merging\%20multiple\%20data\%20sets}{1.3.2}.

\subsection{Merging two data sets}\label{merging-two-data-sets}

In Section
\hyperref[example-loading-and-cleaning-nrfa-river-flow-data]{1.1.4}, we
focused on the river flow data collected at Bathford. The National River
Flow Archive provides data for another gauge located to the west of Bath
city centre; you can find the data file ``Bath River Flow.csv'' on
Moodle. Our aim is to combine the river flow measurements into a single
data frame.

We start by again loading the data for Bathford and renaming the
variables,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Bathford\_RF }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/Bathford River Flow.csv"}\NormalTok{, }\AttributeTok{skip=}\DecValTok{20}\NormalTok{, }\AttributeTok{header=}\ConstantTok{FALSE}\NormalTok{,}
                         \AttributeTok{colClasses =} \FunctionTok{c}\NormalTok{(}\StringTok{"character"}\NormalTok{,}\StringTok{"numeric"}\NormalTok{,}\StringTok{"NULL"}\NormalTok{) )}
\NormalTok{Bathford\_RF }\OtherTok{\textless{}{-}} \FunctionTok{rename}\NormalTok{( Bathford\_RF, }\AttributeTok{Date =}\NormalTok{ V1, }\AttributeTok{RiverFlow =}\NormalTok{ V2 )}
\end{Highlighting}
\end{Shaded}

A closer look at data file for the Bath gauge suggests that the data
format is similar to that for Bathford. The only difference is that we
now have to ignore the first 19 instead of the first 20 lines:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Bath\_RF }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/Bath River Flow.csv"}\NormalTok{, }\AttributeTok{skip=}\DecValTok{19}\NormalTok{, }\AttributeTok{header=}\ConstantTok{FALSE}\NormalTok{,}
                     \AttributeTok{colClasses =} \FunctionTok{c}\NormalTok{(}\StringTok{"character"}\NormalTok{,}\StringTok{"numeric"}\NormalTok{,}\StringTok{"NULL"}\NormalTok{) )}
\NormalTok{Bath\_RF }\OtherTok{\textless{}{-}} \FunctionTok{rename}\NormalTok{( Bath\_RF, }\AttributeTok{Date =}\NormalTok{ V1, }\AttributeTok{RiverFlow =}\NormalTok{ V2 )}
\end{Highlighting}
\end{Shaded}

Let's investigate the first element in each data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Bath\_RF }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{slice\_head}\NormalTok{( }\AttributeTok{n=}\DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Date RiverFlow
1 1976-09-01      3.39
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Bathford\_RF }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{slice\_head}\NormalTok{( }\AttributeTok{n=}\DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Date RiverFlow
1 1969-10-27     3.998
\end{verbatim}

We see that the two gauges started operating in different years - Bath
in 1976 and Bathford in 1969. So the number of rows in the two data
frames is different.

When combining the two data frames, we want to match observations based
on the variable \textbf{Date}, this is our ``key''. Here we use the
function full\_join(), which ensures that all observations for Bath and
Bathford are contained in the combined data set, and we specify that
observations should be matched based on the variable \textbf{Date},

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RF }\OtherTok{\textless{}{-}}\NormalTok{ Bathford\_RF }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{full\_join}\NormalTok{( Bath\_RF, }\AttributeTok{by=}\FunctionTok{c}\NormalTok{(}\StringTok{"Date"} \OtherTok{=} \StringTok{"Date"}\NormalTok{) )}
\FunctionTok{glimpse}\NormalTok{( RF )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 19,697
Columns: 3
$ Date        <chr> "1969-10-27", "1969-10-28", "1969-10-29", "1969-10-30", "1~
$ RiverFlow.x <dbl> 3.998, 3.958, 4.210, 4.480, 4.205, 3.830, 3.723, 3.986, 4.~
$ RiverFlow.y <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA~
\end{verbatim}

We see that the values for the first dates are correctly identified as
being missing for Bath - the gauge was not in operation at the time. We
are left with changing the variable names and converting the data type
of \textbf{Date}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RF}\SpecialCharTok{$}\NormalTok{Date }\OtherTok{\textless{}{-}} \FunctionTok{as\_date}\NormalTok{( RF}\SpecialCharTok{$}\NormalTok{Date, }\AttributeTok{format=}\StringTok{"\%Y{-}\%m{-}\%d"}\NormalTok{ )}
\NormalTok{RF }\OtherTok{\textless{}{-}} \FunctionTok{rename}\NormalTok{( RF, }\AttributeTok{Bathford =}\NormalTok{ RiverFlow.x, }\AttributeTok{Bath =}\NormalTok{ RiverFlow.y )}
\end{Highlighting}
\end{Shaded}

Let's plot the observations for Bath and Bathford against each other,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{( RF}\SpecialCharTok{$}\NormalTok{Bath, RF}\SpecialCharTok{$}\NormalTok{Bathford, }\AttributeTok{cex.lab =} \FloatTok{1.5}\NormalTok{, }\AttributeTok{pch=}\DecValTok{19}\NormalTok{,}
      \AttributeTok{xlab=}\StringTok{"River Flow at Bath"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"River Flow at Bathford"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{01-DataWrangling_files/figure-pdf/unnamed-chunk-42-1.pdf}

}

\caption{Comparison of river flow for Bath and Bathford for 1 September
1976 - 30 October 2023.}

\end{figure}%

\textbf{What can we conclude from this plot?}

\textbf{Remark:} If we want the first element in the combined data frame
to be 01/09/1976 (the date when the gauge at Bath started operations),
we would use the function inner\_join(),

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RF\_1976\_2020 }\OtherTok{\textless{}{-}}\NormalTok{ Bathford\_RF }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{inner\_join}\NormalTok{( Bath\_RF, }\AttributeTok{by=}\FunctionTok{c}\NormalTok{(}\StringTok{"Date"} \OtherTok{=} \StringTok{"Date"}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\textbf{Remark:} The function inner\_join() does not remove the dates
after 1976 for which the observations for Bath (or Bathford) are
missing, but only the days which are not listed in both files.

\subsection{Merging multiple data
sets}\label{merging-multiple-data-sets}

In practice we may work with \(N\) data sets of the same (or a very
similar) format. For instance, we may have 20 data sets, and each data
set contains the river flow measurements for a gauge in Somerset. Then,
we do not want to implement a lot of code of the form in Section
\href{Merging\%20two\%20data\%20sets}{1.3.1} just to combine all these
data sets into a single data frame. Instead, we will use the
\textbf{for()} loop in R.

\textbf{Example:} Suppose that, in addition to the river flow
measurements for Bath and Bathford, we also need to consider the
observations for Compton Dando, a small village to the west of Bath not
located at the River Avon. For our analysis, it may be good to combine
all three data sets into a single data frame, and the following piece of
code is one way to create it.

We start by defining the file names and the number of lines that we have
to ignore when loading the data files

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gauges }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{( }\StringTok{"Bath"}\NormalTok{, }\StringTok{"Bathford"}\NormalTok{, }\StringTok{"Compton Dando"}\NormalTok{ )}
\NormalTok{lines\_to\_ignore }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{( }\DecValTok{19}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The next step is to load the data from the different files, store the
data frames in a list we call \textbf{RF\_individual}, and update the
variable names.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{setwd}\NormalTok{(}\StringTok{"Data/"}\NormalTok{)}
\NormalTok{RF\_individual }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{( k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(gauges) )\{}
  
  \DocumentationTok{\#\# Load the data from the .csv file}
\NormalTok{  file\_name }\OtherTok{\textless{}{-}} \FunctionTok{paste}\NormalTok{( gauges[k], }\StringTok{"River Flow.csv"}\NormalTok{ )}
\NormalTok{  RF\_individual[[k]] }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( file\_name, }\AttributeTok{skip=}\NormalTok{lines\_to\_ignore[k], }\AttributeTok{header=}\ConstantTok{FALSE}\NormalTok{,}
                                  \AttributeTok{colClasses =} \FunctionTok{c}\NormalTok{(}\StringTok{"character"}\NormalTok{,}\StringTok{"numeric"}\NormalTok{,}\StringTok{"NULL"}\NormalTok{) )}
  
  \DocumentationTok{\#\# Change the variable names}
  \FunctionTok{names}\NormalTok{( RF\_individual[[k]] ) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{( }\StringTok{"Date"}\NormalTok{, gauges[k] )}
  
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The code above includes two functions you may not have used so far and
so we briefly describe them:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  \textbf{paste()} is used to append ``River Flow.csv'' to the name of
  the gauge to get the file name.
\item
  \textbf{names()} is used to rename the variable names. In this case,
  this function was easier to use than rename(); the latter does not
  like to be given names from a vector.
\end{enumerate}

Now we are ready to merge the different data frames by repeatedly using
the function full\_join():

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RF }\OtherTok{\textless{}{-}}\NormalTok{ RF\_individual[[}\DecValTok{1}\NormalTok{]]}
\ControlFlowTok{for}\NormalTok{( k }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(gauges) )}
\NormalTok{  RF }\OtherTok{\textless{}{-}}\NormalTok{ RF }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{full\_join}\NormalTok{( RF\_individual[[k]], }\AttributeTok{by=}\FunctionTok{c}\NormalTok{(}\StringTok{"Date"}\OtherTok{=}\StringTok{"Date"}\NormalTok{) )}
\FunctionTok{glimpse}\NormalTok{( RF )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 23,955
Columns: 4
$ Date            <chr> "1976-09-01", "1976-09-02", "1976-09-03", "1976-09-04"~
$ Bath            <dbl> 3.39, 2.83, 2.97, 2.81, 2.90, 2.81, 2.59, 3.11, 2.78, ~
$ Bathford        <dbl> 2.811, 2.560, 2.337, 2.385, 2.146, 2.359, 2.367, 2.416~
$ `Compton Dando` <dbl> 0.188, 0.173, 0.170, 0.172, 0.174, 0.174, 0.176, 0.195~
\end{verbatim}

The final step is to convert the type of the variable \textbf{Date} and
to sort observations by date; we know that Bathford started collecting
data in 1969 but the first entry is for 1976. So we obtain the final
data frame using

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RF }\OtherTok{\textless{}{-}}\NormalTok{ RF }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{Date =} \FunctionTok{as\_date}\NormalTok{( Date, }\AttributeTok{format=}\StringTok{"\%Y{-}\%m{-}\%d"}\NormalTok{ ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{( Date )}
\FunctionTok{slice\_head}\NormalTok{( RF, }\AttributeTok{n=}\DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Date Bath Bathford Compton Dando
1 1958-03-01   NA       NA          2.97
2 1958-03-02   NA       NA          2.32
3 1958-03-03   NA       NA          1.98
4 1958-03-04   NA       NA          1.70
5 1958-03-05   NA       NA          1.42
\end{verbatim}

We can now start our analysis, for instance, by plotting the different
river flows against each other:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{( }\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{), }\AttributeTok{cex.lab =} \FloatTok{1.5}\NormalTok{ )}
\FunctionTok{plot}\NormalTok{( RF}\SpecialCharTok{$}\NormalTok{Bath, RF}\SpecialCharTok{$}\NormalTok{Bathford, }\AttributeTok{pch=}\DecValTok{19}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Bath"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Bathford"}\NormalTok{ )}
\FunctionTok{plot}\NormalTok{( RF}\SpecialCharTok{$}\NormalTok{Bath, RF}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Compton Dando}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{pch=}\DecValTok{19}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Bath"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Compton Dando"}\NormalTok{ )}
\FunctionTok{plot}\NormalTok{( RF}\SpecialCharTok{$}\NormalTok{Bathford, RF}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Compton Dando}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{pch=}\DecValTok{19}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Bathford"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Compton Dando"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{01-DataWrangling_files/figure-pdf/unnamed-chunk-48-1.pdf}

}

\caption{Scatter plots of river flow measurements for each pair of
gauges in the combined data set}

\end{figure}%

\textbf{What do you conclude from these plots?}

\textbf{Remark:} If we wanted to add more gauges to the data, we only
need to update the first two lines of R code in this example; the rest
of the code can be left unchanged.

\section{Summary}\label{summary}

We have covered some of the key concepts regarding data cleaning and
wrangling:

\begin{itemize}
\item
  Ensure that variables have the correct type and are given informative
  names
\item
  Use the dplyr R package when working with a single data frame. The
  package allows you to create subsets, sort the data, etc.
\item
  In many real-world applications we have to combine multiple data sets.
  The dplyr R package also provides functions to achieve this.
\end{itemize}

\textbf{Important:} Hardly any real-world data set is ``standard'' - we
had to use some additional functions/options for the river flow data, as
well as the Airbnb data analyzed in Problem Class 1. While we introduced
useful functions to perform data wrangling, we still usually have to
investigate the data file ``by hand'' before loading the data into R. In
this course we cannot possibly cover all scenarios that may occur when
working with real-world data, but you can usually find a satisfying
solution using Google (or other search engines).

\bookmarksetup{startatroot}

\chapter{Data Visualization}\label{data-visualization}

In Chapter 1 we explored how to restructure a data set and extract
summaries. In this chapter we will focus on \textbf{data visualization},
i.e., the creation and interpretation of plots that give us further
insight into the data. Data visualization is not just important for data
exploration, but also for presenting and communicating results.

There are two very important aspects we need to keep in mind:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Effective data visualization is more about clear communication than
  creating impressive plots. Your analysis may be excellent, but it
  won't attract any attention if you cannot convey your results
  effectively.
\item
  Plots support our arguments and/or highlight the reason for our
  conclusion. As such we should interpret plots in the context of the
  research question and not just provide a plot as the answer.
\end{enumerate}

We start by introducing a general framework for describing data graphics
in Section \hyperref[background-on-data-visualization]{2.1}. Sections
\hyperref[analysis-of-australian-weather-data-using-ggplot2]{2.2} and
\hyperref[creating-advanced-plots-with-ggplot2]{2.3} then demonstrate
how to use the R package \textbf{ggplot2} for data visualization.
Finally, some further aspects are considered in Section
\hyperref[changing-the-data-structure]{2.4}.

\textbf{Remark:} The methods and techniques considered in this chapter
cover general aspects of data visualization. Specific methods for
illustrating text and spatial data are left to the next chapters.

\textbf{Important:} Data visualization is to some degree subjective,
because there is often not just one way to visualize the data. However,
you should follow the principles outlined in this chapter, and your
conclusions need to be supported by your plot. Do not claim something
that is not clearly visible in your output!

\section{Background on data
visualization}\label{background-on-data-visualization}

Before starting to create plots in R, we establish a framework to
analyze plots in terms of four basic elements: \textbf{visual cues},
\textbf{coordinate system}, \textbf{scale} and \textbf{context}.
Understanding these elements will help us with producing our own plots
later.

\subsection{Visual cues}\label{visual-cues}

Visual cues are graphical elements that draw the audience to the aspects
we want them to focus on. The book \emph{``Data points: Visualization
that means something''} by Nathan Yau (link provided on Moodle) lists
nine distinct visual cues to encode a category or quantity:

\begin{itemize}
\item
  Position (quantity) - relation to other things
\item
  Length (quantity) - size in one dimension
\item
  Angle (quantity) - width of angle may, for instance, represent
  proportions (pie chart)
\item
  Direction (quantity) - slope of line
\item
  Shape (category) - which observations are in the same group
\item
  Area (quantity) - size in two dimensions
\item
  Volume (quantity) - size in three dimensions
\item
  Shade (quantity or category) - shade in comparison to others, or
  grouping
\item
  Colour (quantity or category) - colour in comparison to others, or
  grouping
\end{itemize}

Research has shown that our ability to perceive differences in magnitude
descends in this order. One of many publications supporting this
argument is \emph{``Graphical perception: Theory, experimentation, and
application to the development of graphical methods.''}, which you can
find on Moodle.

\textbf{Important:} One crucial conclusion is that we should not rely
too much on colour. Many people have colour deficiencies, which makes it
very hard for them to distinguish certain colours. Consequently, before
using colour, we should consider whether we could use shapes or shades
instead.

\textbf{Remark:} In this course we focus on creating 2D graphics. While
3D plots and animations allow us to visualize a larger number of
variables (and you may think they look more impressive), I would avoid
using such plots except for a very limited number of cases. That's
because it is often difficult to see the exact positions of the points.

\subsection{Example: Freediving world
records}\label{example-freediving-world-records}

The data set ``Freediving Records.csv'' provides information on the
progression of the world record in multiple disciplines for men and
women. We will focus on the discipline ``dynamic apnea with fins (DYN)''
and visualize how the world records for men and women have progressed
over time:

\begin{figure}[H]

{\centering \includegraphics[width=0.55\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/DNY-1.pdf}

}

\caption{Development of the world record in dynamic apnea with fins
(DYN) for men and women between 1993 and 2020.}

\end{figure}%

We can identify that the following visual cues have been used:

\begin{itemize}
\item
  Shape and colour indicate whether the observations refer to the world
  record for men or women. Note, there is no issue with using multiple
  cues for the same information.
\item
  Length of line is used to convey two pieces of information: (i) the
  length of the line in horizontal direction indicates the time it took
  until the world record was broken, and (ii) the length of the line in
  vertical direction represents the magnitude of improvement.
\item
  Position is used to compare the world records for men and women (both
  lines are provided in the plot).
\item
  Shape is used to highlight the times when a world record was broken -
  this makes it is easy to count the number of times the record was
  broken. Without this visual cue, such information would be much harder
  to extract from the plot.
\end{itemize}

\subsection{Coordinate system}\label{coordinate-system}

Choosing a suitable coordinate system is critical to present the data
accurately and in a meaningful way. The three most common coordinate
systems used in data science are

\begin{description}
\item[\textbf{Cartesian:}]
Our familiar (\(x,y\))-rectangular coordinate system with two
perpendicular axes.
\item[\textbf{Polar:}]
Points are identified by their radius \(r\) (distance from the origin)
and angle \(\theta\). A point (\(x,y\)) in Cartesian coordinates can be
transformed to polar coordinates using the relationship {[} (x,y) =
(r\cos\theta, r\sin\theta), \qquad r=\sqrt{x^2+y^2}. {]}
\item[\textbf{Geographical:}]
In the chapter on spatial data analysis, we will work with points
located across the earth, with their positions being defined by
longitude and latitude.
\end{description}

There are two common cases when we may want to consider polar
coordinates:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Pie charts
\item
  Variables which naturally lie on a circle, such as wind direction.
\end{enumerate}

\textbf{Tip:} Since \textbf{Position} and \textbf{Length} are better
visual cues than \textbf{Angle}, we should prefer Cartesian coordinates
to polar coordinates. For instance, before using pie charts, consider
whether the same information can be displayed effectively in a bar plot.
However, we will see some examples where polar coordinates may be
considered the better choice.

\subsection{Example: Analysis of wind direction and
speed}\label{example-analysis-of-wind-direction-and-speed}

Hourly weather data was collected for Bela Vista, Brazil, for 2017 and
2018. Suppose we were asked to explore the distribution of wind
direction and the relation between wind direction and speed of wind
gusts.

To analyse the distribution of wind direction, we may create a histogram
of the collected data. Let's compare the histograms obtained for the
Cartesian and polar coordinate systems (both display frequency):

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/BVWind1-1.pdf}

}

\caption{Frequency of wind direction for Bela Vista, Brazil, in 2017 and
2018 displayed using Cartesian coordinates (left) and polar coordinates
(right).}

\end{figure}%

Both plots, Cartesian (left) and polar (right), show that the wind
predominately comes from a north-easterly direction. The right plot is a
tick nicer, because it better handles that \(0^{\circ}=360^{\circ}\) in
terms of wind direction, which is recorded as an angle.

Let's turn to analyzing the relation between wind direction and speed of
wind gusts. For this we create plots which map wind direction against
the speed of the wind gusts, again using different coordinate systems:

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/BVWind2-1.pdf}

}

\caption{Wind direction versus speed of wind gust for Bela Vista in 2017
and 2018 displayed using Cartesian coordinates (left) and polar
coordinates (right).}

\end{figure}%

\textbf{What can we conclude about the relation between wind direction
and speed of wind gusts?}

I personally prefer the left plot because it indicates the difference in
frequency of wind directions. The right plot (using polar coordinates)
does not provide this information and it seems almost as if wind
direction is uniformly distributed - so we would need the histogram in
Figure @ref(fig:BVWind1) to correctly interpret the plot.

\subsection{Scale and context}\label{scale-and-context}

The concepts of scale and context refer to the choice of axes and the
labeling of the data graphic respectively.

\subsubsection*{Scale}\label{scale}
\addcontentsline{toc}{subsubsection}{Scale}

Scales allow us to translate values into visual cues by influencing, for
instance, the distance (length) between points in a scatter plot. When
choosing scales, we have to consider how the displayed distances
translate into meaningful differences. Each coordinate axis can have its
own scale, and we have three different choices:

\textbf{Numerical:} Numeric quantities, such as speed, age, etc., are
commonly set on a linear, logarithmic or percentage scale.

\textbf{Categorical:} Categorical variables may have no ordering
(political parties), or be \textbf{ordinal} (restaurant ratings).
Ordinal variables differ from numeric quantities in that distances are
unknown (or not meaningful). For instance, a first class degree is
better than a 2.1, but what does this mean in terms of distance?

\textbf{Time:} While being a numeric quantity, time has some special
properties: 1) it can be demarcated by year, month,.. and 2) it can be
considered periodical. This second aspect can be highlighted using polar
coordinates as seen in Section
\hyperref[example-analysis-of-wind-direction-and-speed]{2.1.4}.

\subsubsection*{Context}\label{context}
\addcontentsline{toc}{subsubsection}{Context}

Data graphics are provided to aid the reader/viewer with making
meaningful comparisons. Context can be added in the form of titles,
subtitles and axis labels that explain what is being shown, including
the scales and units. It sometimes also helps to include reference
points or lines. While we should avoid cluttering or providing excessive
annotations, it is necessary to provide proper context.

\subsubsection*{Example: Relation between body and brain weight for
mammals}\label{example-relation-between-body-and-brain-weight-for-mammals}
\addcontentsline{toc}{subsubsection}{Example: Relation between body and
brain weight for mammals}

The following two plots both illustrate the relation between body weight
and brain weight for several mammals, but they use different scales:

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/Mammals-1.pdf}

}

\caption{Body weight vs brain weight for 62 mammals on linear scale
(left) and logarithmic scale (right).}

\end{figure}%

We make two observations:

\begin{itemize}
\item
  Linear scales were used in the left plot, while logarithmic scales
  were used in the right plot - this context was provided via the title.
\item
  The right plot is more informative as it shows a linear relationship
  between body and brain weight on logarithmic scale. This aspect is not
  clear in the left plot, and a viewer may focus on the three mammals
  with the highest brain weight as they appear to be quite different
  from the rest.
\end{itemize}

\section{Analysis of Australian weather data using
ggplot2}\label{analysis-of-australian-weather-data-using-ggplot2}

After outlining the different elements of a plot in the previous
section, we now explore how we can create informative plots using the
\textbf{ggplot2} R package

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( ggplot2 )}
\end{Highlighting}
\end{Shaded}

We will introduce and utilize a wide range of plots to analyze
historical weather data for five Australian cities. The data is provided
on Moodle as ``WeatherAustralia.csv'' and we start by loading the data
and looking at the first few entries:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AUS }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/WeatherAustralia.csv"}\NormalTok{ )}
\FunctionTok{slice\_head}\NormalTok{( AUS, }\AttributeTok{n=}\DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Date Location MinTemp MaxTemp Rainfall WindGustSpeed
1 01/01/2009   Sydney    17.7    35.1        0            72
2 02/01/2009   Sydney    18.5    23.0        0            63
3 03/01/2009   Sydney    16.9    23.2        0            NA
4 04/01/2009   Sydney    18.7    27.1        0            65
5 05/01/2009   Sydney    20.2    31.6        0            63
\end{verbatim}

We see that the data provide daily information on minimum and maximum
temperature, amount of rainfall and wind speed. Further, the data also
include some missing values, as highlighted by the
\(\mathrm{\texttt{NA}}\) entries.

In the following, we perform a data exploration which includes the
creation and interpretation of multiple plots. At the end of the
analysis, you will be familiar with the general functionality of the
ggplot2 package. Further types of plots are listed on the ggplot2 cheat
sheet (provided on Moodle). I would advise you to try creating some of
them for practice, as they may be useful.

\textbf{Important:} In this section we focus on the different types of
plots and visual cues. The other elements described in Section
\href{Background\%20on\%20data\%20visualization}{2.1}, coordinate
system, scale and context, will be considered in Section
\href{Creating\%20advanced\%20plots\%20with\%20ggplot2}{2.3}.

\subsection{Initializing a plot}\label{initializing-a-plot}

The first step is to call the function \textbf{ggplot()} and we usually
specify the following inputs:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The \textbf{name of the data frame} that contains the data we want to
  plot
\item
  The \textbf{names of the variables} which specify the axes. This is
  done using the function \textbf{aes()}, which refers to
  \emph{aesthetics} (we will see this very often).
\end{enumerate}

Suppose we want to create a scatter plot of minimum against maximum
temperature for the five Australian cities. To initialize the plot, we
specify

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PlotAUS }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{( AUS, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{MinTemp, }\AttributeTok{y=}\NormalTok{MaxTemp ) )}
\end{Highlighting}
\end{Shaded}

However, if we call \textbf{PlotAUS} in R, we will see a Cartesian
coordinate system, but no points. This is because the coordinate system
and visual cues are specified using separate functions in ggplot2. We
will consider in the next subsection how to add points as an additional
layer.

\textbf{Remark:} While it is important to use meaningful variable names
when programming, I usually make an exception when using ggplot2. For
instance, instead of \textbf{PlotAUS}, we could also call it \textbf{g}
(for graphic) - I will later use this for brevity.

\subsection{Scatter plots}\label{scatter-plots}

The function \textbf{geom\_point()} adds the points to the coordinate
system created by ggplot(). Returning to the Australian weather data, we
generate the scatter plot of daily minimum and maximum temperature using

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PlotAUS }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.4\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\caption{Minimum versus maximum observed daily temperature for five
Australian cities between 2008 and 2017.}

\end{figure}%

We observe a positive correlation between minimum and maximum daily
temperature. However, this plot does not highlight that the data are
coming from five different cities. In other words, the plot does not
allow us to explore differences between the cities.

One possible solution is to use the visual cues shape and colour to
indicate to which city a data point belongs. Specifically, we specify
via aes() that the variable \textbf{Location} should be used to
determine the shape and colour of the points created by the
geom\_point() function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PlotAUS }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{( }\FunctionTok{aes}\NormalTok{( }\AttributeTok{shape=}\NormalTok{Location, }\AttributeTok{color=}\NormalTok{Location ) )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\caption{Minimum versus maximum observed daily temperature for five
Australian cities between 2008 and 2017, with the city being highlighted
using the visual cues of shape and colour.}

\end{figure}%

\textbf{Remark:} If we wanted to use shade, we would have specified
\(\mathrm{\texttt{aes( alpha=Location )}}\).

While our plot now makes clear that the data come from different cities,
it is still very cluttered. This makes it hard to draw conclusions on
differences between the five Australian cities. Luckily, there is a
better option available, which we consider next.

\subsubsection*{Facets}\label{facets}
\addcontentsline{toc}{subsubsection}{Facets}

Instead of creating a single plot which contains all points, it seems a
nice idea to have a separate plot for each city. In other words, we want
to split the data using the variable \textbf{Location} and create one
plot per subset.

This type of plot is called a \textbf{facet} plot and it provides a
simple and effective way to display the data for the separate levels of
a categorical variable.

To create the facet plot, we use the function \textbf{facet\_wrap()} and
change the axes labels using the \textbf{labs()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PlotAUS }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{Location ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Minimum daily temperature"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Maximum daily temperature"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.89\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/Facets-1.pdf}

}

\caption{Plots of minimum versus maximum daily temperature between 2008
and 2017 for five Australian cities.}

\end{figure}%

Our plot now reveals that Darwin is quite different from the other four
cities in terms of the distribution of minimum and maximum daily
temperature, in particular, in terms of the range of values.

\textbf{What may be the reason for this difference? What else can we
conclude?}

\subsection{Line plots}\label{line-plots}

As for the river flow data in Section
\hyperref[example-loading-and-cleaning-nrfa-river-flow-data]{1.1.4}, we
may want to explore how a variable changes over time for one of the five
cities.

Scatter plots are often not the right choice for such a task, because
it's hard to see patterns in the data. Instead we create a line plot,
where the length of the line connecting consecutive dates illustrates
the magnitude of change in the values of the variable.

Let's see how we can create a line plot for daily maximum temperature
over time for Darwin. We start by converting the variable \textbf{Date}
in the data frame to the correct type,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AUS }\OtherTok{\textless{}{-}}\NormalTok{ AUS }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{( }\AttributeTok{Date =} \FunctionTok{as\_date}\NormalTok{( Date, }\AttributeTok{format=}\StringTok{"\%d/\%m/\%Y"}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

The next step is to extract the subset of observations for Darwin, which
we store in a separate data frame called \textbf{Darwin}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Darwin }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{( AUS, Location }\SpecialCharTok{==} \StringTok{"Darwin"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

A line plot is then created by using the \textbf{geom\_line()} function,
again by first defining the axes in the ggplot() function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( Darwin, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{Date, }\AttributeTok{y=}\NormalTok{MaxTemp ) ) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Date"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Maximum daily temperature"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/unnamed-chunk-8-1.pdf}

}

\caption{Time series plot of daily maximum temperature for Darwin
between 2008 and 2017.}

\end{figure}%

To create such a (time series) plot for all five cities, we again use
facet\_wrap(), and we use colour and shape (line type) to highlight the
different cities:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( AUS, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{Date, }\AttributeTok{y=}\NormalTok{MaxTemp ) ) }\SpecialCharTok{+} \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{Location ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{( }\FunctionTok{aes}\NormalTok{( }\AttributeTok{color=}\NormalTok{Location, }\AttributeTok{linetype=}\NormalTok{Location ) ) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Date"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Maximum daily temperature"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/DateTempFacet-1.pdf}

}

\caption{Time series plot of daily maximum temperature for five
Australian cities between 2008 and 2017.}

\end{figure}%

The gap in the plot for Melbourne is due to the data being missing for
this time period. One conclusion we draw from the plot is that the
maximum daily temperature is less varied for Darwin than the other
cities, which exhibit a clear seasonal pattern.

\textbf{Remark 1:} To draw steps as in Section
\hyperref[example-freediving-world-records]{2.1.2}, we would use
\textbf{geom\_step()} instead of geom\_line().

\textbf{Remark 2:} The use of colour and shape in Figure
@ref(fig:DateTempFacet) is not really necessary. We just did it do
illustrate that visual cues can be used together with facet\_wrap().

\subsection{Histograms and density
plots}\label{histograms-and-density-plots}

We now want to investigate the distribution of a single variable, such
as the speed of the wind gusts in one (or more) of the cities.
Histograms and \textbf{density plots} are useful in such cases. You
already created histograms in Year 1 Probability \& Statistics and we
analyzed such a plot in Section
\hyperref[example-analysis-of-wind-direction-and-speed]{2.1.4}. We start
by producing a histogram using ggplot2 and then introduce the density
plot in more detail.

\subsubsection*{Histograms}\label{histograms}
\addcontentsline{toc}{subsubsection}{Histograms}

The ggplot2 package provides the function \textbf{geom\_histogram()} to
create a histogram. As for the hist() function you used before, we have
to set a suitable number of bins, and this is done using the
\textbf{bins} option inside the geom\_histogram() function.

Let's return to the data for Darwin. To create a histogram for wind
speed gusts, we have to call ggplot() and geom\_histogram(), but we now
only specify the x-axis:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( Darwin, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{WindGustSpeed ) ) }\SpecialCharTok{+} \FunctionTok{geom\_histogram}\NormalTok{( }\AttributeTok{bins=}\DecValTok{20}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Speed of wind gust in km/h"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Count"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/unnamed-chunk-9-1.pdf}

}

\caption{Histogram of the speed of wind gusts for Darwin between 2008
and 2017.}

\end{figure}%

\textbf{What could be a sensible distribution for the speed of wind
gusts, based on the histogram?}

\subsubsection*{Density plots}\label{density-plots}
\addcontentsline{toc}{subsubsection}{Density plots}

A density plot is pretty much a \emph{smoothed} version of the
histogram. Given data \(x_1,\ldots,x_n\), we define the estimate
\(\hat{f}_X(\cdot)\) for the probability density function \(f_X(\cdot)\)
of the random variable \(X\) as \begin{equation}
\hat{f}_X(x) = \frac{1}{nh}\sum_{i=1}^{n} K\left(\frac{x-x_i}{h}\right), \qquad x\in\mathbb{R},
(\#eq:KDE)
\end{equation} where \(K(\cdot)\) is termed the \textbf{kernel} and
\(h\) is called the \textbf{bandwidth}. The kernel is a non-negative
probability density function. One common choice is to set \(K(\cdot)\)
as the density of the standard normal distribution, with {[}
\hat{f}\emph{X(x) =
\frac{1}{nh}\sum}\{i=1\}\^{}\{n\}\frac{1}{\sqrt{2\pi}}\exp\left\{-\frac{(x-x_i)^2}{2h^2}\right\}.
{]}

In principle, the bandwidth \(h\) has to be set by us. If \(h\) is too
small, the density plot will look very jittered, while a too large \(h\)
will obscure the underlying data structure. Consequently, care should be
taken when setting the bandwidth.

\textbf{Remark 1:} When generating density plots in R, a suitable value
for \(h\) is often provided automatically. However, we sometimes have to
set the bandwidth manually.

\textbf{Remark 2:} It is usually sufficient to provide either the
histogram or the density plot, because they visualize similar aspects of
the data. Note, the histogram should be used for discrete data.

In ggplot2, \textbf{geom\_density()} is used to create density plots
and, as for geom\_histogram(), we only specify the x-axis. Let's use
this type of plot to compare the distributions of the speed of wind
gusts for Darwin and Adelaide. To aid the comparison, we place the
estimates for the two cities in the same plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{WeatherAD }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{( AUS, Location }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Adelaide"}\NormalTok{,}\StringTok{"Darwin"}\NormalTok{) )}
\FunctionTok{ggplot}\NormalTok{( WeatherAD, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{WindGustSpeed ) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{( }\FunctionTok{aes}\NormalTok{( }\AttributeTok{linetype=}\NormalTok{Location, }\AttributeTok{color=}\NormalTok{Location ), }\AttributeTok{size=}\FloatTok{1.2}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Speed of wind gust in km/h"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Density"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/DAWind-1.pdf}

}

\caption{Density plots of the speed of wind gusts for Adelaide (solid)
and Darwin (dashed) for 2008--2017.}

\end{figure}%

The two estimated curves indicate that Adelaide and Darwin differ in the
distribution of wind gust speeds. For instance, Adelaide observes wind
gust speeds below 30 km/h more often than Darwin.

\textbf{Remark 1:} The option \(\mathrm{\texttt{size=1.2}}\) in
geom\_density() increases the thickness of the line in the plot - you
can set \textbf{size} to any positive value, and higher values will give
thicker lines. If you wanted to specify the bandwidth \(h\) in
@ref(eq:KDE), you have the option \(\mathrm{\texttt{bw=..}}\).

\textbf{Remark 2:} Density plots can sometimes be misleading because
they are a smoothed version of the histogram. Although Adelaide and
Darwin appear to have a similar density in Figure @ref(fig:DAWind) when
it comes to high wind speeds, the highest value for Adelaide is 86 km/h,
while it is 126 km/h for Darwin. Consequently, we should be cautious
when drawing conclusions about the distribution tails based on density
plots.

\subsection{Box and violin plots}\label{box-and-violin-plots}

Box plots and violin plots are useful for comparing a number of
distributions. For instance, we may want to compare the distributions of
wind gust speed for the five Australian cities. While we could produce a
separate density plot for each city, box and violin plots tend to be a
better choice.

The two types of plot differ in terms of the information they provide:

\begin{itemize}
\item
  Box plots visualize the median, interquartile range (25\% and 75\%
  quantile) and outliers.
\item
  Violin plots visualize the density estimate \(\hat{f}_X(\cdot)\) as
  defined in Equation @ref(eq:KDE).
\end{itemize}

\subsubsection*{Box plots}\label{box-plots}
\addcontentsline{toc}{subsubsection}{Box plots}

In ggplot2, box plots are created using the \textbf{geom\_boxplot()}
function. Let's use this type of plot to compare the wind gust speeds
across the five cities. We set the x-axis to the categorical variable
\textbf{Location}, and the y-axis as speed of wind gust:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( AUS, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{Location, }\AttributeTok{y=}\NormalTok{WindGustSpeed ) ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_boxplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( }\AttributeTok{fill=}\NormalTok{Location ) ) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{y=}\StringTok{"Speed of wind gust in km/h"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/Box-1.pdf}

}

\caption{Box plots of the speed of wind gusts for five Australian cities
between 2008 and 2017.}

\end{figure}%

We can extract a few details about the distributions of the speed of
wind gusts for the five cities. For instance, when considering the
median, Melbourne and Sydney record the strongest wind gusts, while
Adelaide and Perth record the lowest wind gust speeds.

\textbf{Remark 1:} The option \(\mathrm{\texttt{fill=Location}}\) gives
each box plot a different colour.

\textbf{Remark 2:} If we wanted a single box plot for a variable, we
would only specify one of the axes.

\subsubsection*{Violin plots}\label{violin-plots}
\addcontentsline{toc}{subsubsection}{Violin plots}

To conclude our analysis, we compare the distribution of daily wind gust
speeds across the five Australian cities using violin plots. This type
of plot is generated using the \textbf{geom\_violin()} function, and the
syntax is the same as for geom\_boxplot(). We make two additions
compared to Figure @ref(fig:Box) though:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  A plot title is set using the labs() function.
\item
  It's good practice to order box and violin plots, for instance, based
  on the median of the different subgroups. This is achieved using the
  \textbf{reorder()} function.
\end{enumerate}

Putting everything together, the five violin plots are generated using

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( AUS, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\FunctionTok{reorder}\NormalTok{(Location, WindGustSpeed, median, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{), }
                  \AttributeTok{y=}\NormalTok{WindGustSpeed ) ) }\SpecialCharTok{+} \FunctionTok{geom\_violin}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Location"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Wind gust speed in mph"}\NormalTok{, }
        \AttributeTok{title=}\StringTok{"Wind gust speed across five Australian cities"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.68\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/unnamed-chunk-10-1.pdf}

}

\caption{Violin plots of wind speed across five cities in Australia. The
cities are ordered in ascending order in terms of their median wind gust
speed.}

\end{figure}%

\textbf{Remark:} The four components within reorder() should be read as
follows: (i) We want to reorder the values in \textbf{Location}, (ii)
which should be based on the variable \textbf{WindGustSpeed}, (iii) the
value to be used for the reordering is the median and (iv) ignore any
missing values when calculating the median.

By ordering the violin plots, we can directly draw the conclusion that
Adelaide has the lowest median wind gust speed, while Sydney has the
highest. There are also other conclusions that are worth pointing out,
such as the differences in terms of highest observed wind gust speeds.

\section{Creating advanced plots with
ggplot2}\label{creating-advanced-plots-with-ggplot2}

In the previous section we generated plots using the structure {[}
\mathrm{\texttt{ggplot(data=, aes(x=.., y=..) )}} +
\mathrm{\texttt{<GEOM}}\_\mathrm{\texttt{FUNCTION>( aes(..) )} }. {]}
and this allowed us to create several type of plots and to use multiple
visual cues. The following table lists the types of plots we explored so
far (plus the functions for generating bar plots):

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1905}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3095}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type of plot
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\mathrm{\texttt{<GEOM}}\_\mathrm{\texttt{FUNCTION>}}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Visual cues we may use
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Scatter plot & \textbf{geom\_point} & shape, shade, colour, size \\
Line plot & \textbf{geom\_line} & shade, colour, linetype, size \\
& \textbf{geom\_step} & shade, colour, linetype, size \\
Bar plot & \textbf{geom\_bar} & shade, colour, size \\
& \textbf{geom\_col} & shade, colour, size \\
Histogram & \textbf{geom\_histogram} & shade, colour, linetype \\
Box plot & \textbf{geom\_boxplot} & shade, colour \\
Density plot & \textbf{geom\_density} & shade, colour, linetype \\
Violin plot & \textbf{geom\_violin} & shade, colour, linetype \\
\end{longtable}

\newpage

We also covered a few additional aspects, such as using facet\_wrap() to
create facets and labs() to specify the axes labels. In this section we
consider the remaining plot elements described in Section
\hyperref[background-on-data-visualization]{2.1}:

\begin{itemize}
\item
  Changing the coordinate system to polar coordinates
\item
  Changing the scale of the x-axis (y-axis)
\end{itemize}

We will also explore how to change the font size of the labels and the
colour scheme in our plot. This will give us a lot of flexibility to
visualize data.

\subsection{Changing the coordinate
system}\label{changing-the-coordinate-system}

We saw in Section
\hyperref[analysis-of-australian-weather-data-using-ggplot2]{2.2} that
ggplot() uses the Cartesian coordinate system by default. In Section
\hyperref[coordinate-system]{2.1.3} we highlighted that we sometimes
want to use polar coordinates, for instance, when displaying wind
direction. The \textbf{coord\_polar()} function in ggplot2 allows us to
switch from Cartesian to polar coordinates. Let's consider two cases
where such a transformation may be useful.

\subsubsection*{Visualizing direction}\label{visualizing-direction}
\addcontentsline{toc}{subsubsection}{Visualizing direction}

In Figures @ref(fig:BVWind1) and @ref(fig:BVWind2) we used plots with
polar coordinates to explore wind direction and speed for Bela Vista,
Brazil. We can reproduce these plots in ggplot2:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(patchwork) }\CommentTok{\# required to place plots next to each other}
\NormalTok{wind\_BV }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/Wind\_Bela\_Vista.csv"}\NormalTok{ )}

\NormalTok{g1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{( wind\_BV, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{Wind.Direction ) ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{( }\AttributeTok{bins =} \DecValTok{120}\NormalTok{ ) }\SpecialCharTok{+} \FunctionTok{coord\_polar}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Wind Direction"}\NormalTok{ )}
\NormalTok{g2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{( wind\_BV, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{Wind.Direction, }\AttributeTok{y=}\NormalTok{Gust.Speed ) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_smooth}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{coord\_polar}\NormalTok{( }\AttributeTok{theta=}\StringTok{"x"}\NormalTok{ )  }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Wind Direction"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Wind gust in m/s"}\NormalTok{ )}
\NormalTok{g1 }\SpecialCharTok{+}\NormalTok{ g2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = "cs")'
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics[width=0.88\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/BVWindNew-1.pdf}

}

\caption{Histogram illustrating the frequency of wind direction (left)
and scatter plot of wind direction vs wind speed (right) for Bela Vista,
Brazil. Both plots use polar coordinates. The solid line in the right
plot shows the average wind speed for each wind direction.}

\end{figure}%

The option \(\mathrm{\texttt{theta="x"}}\) in coord\_polar() specifies
that the angle should be defined by the x-axis. We do not have to do
anything else - ggplot2 is doing all the calculations for us.

\textbf{Remark 1:} The R package \textbf{patchwork} makes it easy to
place two graphics created by ggplot2 next to each other by using the
\textbf{+} sign. There are more options available - have a look at the
documentation for the package.

\textbf{Remark 2:} The example also shows that we can use multiple
geom\_..() functions in the same plot. Here, we used geom\_smooth() to
add a line representing the average wind speed in each direction to the
plot. However, we note that the line does not match up at \(0^{\circ}\).
This is not particularly elegant, but there is not a very easy fix to
this.

\textbf{Are there any other plots we should create to explore the
distribution of wind direction and speed of wind gusts at Bela Vista,
Brazil?}

\subsubsection*{Pie Charts}\label{pie-charts}
\addcontentsline{toc}{subsubsection}{Pie Charts}

In a pie chart we use the size of the angle as a visual cue and such a
plot may be useful to visualize proportions. The following R code
illustrates how to create a pie chart using ggplot2:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{( }\StringTok{"prob"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{,}\FloatTok{0.4}\NormalTok{,}\FloatTok{0.3}\NormalTok{), }\StringTok{"group"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{,}\StringTok{"B"}\NormalTok{,}\StringTok{"C"}\NormalTok{) )}
\FunctionTok{ggplot}\NormalTok{( df, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\NormalTok{prob, }\AttributeTok{fill=}\NormalTok{group ) ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{coord\_polar}\NormalTok{( }\AttributeTok{theta=}\StringTok{"y"}\NormalTok{ ) }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.55\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/unnamed-chunk-11-1.pdf}

}

\caption{Example of a pie chart with three slices.}

\end{figure}%

The code first creates a plot on Cartesian coordinates - a stacked bar
plot in this case - which is then converted to polar coordinates. To see
this, you may want to have a look at the plot you obtain when removing
coord\_polar() from the code above.

\subsection{Changing scales}\label{changing-scales}

We saw in Section \hyperref[scale-and-context]{2.1.5} that changing the
scale of one (or more) variables in a plot can help with the data
analysis. In applications, transformations are often used to reduce the
influence of extreme outliers in the plot. For instance, by considering
logarithmic scales in Section \hyperref[scale-and-context]{2.1.5}, the
three mammals with very high brain weight in the data appeared less
extreme, and we were instead able to see the possible linear
relationship on logarithmic scale between brain and body weight.

When using ggplot(), a linear scale is used by default. The function
\textbf{coord\_trans()} enables us to change the scale of the x-axis or
y-axis. The most common transformations are logarithmic
(\(\mathrm{\texttt{x="log"}}\) or \(\mathrm{\texttt{x="log10"}}\)) and
square root (\(\mathrm{\texttt{x="sqrt"}}\)).

\subsubsection*{Example: Analysis of Facebook
Data}\label{example-analysis-of-facebook-data}
\addcontentsline{toc}{subsubsection}{Example: Analysis of Facebook Data}

The data in ``Facebook.csv'' contains data related to messages posted by
a few Olympic athletes. We want to explore the relation between number
of likes (received for individual posts) and the number of followers of
the athlete. Let's produce two plots, one with linear scales and one
with logarithmic scales:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Facebook }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/Facebook.csv"}\NormalTok{, }\AttributeTok{header=}\ConstantTok{TRUE}\NormalTok{ )}

\NormalTok{g  }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{( Facebook, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{follow, }\AttributeTok{y=}\NormalTok{postlikes ) ) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Number of Followers"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Number of likes"}\NormalTok{ )}
\NormalTok{g1 }\OtherTok{\textless{}{-}}\NormalTok{ g }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }
\NormalTok{g2 }\OtherTok{\textless{}{-}}\NormalTok{ g }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{coord\_trans}\NormalTok{( }\AttributeTok{x=}\StringTok{"log"}\NormalTok{, }\AttributeTok{y=}\StringTok{"log"}\NormalTok{ )}
\NormalTok{g1 }\SpecialCharTok{+}\NormalTok{ g2}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/Facebook-1.pdf}

}

\caption{Number of followers vs number of likes on linear scale (left)
and logarithmic scale (right) for Facebook posts by a group of Olympic
athletes.}

\end{figure}%

We see that the two plots provide quite different information: The
graphic with log-transformed scales shows a possible linear relationship
between log(number of followers) and log(number of likes), which is not
visible when considering the plot with linear scales.

\textbf{Remark:} If you want to use another transformation than
logarithmic or square root, you can specify it within ggplot. For
instance, we could use

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( Facebook, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\FunctionTok{log}\NormalTok{(follow), }\AttributeTok{y=}\FunctionTok{log}\NormalTok{(postlikes) ) ) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"log(Number of Followers)"}\NormalTok{, }\AttributeTok{y=}\StringTok{"log(Number of likes)"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

You have to be aware that the numbers along the axes differ in this
case, and you have to change the axis labels to highlight that the
values are on logarithmic scale.

\subsection{Changing font size}\label{changing-font-size}

When we produce plots, we may find that, for instance, the axis labels
are too small. Further, we may want to change the margins around our
graphics. The \textbf{theme()} function allows us to do all this (and
much more).

In Figure @ref(fig:Facebook) the axis labels are quite small and it may
be nice to have a little bit of a gap between the plots. Let's use the
theme() function to achieve this. To visualize the difference, we only
alter the setup of the second plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g2 }\OtherTok{\textless{}{-}}\NormalTok{ g2 }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{( }\AttributeTok{plot.margin=}\FunctionTok{margin}\NormalTok{( }\AttributeTok{t=}\DecValTok{0}\NormalTok{, }\AttributeTok{l=}\DecValTok{100}\NormalTok{, }\AttributeTok{b=}\DecValTok{0}\NormalTok{, }\AttributeTok{r=}\DecValTok{0}\NormalTok{ ),}
         \AttributeTok{axis.title=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{16}\NormalTok{),}
         \AttributeTok{axis.text=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{14}\NormalTok{) )}
\NormalTok{g1 }\SpecialCharTok{+}\NormalTok{ g2}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.98\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/unnamed-chunk-13-1.pdf}

}

\caption{Number of followers vs number of likes on linear scale (left)
and logarithmic scale (right) for Facebook posts by a group of Olympic
athletes.}

\end{figure}%

We see that the gap between the plots has widened, and that the font
size of the axis labels and numbers in the right plot have increased.
Font size in ggplot2 is specified via \textbf{element\_text(size=..)}:

\begin{itemize}
\item
  Font size of the title is changed with \textbf{theme(
  title=element\_text(size=..) )}
\item
  Font size of the axis labels is changed with \textbf{theme(
  axis.title=element\_text(size=..) )}
\item
  Font size in the legend can be changed with \textbf{theme(
  legend.title=element\_text(size=..) )} and \textbf{theme(
  legend.text=element\_text(size=..) )}
\end{itemize}

\textbf{Remark:} Making sure that labels are provided in a suitable font
size is important. The gap between the plots is more of a personal
preference.

\subsection{Changing colour schemes}\label{changing-colour-schemes}

So far we used the default colour scheme provided by ggplot2. There are
situations, however, where we may want to change the colour scheme. One
easy way to achieve this is by using the functions
\textbf{scale\_filler\_brewer()} and \textbf{scale\_colour\_brewer()} in
ggplot2 - which of these to use depends on whether you specified
\textbf{fill=..} or \textbf{colour=..} in aes().

\textbf{Example:} Let's reproduce the box plots in Section
\hyperref[box-and-violin-plots]{2.2.6}, but with an orange colour scheme

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AUS }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/WeatherAustralia.csv"}\NormalTok{ )}
\FunctionTok{ggplot}\NormalTok{( AUS, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{Location,}\AttributeTok{y=}\NormalTok{WindGustSpeed ) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{Location) ) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{y=}\StringTok{"Speed of wind gust in km/h"}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{scale\_fill\_brewer}\NormalTok{( }\AttributeTok{palette=}\StringTok{"Oranges"}\NormalTok{ ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{( }\AttributeTok{axis.title=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{14}\NormalTok{), }\AttributeTok{axis.text=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{14}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/unnamed-chunk-14-1.pdf}

}

\caption{Box plots of the speed of wind gust for 5 cities in Australia.}

\end{figure}%

\textbf{Remark:} We used \textbf{theme\_bw()} to change the background
colour from grey to white.

There are multiple colour blind friendly patterns available for
scale\_filler\_brewer() and scale\_colour\_brewer(), which you can view
using

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( RColorBrewer )}
\FunctionTok{display.brewer.all}\NormalTok{( }\AttributeTok{colorblindFriendly =} \ConstantTok{TRUE}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.75\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/unnamed-chunk-15-1.pdf}

}

\caption{List of colour blind friendly palettes that can be used to
visualize discrete variables in ggplot2.}

\end{figure}%

Note, the colour schemes above are useful when applied to visualize a
discrete variable with a small number of different values, such as the
five Australian cities. We will see in Chapter
\hyperref[spatial-data-analysis]{4} how to change the colour scheme when
visualizing a continuous random variable.

\section{Changing the data structure}\label{changing-the-data-structure}

Generally speaking, ggplot2 plots different columns in a data frame
against each other, and each row is considered as a single observation.
However, in practice the data structure may not be as required.

The functions \textbf{pivot\_wider()} and \textbf{pivot\_longer()} in
the \textbf{tidyr} package may provide one way to address this. Before
considering an example, let's load the tidyr package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( tidyr )}
\end{Highlighting}
\end{Shaded}

\subsubsection*{Temperature in Manaus - Comparison of
months}\label{temperature-in-manaus---comparison-of-months}
\addcontentsline{toc}{subsubsection}{Temperature in Manaus - Comparison
of months}

The file ``Manaus Temperature.csv'' provides the average temperature for
each month in the years 1910-2019 for the city of Manaus, Brazil. Let's
load and investigate the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Manaus\_raw }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/Manaus Temperature.csv"}\NormalTok{ )}
\FunctionTok{slice\_head}\NormalTok{( Manaus\_raw, }\AttributeTok{n=}\DecValTok{4}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  YEAR  JAN  FEB  MAR  APR  MAY  JUN  JUL  AUG  SEP  OCT  NOV  DEC
1 1910 27.3 27.0 26.5 26.2 27.2 27.5 27.7 28.0 29.0 28.3 28.3 27.8
2 1911 27.0 27.4 27.4 27.3 27.2 27.0 27.4 28.3 29.0 29.1 28.8 28.3
3 1912 29.0 28.8 28.3 28.0 27.3 28.1 27.4 28.8 28.3 29.3 29.2 27.5
4 1913 27.2 28.1 27.3 27.6 27.0 27.6 27.7 27.7 28.7 28.7 28.8 28.5
\end{verbatim}

We see that each row corresponds to one year of data. If we wanted to
compare two months with each other, the data structure would be ideal
for us to create a scatter plot with ggplot2. Let's compare the
temperatures for January and February:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( Manaus\_raw, }\FunctionTok{aes}\NormalTok{( }\StringTok{"x"}\OtherTok{=}\NormalTok{JAN, }\StringTok{"y"}\OtherTok{=}\NormalTok{FEB ) ) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Average Temperature in January"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Average Temperature in February"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.51\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/unnamed-chunk-18-1.pdf}

}

\caption{Scatter plot of the reported average temperatures for January
and February for Manaus, Brazil.}

\end{figure}%

The plot looks very odd and some of the data points have a value of
\(999.9\). Since such high temperatures are highly unlikely, we conclude
that the value \(999.9\) is used to indicate missing data. Let's replace
these values for January and February by \(\mathrm{\texttt{NA}}\). We
can do this using case\_when() or by using the na\_if() function in
dplyr:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Manaus }\OtherTok{\textless{}{-}}\NormalTok{ Manaus\_raw }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{JAN =} \FunctionTok{na\_if}\NormalTok{( JAN, }\FloatTok{999.9}\NormalTok{ ), }\AttributeTok{FEB =} \FunctionTok{na\_if}\NormalTok{( FEB, }\FloatTok{999.9}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

We can now create the plot again

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( Manaus, }\FunctionTok{aes}\NormalTok{( }\StringTok{"x"}\OtherTok{=}\NormalTok{JAN, }\StringTok{"y"}\OtherTok{=}\NormalTok{FEB ) ) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Average Temperature in January"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Average Temperature in February"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{( }\AttributeTok{axis.title=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{16}\NormalTok{), }\AttributeTok{axis.text=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{14}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: Removed 16 rows containing missing values or values outside the scale range
(`geom_point()`).
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics[width=0.51\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/unnamed-chunk-20-1.pdf}

}

\caption{Scatter plot of the monthly average temperature for January and
February for Manaus, Brazil.}

\end{figure}%

We see that there is a positive correlation between the average monthly
temperatures for Sao Paulo and Rio de Janeiro.

\subsubsection*{Temperature in Manaus - Temperatures across
time}\label{temperature-in-manaus---temperatures-across-time}
\addcontentsline{toc}{subsubsection}{Temperature in Manaus -
Temperatures across time}

Let's assume that we also wanted to explore average temperature over
time. In this case, a line plot would be a good choice. However, the
data frame provides no single column which contains the monthly average
temperatures.

The function pivot\_longer() allows us to change the structure of the
data frame by combining the columns \textbf{JAN} to \textbf{DEC} into a
single column

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Manaus\_long }\OtherTok{\textless{}{-}}\NormalTok{ Manaus\_raw }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{( }\AttributeTok{cols=}\NormalTok{JAN}\SpecialCharTok{:}\NormalTok{DEC, }\AttributeTok{names\_to=}\StringTok{"Month"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{( }\AttributeTok{Temperature =}\NormalTok{ value )}
\FunctionTok{slice\_head}\NormalTok{( Manaus\_long, }\AttributeTok{n=}\DecValTok{3}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 3
   YEAR Month Temperature
  <int> <chr>       <dbl>
1  1910 JAN          27.3
2  1910 FEB          27  
3  1910 MAR          26.5
\end{verbatim}

So we have converted the original table to a narrower but substantially
longer table with 3 columns; the pivot\_longer() function by default
stores the observed monthly averages in a column named \textbf{value},
which we renamed to \textbf{Temperature}.

If we wanted to reverse back to the original format, we would use the
function \textbf{pivot\_wider()},

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Manaus\_wide }\OtherTok{\textless{}{-}}\NormalTok{ Manaus\_long }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{( }\AttributeTok{names\_from=}\NormalTok{Month, }\AttributeTok{values\_from=}\NormalTok{Temperature )}
\end{Highlighting}
\end{Shaded}

Each row in \textbf{Manaus\_long} now corresponds to one month instead
of one year. The one challenge that remains is to convert the variables
\textbf{YEAR} and \textbf{MONTH} into a single variable that represents
the date. There is unfortunately no easy way, but the following code
does the job

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Convert abbreviation for month into number}
\NormalTok{Manaus\_long }\OtherTok{\textless{}{-}} 
\NormalTok{  Manaus\_long }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{Month =} \FunctionTok{case\_when}\NormalTok{( }
\NormalTok{    Month }\SpecialCharTok{==} \StringTok{"JAN"} \SpecialCharTok{\textasciitilde{}} \StringTok{"01"}\NormalTok{, Month }\SpecialCharTok{==} \StringTok{"FEB"} \SpecialCharTok{\textasciitilde{}} \StringTok{"02"}\NormalTok{, Month }\SpecialCharTok{==} \StringTok{"MAR"} \SpecialCharTok{\textasciitilde{}} \StringTok{"03"}\NormalTok{, }
\NormalTok{    Month }\SpecialCharTok{==} \StringTok{"APR"} \SpecialCharTok{\textasciitilde{}} \StringTok{"04"}\NormalTok{, Month }\SpecialCharTok{==} \StringTok{"MAY"} \SpecialCharTok{\textasciitilde{}} \StringTok{"05"}\NormalTok{, Month }\SpecialCharTok{==} \StringTok{"JUN"} \SpecialCharTok{\textasciitilde{}} \StringTok{"06"}\NormalTok{,}
\NormalTok{    Month }\SpecialCharTok{==} \StringTok{"JUL"} \SpecialCharTok{\textasciitilde{}} \StringTok{"07"}\NormalTok{, Month }\SpecialCharTok{==} \StringTok{"AUG"} \SpecialCharTok{\textasciitilde{}} \StringTok{"08"}\NormalTok{, Month }\SpecialCharTok{==} \StringTok{"SEP"} \SpecialCharTok{\textasciitilde{}} \StringTok{"09"}\NormalTok{, }
\NormalTok{    Month }\SpecialCharTok{==} \StringTok{"OCT"} \SpecialCharTok{\textasciitilde{}} \StringTok{"10"}\NormalTok{, Month }\SpecialCharTok{==} \StringTok{"NOV"} \SpecialCharTok{\textasciitilde{}} \StringTok{"11"}\NormalTok{, Month }\SpecialCharTok{==} \StringTok{"DEC"} \SpecialCharTok{\textasciitilde{}} \StringTok{"12"}
\NormalTok{    ) }
\NormalTok{  )}

\DocumentationTok{\#\# Combine year and month and convert to date}
\NormalTok{Manaus\_long }\OtherTok{\textless{}{-}}\NormalTok{ Manaus\_long }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{Date =} \FunctionTok{paste}\NormalTok{( Manaus\_long}\SpecialCharTok{$}\NormalTok{YEAR, Manaus\_long}\SpecialCharTok{$}\NormalTok{Month, }\AttributeTok{sep=}\StringTok{"{-}"}\NormalTok{ ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{Date =} \FunctionTok{ym}\NormalTok{( Date ) )}
\end{Highlighting}
\end{Shaded}

Finally, we can create our line plot. We have to keep in mind that we
still have entries of value 999.9 which we do not want to plot. We again
replace values of 999.9 with \(\mathrm{\texttt{NA}}\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Manaus\_long }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{Temperature =} \FunctionTok{na\_if}\NormalTok{( Temperature, }\FloatTok{999.9}\NormalTok{ ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{(}\StringTok{"x"}\OtherTok{=}\NormalTok{Date, }\StringTok{"y"}\OtherTok{=}\NormalTok{Temperature) ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_smooth}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{y=}\StringTok{"Average Monthly Temperature"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{( }\AttributeTok{axis.title=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{16}\NormalTok{), }\AttributeTok{axis.text=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{14}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = "cs")'
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics[width=0.55\linewidth,height=\textheight,keepaspectratio]{02-DataVisualization_files/figure-pdf/unnamed-chunk-24-1.pdf}

}

\caption{Average monthly temperature for Manaus, Brazil, between 1910
and 2019.}

\end{figure}%

The plot indicates an increase in the average monthly temperature from
about 27.5 to 29 degree Celsius over the time period.

\textbf{Remark:} The last piece of code shows that we can use the pipe
together with the functions from ggplot2.

\section{Summary}\label{summary-1}

We covered some of the principles regarding data visualization:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  When designing your data graphic, ensure that the main message/aspect
  is clearly visible and provide an interpretation of your plot. Avoid
  presenting plots that are not focused towards answering the research
  question.
\item
  The important types of graphics we use for data visualization and
  exploration are line plot, scatter plot, bar plot, histogram, density
  plot, box plot and violin plot.
\item
  Graphics can be discussed in terms of four elements:

  \begin{itemize}
  \item
    Visual cues
  \item
    Coordinate system
  \item
    Scale
  \item
    Context
  \end{itemize}
\item
  The ggplot2 R package provides a wide range of tools for creating data
  graphics from a given data frame. This includes changing the
  coordinate system, the scale and selecting a wide range of types of
  graphics.
\item
  For complex data sets it is often useful to use facets and a wide
  range of visual cues. This may improve accessibility for the reader.
\item
  In practice we often have to restructure the data using the dplyr and
  tidyr packages before creating the data graphic.
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Text Data Analysis}\label{text-data-analysis}

Text data occurs in various forms: short messages (Twitter/X, Instagram,
etc.), websites, books, etc. There are many real-world applications
where we need to extract information from text data. Just to list a few
examples:

\begin{itemize}
\item
  Search engines need to evaluate whether a website is relevant to us;
\item
  Identification of messages (out of billions) that are relevant to
  prevent serious crimes;
\item
  Interactive customer service pages have to provide a good response to
  our question.
\end{itemize}

Text data analysis (also known as \textbf{text mining}) refers to the
area of data science that considers the derivation of information from
text data. These methods are also important in the context of
\textbf{natural language processing}.

In this chapter we explore some important text data analysis techniques.
Section \hyperref[analyzing-word-frequency]{3.1} introduces methods for
visualizing the frequency of words within a text. Section
\hyperref[sentiment-analysis]{3.2} then explores sentiment analysis,
which is concerned with studying the intention of a text. In Section
\hyperref[comparing-text-documents]{3.3} we outline approaches for
comparing text data sets based on word frequency. Finally, Section
\hyperref[topic-modelling]{3.4} outlines a statistical framework for
document classification.

\textbf{Remark:} We will focus on analyzing individual words within a
document, but there are many more aspects that could be studied, such as
the structure of sentences.

\section{Analyzing word frequency}\label{analyzing-word-frequency}

\subsection{The tidy text format}\label{the-tidy-text-format}

Consider the following quote by Hermann Hesse (a German-Swiss poet):

\begin{center}
{\large
  \textit{"Some of us think holding on makes us strong,}\\
\textit{but sometimes it is letting go."}
}
\end{center}

The text is stored in the file ``Hesse quote.txt'' on Moodle, with the
quote being split into two lines as above. To load the data into R, we
use the function \textbf{readLines()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text\_Hesse }\OtherTok{\textless{}{-}} \FunctionTok{readLines}\NormalTok{( }\StringTok{"Data/Hesse quote.txt"}\NormalTok{ )}
\NormalTok{text\_Hesse}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Some of us think holding on makes us strong,"
[2] "but sometimes it is letting go."             
\end{verbatim}

We see that \textbf{text\_Hesse} is a vector with two entries, each
entry corresponding to a line in the .txt file. The entries are of type
\textbf{character} and are also referred to as \textbf{strings}, i.e,
combinations of words.

Let's store the data as a data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quote\_Hesse }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{( }\AttributeTok{line=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\AttributeTok{text=}\NormalTok{text\_Hesse )}
\end{Highlighting}
\end{Shaded}

The text data in its current format is of little use, because we usually
want to analyze single words within a text. As such, we need to separate
the strings into individual words. Further, any punctuation should be
removed because we are not interested in it.

The function \textbf{unnest\_tokens()} in the \textbf{tidytext} R
package does all of this for us:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidytext)}
\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{Hesse\_tidy }\OtherTok{\textless{}{-}}\NormalTok{ quote\_Hesse }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{unnest\_tokens}\NormalTok{( }\AttributeTok{output=}\NormalTok{word, }\AttributeTok{input=}\NormalTok{text )}
\NormalTok{Hesse\_tidy }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{slice\_head}\NormalTok{( }\AttributeTok{n=}\DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  line    word
1    1    some
2    1      of
3    1      us
4    1   think
5    1 holding
\end{verbatim}

The data are now stored in the column \textbf{word}. This format is
known as the \textbf{tidy text} format and it follows two principles:

\begin{itemize}
\item
  Each variable is a column: our variable of interest is \textbf{word};
\item
  Each observation is a row: we have one word per row.
\end{itemize}

Now that the text is in this new format, compared to the previous
chapters, the only difference is that we have to deal with non-numerical
observations. However, there are a wide range of techniques unique to
analyze of such data.

\subsection{\texorpdfstring{Analysis of \emph{Jane
Eyre}}{Analysis of Jane Eyre}}\label{analysis-of-jane-eyre}

When analyzing text data, we often want to extract words which are used
frequently. \textbf{Bar plots} and \textbf{word clouds} are widely
applied graphics to visualize word frequency, i.e.~the number of times a
word appears. In the following, we demonstrate how these plots can be
created in R for the book \emph{Jane Eyre} by Charlotte Bront.

\subsubsection*{Preparing the data}\label{preparing-the-data}
\addcontentsline{toc}{subsubsection}{Preparing the data}

The full text for \emph{Jane Eyre} is freely available from Project
Gutenberg; we only require the Gutenberg ID for the book, which is 1260.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{URL }\OtherTok{\textless{}{-}} \StringTok{"https://www.gutenberg.org/cache/epub/1260/pg1260.txt"}
\NormalTok{JaneEyre\_raw }\OtherTok{\textless{}{-}} \FunctionTok{readLines}\NormalTok{( URL, }\AttributeTok{encoding =} \StringTok{"UTF{-}8"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

We have to remove the meta data and disclaimers at the beginning and end
of the file. We do this by detecting the lines containing the word
``EBOOK'', which signals the beginning and end of the book:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ind }\OtherTok{\textless{}{-}} \FunctionTok{grep}\NormalTok{( }\StringTok{"EBOOK"}\NormalTok{, JaneEyre\_raw )}
\NormalTok{JaneEyre\_raw}\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{( }\AttributeTok{text=}\NormalTok{JaneEyre\_raw[ (ind[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(ind[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{) ] )}
\end{Highlighting}
\end{Shaded}

The individual rows in \textbf{JaneEyre\_raw} represent one line from
the book, as provided in the printed version stored on the Project
Gutenberg website, with the first lines being from the title page.

After importing the data, we separate the lines of text into individual
words using unnest\_tokens():

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{JaneEyre }\OtherTok{\textless{}{-}}\NormalTok{ JaneEyre\_raw }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{unnest\_tokens}\NormalTok{( word, text )}
\end{Highlighting}
\end{Shaded}

A bit more data cleaning is required before starting the analysis.
Specifically, some of the extracted words appear with an ``\_``, such as
the eighth extracted word -''\_illustrated''. This indicates that the
word is printed in italics, but we do not want to tread ``the'' and
``\emph{the}'' differently. To remove these underscores, we apply the
\textbf{gsub()} function we saw in Problem Class 1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{JaneEyre}\SpecialCharTok{$}\NormalTok{word }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{( }\StringTok{"\_"}\NormalTok{, }\StringTok{""}\NormalTok{, JaneEyre}\SpecialCharTok{$}\NormalTok{word )}
\end{Highlighting}
\end{Shaded}

\subsubsection*{Calculating the term
frequency}\label{calculating-the-term-frequency}
\addcontentsline{toc}{subsubsection}{Calculating the term frequency}

With the data in the tidy text format, the function \textbf{count()}
from the dplyr R package can be used to extract the number of times each
word was used. We further calculate each word's proportion amongst the
total number of words, called the \textbf{term frequency}, and its rank:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{JaneEyre\_Count }\OtherTok{\textless{}{-}}\NormalTok{ JaneEyre }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{( word, }\AttributeTok{sort=}\ConstantTok{TRUE}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\StringTok{\textquotesingle{}term frequency\textquotesingle{}} \OtherTok{=}\NormalTok{ n }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(n), }\AttributeTok{rank =} \FunctionTok{row\_number}\NormalTok{() )}
\FunctionTok{slice\_head}\NormalTok{( JaneEyre\_Count, }\AttributeTok{n=}\DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  word    n term frequency rank
1  the 7856     0.04169874    1
2    i 7169     0.03805222    2
3  and 6632     0.03520189    3
4   to 5238     0.02780270    4
5    a 4470     0.02372624    5
\end{verbatim}

Looking at the output, the words with the highest term frequency are the
ones we usually use when writing a longer piece of text.

\subsubsection*{Zipf's Law}\label{zipfs-law}
\addcontentsline{toc}{subsubsection}{Zipf's Law}

Before analyzing the words most commonly used in \emph{Jane Eyre} in
more detail, we create a plot of \textbf{term frequency} versus
\textbf{rank}, both on logarithmic scale:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{ggplot}\NormalTok{( JaneEyre\_Count, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{rank, }\AttributeTok{y=}\StringTok{\textasciigrave{}}\AttributeTok{term frequency}\StringTok{\textasciigrave{}}\NormalTok{ ) ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size=}\FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{coord\_trans}\NormalTok{( }\AttributeTok{x=}\StringTok{"log10"}\NormalTok{, }\AttributeTok{y=}\StringTok{"log10"}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{( }\AttributeTok{axis.title=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{15}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{03-Text-Data-Analysis_files/figure-pdf/unnamed-chunk-9-1.pdf}

}

\caption{Plot of rank versus term frequency for the words in Jane Eyre,
illustrating the negative proportionality stated by Zipf's Law.}

\end{figure}%

There is pretty much a linear relationship between rank and term
frequency on logarithmic scale (apart from the words with the highest
term frequency). This is known as \textbf{Zipf's Law} which states that
empirically a word's term frequency is inversely proportional to its
rank.

\textbf{Remark:} Creating this plot is not necessary in a text analysis.
We just produced it to illustrate Zipf's Law.

\subsubsection*{Removing stop words}\label{removing-stop-words}
\addcontentsline{toc}{subsubsection}{Removing stop words}

We saw that the five words with the highest term frequency in
\textbf{JaneEyre\_Count} include ``the'', ``I'' and ``and''. This is
quite common when analyzing longer pieces of text. One may argue that
such words are not of interest in an analysis, because we cannot write a
text without using them.

This has led to the concept of \textbf{stop words}. The idea is to
specify words that are dropped from the analysis, because they are not
relevant. A set of stop words is called a \textbf{stop list}.

The tidytext R package provides its own list, \textbf{stop\_words},
which includes 1149 stop words. We can remove these stop words from
\textbf{JaneEyre\_Count} using the function \textbf{anti\_join()} from
the dplyr R package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{( stop\_words )}
\NormalTok{JaneEyre\_Count }\OtherTok{\textless{}{-}}\NormalTok{ JaneEyre\_Count }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{anti\_join}\NormalTok{( stop\_words )}
\FunctionTok{slice\_head}\NormalTok{( JaneEyre\_Count, }\AttributeTok{n=}\DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       word   n term frequency rank
1      jane 341    0.001809988   69
2 rochester 317    0.001682599   71
3       sir 316    0.001677291   72
4      miss 310    0.001645444   73
5      time 244    0.001295124   98
\end{verbatim}

\subsubsection*{Visualizing word frequency using bar
plots}\label{visualizing-word-frequency-using-bar-plots}
\addcontentsline{toc}{subsubsection}{Visualizing word frequency using
bar plots}

Let's create a bar plot for the most commonly used words in \emph{Jane
Eyre} that are not on the stop list:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{JaneEyre\_Count }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_head}\NormalTok{( }\AttributeTok{n=}\DecValTok{10}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{word =} \FunctionTok{reorder}\NormalTok{(word,n) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{n, }\AttributeTok{y=}\NormalTok{word ) ) }\SpecialCharTok{+} \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Count"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Word"}\NormalTok{ ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{( }\AttributeTok{axis.title=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{17}\NormalTok{), }\AttributeTok{axis.text=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{15}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{03-Text-Data-Analysis_files/figure-pdf/unnamed-chunk-11-1.pdf}

}

\caption{Word frequency of the most commonly used words in Jane Eyre.}

\end{figure}%

We see that the names of two main characters, Jane Eyre and Edward
Rochester, and their titles appear the most often, followed by ``time'',
``day'', ``looked'' and ``night''.

\textbf{Remark:} When using ggplot2, words are by default ordered in
alphabetical order, which is not what we want here. Therefore,
\textbf{mutate( word = reorder(word,n) )} is used to ensure that the
words are instead ordered based on word frequency.

\subsubsection*{Word clouds}\label{word-clouds}
\addcontentsline{toc}{subsubsection}{Word clouds}

Word clouds are a type of graphic that we can use to visualize the
frequency of words within a text. Instead of using the visual cues
``position'' and ``length'' as in a bar plot, frequency is illustrated
via the ``size'' of the words.

The R package \textbf{wordcloud} provides a function to produce a word
cloud. For \emph{Jane Eyre}, we produce a word cloud of the 40 most
common words (ignoring the stop words) as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(wordcloud)}
\NormalTok{JaneEyre\_Count }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{with}\NormalTok{( }\FunctionTok{wordcloud}\NormalTok{( word, n, }\AttributeTok{max.words=}\DecValTok{40}\NormalTok{, }\AttributeTok{colors=}\FunctionTok{topo.colors}\NormalTok{(}\AttributeTok{n=}\DecValTok{40}\NormalTok{) ) )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{03-Text-Data-Analysis_files/figure-pdf/unnamed-chunk-12-1.pdf}

}

\caption{Word cloud illustrating the frequency of the 40 most common
words in Jane Eyre.}

\end{figure}%

\textbf{Remark:} Word clouds are good when we want to visualize term
frequency for a large number of words. In all other cases, a bar plot is
the better choice in terms of accessibility and the amount of
information it provides.

\section{Sentiment analysis}\label{sentiment-analysis}

\subsection{Measuring sentiment}\label{measuring-sentiment}

In many applications we want to understand the emotional intent
(\textbf{sentiment}) of a text. For instance, we may want to quickly
determine whether a product has generally received more positive than
negative reviews or not.

Here we make the (strong) assumption that the sentiment of a text can be
described by the aggregated sentiment of the individual words within it.
This leads to the task of measuring the sentiment of a word. In text
mining and natural language processing, sentiment lexicons are usually
used when analyzing individual words. The \textbf{tidydata} R package
provides access two sentiment lexicons:

\begin{itemize}
\item
  \textbf{AFINN}: Words are assigned a sentiment score between -5 and
  +5, with lower values corresponding to a more negative sentiment. For
  instance, ``anxious'' has a score of -2, while ``pretty'' has a score
  of 1.
\item
  \textbf{Bing}: Words are categorized as ``positive'' (e.g.~``pretty'')
  or ``negative'' (e.g.~``anxious'').
\end{itemize}

Words not listed within the lexicon are considered ``neutral'' in terms
of sentiment. We now apply these two sentiment lexicons to analyze the
sentiment in \emph{Jane Eyre}.

\subsection{\texorpdfstring{Analysis of sentiment for \emph{Jane
Eyre}}{Analysis of sentiment for Jane Eyre}}\label{analysis-of-sentiment-for-jane-eyre}

To load the AFINN sentiment lexicon, we use the
\textbf{get\_sentiments()} function in the tidytext package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AFINN }\OtherTok{\textless{}{-}} \FunctionTok{get\_sentiments}\NormalTok{( }\StringTok{"afinn"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

Let's again prepare the data and store the line number for later use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{JaneEyre }\OtherTok{\textless{}{-}}\NormalTok{ JaneEyre\_raw }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{line =} \FunctionTok{row\_number}\NormalTok{() ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{( word, text ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{word =} \FunctionTok{gsub}\NormalTok{( }\StringTok{"\_"}\NormalTok{, }\StringTok{""}\NormalTok{, word ) )}
\end{Highlighting}
\end{Shaded}

The next step is to extract the words that are in both \emph{Jane Eyre}
and the AFINN sentiment lexicon:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{JaneEyre\_AFINN }\OtherTok{\textless{}{-}} \FunctionTok{inner\_join}\NormalTok{( JaneEyre, AFINN )}
\end{Highlighting}
\end{Shaded}

It is important to note that sentiment lexicons have certain
limitations. For instance, the word ``miss'' is associated with a
negative sentiment within the AFINN sentiment lexicon, but in \emph{Jane
Eyre} it is usually used as the title of a young, unmarried woman - so
we should better ignore the word ``miss'' in the analysis.

\textbf{Important:} When considering sentiment, we should not remove any
stop words to ensure that our analysis considers all words in the text.

Let's create a plot to see how the aggregated sentiment evolves over the
course of the novel, and we filter out the word ``miss'' based on our
previous argument:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{JaneEyre\_AFINN }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{( word }\SpecialCharTok{!=} \StringTok{"miss"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{sentiment =} \FunctionTok{cumsum}\NormalTok{( value ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{line, }\AttributeTok{y=}\NormalTok{sentiment ) ) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{( }\AttributeTok{linewidth=}\FloatTok{1.2}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Line number"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Sentiment"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{03-Text-Data-Analysis_files/figure-pdf/AFINN-1.pdf}

}

\caption{Evolution of the AFINN sentiment score over the course of the
book Jane Eyre. Negative values at the start indicate negative
sentiment, but the sentiment becomes higher as the book comes to its
conclusion.}

\end{figure}%

The plot suggests that the first chapters of \emph{Jane Eyre} have a
more ``negative'' sentiment, while, as the story progresses, the novel
develops a more ``positive'' sentiment.

Another way to visualize sentiment is to group lines into sections and
to derive the sentiment of each section. Here we split the book into its
chapters. We need some functions form the \textbf{stringr} package to
identify the lines which start with ``Chapter'' and we then use this
information to assign a chapter number to each line:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(stringr)}
\NormalTok{JaneEyre\_chapters }\OtherTok{\textless{}{-}}\NormalTok{ JaneEyre\_raw }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{chapter =} \FunctionTok{cumsum}\NormalTok{( }
    \FunctionTok{str\_detect}\NormalTok{( text, }\FunctionTok{regex}\NormalTok{(}\StringTok{"\^{}chapter "}\NormalTok{, }\AttributeTok{ignore\_case =} \ConstantTok{TRUE}\NormalTok{) )}
\NormalTok{    ) )}
\end{Highlighting}
\end{Shaded}

We now remove any lines that do not belong to one of the chapters and
then split the book as before:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{JaneEyre\_chapters }\OtherTok{\textless{}{-}}\NormalTok{ JaneEyre\_chapters }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{( chapter }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{( word, text ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{word =} \FunctionTok{gsub}\NormalTok{( }\StringTok{"\_"}\NormalTok{, }\StringTok{""}\NormalTok{, word ) ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{( word }\SpecialCharTok{!=} \StringTok{"miss"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

Finally, we calculate and visualize the aggregated sentiment for each
chapter

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{JaneEyre\_chapters }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{( AFINN ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( chapter ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{sentiment =} \FunctionTok{sum}\NormalTok{(value) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{chapter, }\AttributeTok{y=}\NormalTok{sentiment ) ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Chapter"}\NormalTok{, }\AttributeTok{y=}\StringTok{"AFINN sentiment score"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{03-Text-Data-Analysis_files/figure-pdf/unnamed-chunk-18-1.pdf}

}

\caption{AFINN sentiment score for the chapters of Jane Eyre.}

\end{figure}%

The plot shows a similar pattern to that identified for Figure
@ref(fig:AFINN). If we consider a chapter with an AFINN score above 0 as
``positive'', and ``negative'' otherwise, we find that the sentiment
tends to remain ``positive'' across multiple consecutive chapters.

When working with the Bing sentiment lexicon, we may count the numbers
of ``positive'' and ``negative'' words in each chapter and visualize
their proportions numbers using a stacked bar plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Bing }\OtherTok{\textless{}{-}} \FunctionTok{get\_sentiments}\NormalTok{( }\StringTok{"bing"}\NormalTok{ )}

\NormalTok{JaneEyre\_chapters }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{( Bing ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( chapter ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{( chapter, sentiment ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{chapter, }\AttributeTok{y=}\NormalTok{n, }\AttributeTok{fill=}\NormalTok{sentiment ) ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{position=}\StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{color=}\StringTok{"black"}\NormalTok{, }\AttributeTok{linewidth=}\FloatTok{1.1}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Chapter"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Proportion"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{03-Text-Data-Analysis_files/figure-pdf/unnamed-chunk-19-1.pdf}

}

\caption{Bing sentiment score for the chapters of Jane Eyre.}

\end{figure}%

When comparing the two bar plots, and assuming that a chapter with a
proportion of ``positive'' words above 50\% is ``positive'', we find
good agreement in terms of whether a chapter has a ``positive'' or
``negative'' sentiment.

\textbf{Important:} Due to our assumption that the sentiment can be
measured by considering the individual words, we should be cautious with
making conclusions on whether a chapter is ``positive'' or ``negative''.
Nevertheless, the change in sentiment score still provides us with some
information on which chapters tell a more ``positive'' story and which
ones a more ``negative'' story, that is, we can still make conclusions
by comparing the scores.

\section{Comparing text documents}\label{comparing-text-documents}

\subsection{Term frequency}\label{term-frequency}

Suppose that we have two separate texts / documents. In such cases, we
may want to compare the term frequency of the various words in the two
texts using a scatter plot.

Let's compare \emph{Jane Eyre} to the novel \emph{Wuthering Heights} by
Emily Bront, Charlotte Bront's sister. The first step is to calculate
term frequencies for \emph{Wuthering Heights}, just as we did for
\emph{Jane Eyre}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{URL }\OtherTok{\textless{}{-}} \StringTok{"https://www.gutenberg.org/files/768/768{-}0.txt"}
\NormalTok{WutheringHeights\_raw }\OtherTok{\textless{}{-}} \FunctionTok{readLines}\NormalTok{( URL, }\AttributeTok{encoding =} \StringTok{"UTF{-}8"}\NormalTok{ )}
\NormalTok{ind }\OtherTok{\textless{}{-}} \FunctionTok{grep}\NormalTok{( }\StringTok{"EBOOK"}\NormalTok{, WutheringHeights\_raw )}
\NormalTok{WutheringHeights\_raw}\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{( }\AttributeTok{text=}\NormalTok{WutheringHeights\_raw[ (ind[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(ind[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{) ] )}

\NormalTok{WutheringHeights }\OtherTok{\textless{}{-}}\NormalTok{ WutheringHeights\_raw }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unnest\_tokens}\NormalTok{(word, text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{word =} \FunctionTok{gsub}\NormalTok{( }\StringTok{"\_"}\NormalTok{, }\StringTok{""}\NormalTok{, word ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{( word, }\AttributeTok{sort=}\ConstantTok{TRUE}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\StringTok{\textquotesingle{}term frequency\textquotesingle{}} \OtherTok{=}\NormalTok{ n }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(n) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{( stop\_words )}
\end{Highlighting}
\end{Shaded}

To compare term frequencies, we combine the two data frames for
\emph{Jane Eyre} and \emph{Wuthering Heights}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frequency\_combined }\OtherTok{\textless{}{-}} 
  \FunctionTok{full\_join}\NormalTok{( JaneEyre\_Count, WutheringHeights, }\AttributeTok{by=}\StringTok{"word"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{( }\AttributeTok{JaneEyre=}\StringTok{\textquotesingle{}term frequency.x\textquotesingle{}}\NormalTok{, }\AttributeTok{Heights=}\StringTok{\textquotesingle{}term frequency.y\textquotesingle{}}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

Before analyzing the term frequencies, we have to account for words that
only appear in one book being given a value of \textbf{NA}, such as the
word ``Jane''. Therefore, we replace any \(\mathrm{\texttt{NA}}\) entry
with with a value of 0 (as we did in Problem Sheet 2, Tutorial Question
2):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frequency\_combined }\OtherTok{\textless{}{-}}\NormalTok{ frequency\_combined }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{JaneEyre =} \FunctionTok{case\_when}\NormalTok{( }\FunctionTok{is.na}\NormalTok{(JaneEyre) }\SpecialCharTok{==} \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{, }\AttributeTok{.default =}\NormalTok{ JaneEyre),}
          \AttributeTok{Heights =} \FunctionTok{case\_when}\NormalTok{( }\FunctionTok{is.na}\NormalTok{(Heights) }\SpecialCharTok{==} \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{, }\AttributeTok{.default =}\NormalTok{ Heights) )}
\end{Highlighting}
\end{Shaded}

Finally, we create our scatter plot of the term frequencies

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( frequency\_combined, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{JaneEyre, }\AttributeTok{y=}\NormalTok{Heights ) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_text}\NormalTok{( }\FunctionTok{aes}\NormalTok{(}\AttributeTok{label=}\NormalTok{word), }\AttributeTok{check\_overlap =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{vjust=}\FloatTok{1.5}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{xlim}\NormalTok{( }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.003}\NormalTok{) ) }\SpecialCharTok{+} \FunctionTok{ylim}\NormalTok{( }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.003}\NormalTok{) ) }\SpecialCharTok{+} 
  \FunctionTok{coord\_trans}\NormalTok{( }\AttributeTok{x=}\StringTok{"sqrt"}\NormalTok{, }\AttributeTok{y=}\StringTok{"sqrt"}\NormalTok{ ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Term Frequency in Jane Eyre"}\NormalTok{,}
        \AttributeTok{y=}\StringTok{"Term Frequency in Wuthering Heights"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{03-Text-Data-Analysis_files/figure-pdf/unnamed-chunk-23-1.pdf}

}

\caption{Comparison of the term frequencies for the words in Jane Eyre
and Wuthering Heights.}

\end{figure}%

Words close to the line, such as ``door'' or ``time'', have a similar
frequency in the two books while words that are far from the line, for
instance, ``Catherine'' and ``John'', are more specific to one of the
books.

\subsection{Term frequency-inverse document
frequency}\label{term-frequency-inverse-document-frequency}

We already saw in the analysis of \emph{Jane Eyre} that we use
\textbf{term frequency} to measure frequency of a word \(t\) within a
text or document \(d\). The formal definition of term frequency is {[}
\mathrm{tf}(t,d)=\frac{\mathrm{Counts~of}~t~\mathrm{within}~d}{\mathrm{Number~of~words~within}~d}.
{]}

Suppose that we now want to study the frequency of a word \(t\) across a
\textbf{corpus} \(D\), that is, a set of documents. The \textbf{inverse
document frequency} \(\mathrm{idf}(t,D)\) measures how common or rare
the word \(t\) is across \(D\). Formally, \(\mathrm{idf}(t,D)\) is
defined as\\
{[} \mathrm{idf}(t,D) =
\log \left(\frac{\mathrm{Number~of~documents}}{\mathrm{Number~of~documents~that~contain~the~word}~t}\right)
= \log\left(\frac{|D|}{|\{d\in D:t\in d\}|}\right), {]} where \(|D|\)
denotes the cardinality of the set \(D\), i.e., the number of documents
in \(D\).

We can also say that \(\mathrm{idf}(t,D)\) describes how specific the
word \(t\) is to a document within \(D\). If \(t\) is contained in all
documents, we get \(\mathrm{idf}(t,D)=0\), while
\(\mathrm{idf}(t,D)=\log |D|\) if \(t\) only appears in a single
document. Consequently, a smaller value for \(\mathrm{idf}(t,D)\)
corresponds to \(t\) being more common across the documents in \(D\).

Finally, we define the \textbf{term frequency-inverse document frequency
(tf-idf)} as {[} \mathrm{tf.idf}(t,d,D)=\mathrm{tf}(t,d)
\times \mathrm{idf}(t,D). {]}

Let's consider some scenarios

\begin{itemize}
\item
  To get a very high value for \(\mathrm{tf.idf}(t,d,D)\), we need \(t\)
  to occur frequently within the document \(d\), and not in any other
  document in \(D\).
\item
  If \(t\) occurs across all documents in the set \(D\), we have
  \(\mathrm{tf.idf}(t,d,D)=0\). For instance, very common words such as
  \(t\)=``the'' or \(t\)=``and'' highly likely yield
  \(\mathrm{tf.idf}(t,d,D)=0\). This is a nice feature, because we are
  usually not interested in these stop words.
\end{itemize}

So the term frequency-inverse document frequency is useful to assess how
important a word is to a document, in relation to the overall set of
documents. This feature is the reason that tf-idf is widely applied in
search engines and text-based recommender systems. For instance, if we
want to search for a website on a specific subject, described by a word
\(t\), the suggestions provided by the search engine may use
\(\mathrm{tf.idf}(t,d,D)\) to rank websites, where \(D\) comprises all
the websites considered by the search engine.

\subsubsection*{Analyzing some books by Charles
Dickens}\label{analyzing-some-books-by-charles-dickens}
\addcontentsline{toc}{subsubsection}{Analyzing some books by Charles
Dickens}

Suppose our corpus comprises four books by Charles Dickens: \emph{A
Christmas Carol}, \emph{A Tale of Two Cities}, \emph{Great Expectations}
and \emph{Oliver Twist}. The text for the books from Project Gutenberg
is available as ``Dickens.csv'' on Moodle:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dickens\_raw }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/Dickens.csv"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

As in the analysis of \emph{Jane Eyre} and \emph{Wuthering Heights}, we
use the unnest\_tokens() function to separate the text into individual
words, and we use gsub() to remove any underscores:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dickens }\OtherTok{\textless{}{-}}\NormalTok{ Dickens\_raw }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{( word, text ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{word =} \FunctionTok{gsub}\NormalTok{( }\StringTok{"\_"}\NormalTok{, }\StringTok{""}\NormalTok{, word ) )}
\end{Highlighting}
\end{Shaded}

Next we count the number of appearances of each word separately for each
of the books:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DickensCount }\OtherTok{\textless{}{-}}\NormalTok{ Dickens }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{count}\NormalTok{( title, word, }\AttributeTok{sort=}\ConstantTok{TRUE}\NormalTok{ )}
\FunctionTok{head}\NormalTok{( DickensCount )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                 title word    n
1         Oliver Twist  the 9633
2   Great Expectations  the 8145
3 A Tale of Two Cities  the 8053
4   Great Expectations  and 7098
5   Great Expectations    i 6667
6         Oliver Twist  and 5428
\end{verbatim}

The quantities \(\mathrm{tf}(t,d)\), \(\mathrm{idf}(t,D)\) and
\(\mathrm{tf.idf}(t,d,D)\) for all terms and books are then calculated
from this table using the function \textbf{bind\_tf\_idf()} in the
tidytext package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dickens\_tf.idf }\OtherTok{\textless{}{-}}\NormalTok{ DickensCount }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{bind\_tf\_idf}\NormalTok{( word, title, n ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{( }\FunctionTok{desc}\NormalTok{(tf\_idf) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{idf=}\FunctionTok{round}\NormalTok{(idf,}\DecValTok{2}\NormalTok{) )}
\FunctionTok{head}\NormalTok{( Dickens\_tf.idf )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                 title    word   n          tf  idf      tf_idf
1    A Christmas Carol scrooge 327 0.011047671 1.39 0.015315323
2         Oliver Twist  oliver 876 0.005397013 1.39 0.007481849
3 A Tale of Two Cities   lorry 369 0.002664549 1.39 0.003693849
4         Oliver Twist  bumble 397 0.002445907 1.39 0.003390747
5         Oliver Twist   sikes 354 0.002180985 1.39 0.003023487
6 A Tale of Two Cities defarge 302 0.002180742 1.39 0.003023150
\end{verbatim}

We see that the words with the highest tf-idf are the names of the
protagonists in the four books, while the term ``the'', that had the
highest count, is not there. It's also common to say that ``scrooge'' is
the most specific term across the considered set of books. The names
with the highest tf-idf are also specific to a book and that's why they
have an idf of \(\log 4 \approx 1.39\).

\section{Topic modelling}\label{topic-modelling}

So far, we have extracted the most common words within a text, and
analyzed its sentiment. However, this information may not be sufficient
in applications, e.g.:

\begin{itemize}
\item
  When working with a large collection of blog posts, news articles or
  scientific papers, we often wish to label these documents based on
  their content. It may be difficult to do this just based on the most
  common words.
\item
  Summarizing a long piece of text in a few sentences. We cannot assume
  that the most common words or the sentiment are necessarily the
  important pieces of information.
\end{itemize}

This has led to the development of several statistical and natural
language processing methods. In this chapter we introduce \textbf{topic
modeling}, which aims to to discover ``natural'' groups (topics) of
words, even when we are not sure what we are looking for.

\subsection{Latent Dirichlet
allocation}\label{latent-dirichlet-allocation}

While there exists several approaches for topic modelling, we only
explore \textbf{Latent Dirichlet allocation (LDA)}, which is a
particularly popular method. The key idea is to treat each document as a
mixture of topics, and each topic as a mixture of words. Let's
illustrate these principles:

\begin{itemize}
\item
  \textbf{Every document is a mixture of topics.} We imagine that each
  document may contain words from several topics in particular
  proportions. For example, in a two-topic model we could say ``Document
  1 is 90\% topic A and 10\% topic B, while Document 2 is 30\% topic A
  and 70\% topic B.''
\item
  \textbf{Every topic is a mixture of words.} Suppose we considered BBC
  news and had topics such as ``politics'', ``sports'',
  ``entertainment'', etc. The most common words in the ``politics''
  topic might be ``prime minister'', ``Commons'', and ``government'',
  while the ``sports'' topic includes ``football'', ``cricket'', etc.
  Importantly, words can be shared between topics; a word like ``bench''
  might appear in both topics with the same frequency.
\end{itemize}

This framework allows documents to ``overlap'' each other in terms of
content, in a way that mirrors typical use of natural language, rather
than being separated into discrete groups. Based on a set of documents,
the aim is then to find the mixture of words that is associated with
each topic, while also determining the mixture of topics that describes
each document at the same time.

\textbf{Remark:} The framework above is different from the clustering
methods you may have seen before, e.g., such as \(k\)-means where each
data point belongs to exactly one cluster. LDA belongs to the class of
soft (or fuzzy) clustering methods, which allow for a data point (in
this case a document) to belong to more than one cluster.

\subsection{Mathematical background}\label{mathematical-background}

Let's dive briefly into the mathematics behind the LDA approach before
focusing on exploring how to fit the model and interpret the results. We
start by introducing the Dirichlet distribution:

\textbf{Definition:} The Dirichlet distribution is a family of
continuous multivariate probability distributions on the d-dimensional
unit simplex \(\{(x_1,\ldots,x_d)\in\mathbb{R}_+^d: x_1+\cdots+x_d=1\}\)
with \(d>1\). A Dirichlet distribution is parametrized by a
\(d\)-dimensional vector of positive reals.

Now suppose we have a corpus with \(N\) documents, with a total of \(M\)
different words, and we specify that there are different \(K\) topics.
The LDA approach then describes two \textbf{Dirichlet distributions}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Distribution of words within a topic:} The term frequencies
  with which the \(M\) distinct words are expected to appear within a
  document from topic \(k\in\{1,\ldots,K\}\) is described by a
  M-dimensional Dirichlet distribution, where each topic has its own set
  of parameters.
\item
  \textbf{Distribution of topics within a document:} The proportions
  \((\psi_{i,1},\ldots,\psi_{i,K})\) with which the \(K\) topics feature
  in document \(i\in\{1,\ldots,N\}\) are modelled via a
  \(K\)-dimensional Dirichlet distribution. These proportions allow us
  to group the different documents.
\end{enumerate}

We will stop considering the theoretical details of the LDA approach at
this point, because the estimation of the parameter vectors is too
complex to be covered in a Year 2 unit (we would require tools from
Bayesian statistics). In the following, we study two examples to
illustrate how to perform topic modelling in R.

\subsection{Example 1: The work by Charles
Dickens}\label{example-1-the-work-by-charles-dickens}

To illustrate LDA we first consider an example where we know the
`truth'. Here we will take two books by Charles Dickens and split each
up into their individual chapters. We then want to see how well LDA
performs at distinguishing between the two books.

We consider the books \emph{Great Expectations} and \emph{A Tale of Two
Cities} we studied previously:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dickens\_raw }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/Dickens.csv"}\NormalTok{ )}
\NormalTok{Dickens }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{( Dickens\_raw, }
\NormalTok{                   title }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{( }\StringTok{"Great Expectations"}\NormalTok{,}\StringTok{"A Tale of Two Cities"}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

\subsubsection*{Preparing the data}\label{preparing-the-data-1}
\addcontentsline{toc}{subsubsection}{Preparing the data}

We want to split the books into their chapters, with each chapter being
handled as a separate document. As in Section 3.2, we extract the
chapter the individual lines in the text document belong to:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dickens\_chapters }\OtherTok{\textless{}{-}}\NormalTok{ Dickens }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( title ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{chapter =} \FunctionTok{cumsum}\NormalTok{( }
    \FunctionTok{str\_detect}\NormalTok{( text, }\FunctionTok{regex}\NormalTok{( }\StringTok{"\^{}chapter "}\NormalTok{, }\AttributeTok{ignore\_case =} \ConstantTok{TRUE}\NormalTok{ ) ) }
\NormalTok{  ) )}
\end{Highlighting}
\end{Shaded}

The next step is to remove any text before the first chapter, and we
further us the unite() function from the tidyr R package to create an
identifier \textbf{document} from the book title and chapter number:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyr)}
\NormalTok{Dickens\_chapters }\OtherTok{\textless{}{-}}\NormalTok{ Dickens\_chapters }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{( chapter }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unite}\NormalTok{( }\AttributeTok{col=}\NormalTok{document, title, chapter )}
\end{Highlighting}
\end{Shaded}

Finally, we split the documents into words, remove any stop words, and
count the frequency for each word within each document, as we did when
analyzing word frequency:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dickens\_chapters }\OtherTok{\textless{}{-}}\NormalTok{ Dickens\_chapters }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{( word, text ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{word =} \FunctionTok{gsub}\NormalTok{( }\StringTok{"\_"}\NormalTok{, }\StringTok{""}\NormalTok{, word ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{( stop\_words, }\AttributeTok{by=}\StringTok{"word"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{( document, word, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\textbf{Remark:} Stop words are removed because they usually perform
poorly at explaining differences in the underlying topics. Further,
their high prevalence in all topics leads to topics appearing more
similar than they are.

\subsubsection*{Defining the document term
matrix}\label{defining-the-document-term-matrix}
\addcontentsline{toc}{subsubsection}{Defining the document term matrix}

The R package we are using for topic modelling requires the data to be
stored as a document term matrix which takes the following form:

\begin{itemize}
\item
  Each row represents one document (a book chapter in our case).
\item
  Each column corresponds to one term / word.
\item
  Each entry contains the number of appearances of a term in a document.
\end{itemize}

The tidytext package provides the \textbf{cast\_dtm()} function to
create such a matrix, but we need to install the \textbf{tm} R package
to do this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dickens\_dtm }\OtherTok{\textless{}{-}}\NormalTok{ Dickens\_chapters }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{cast\_dtm}\NormalTok{( document, word, n )}
\end{Highlighting}
\end{Shaded}

\subsubsection*{Performing LDA}\label{performing-lda}
\addcontentsline{toc}{subsubsection}{Performing LDA}

With everything in place, we now use the \textbf{LDA()} function in the
\textbf{topicmodels} R package. One key decision we have to make is
specifying the number \(K\) of topics. For the purpose of this analysis,
it seems logical to set the number of topics to \(K=2\):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( topicmodels )}
\NormalTok{Dickens\_LDA }\OtherTok{\textless{}{-}} \FunctionTok{LDA}\NormalTok{( Dickens\_dtm, }\AttributeTok{k=}\DecValTok{2}\NormalTok{, }\AttributeTok{method =} \StringTok{"Gibbs"}\NormalTok{, }\AttributeTok{control=}\FunctionTok{list}\NormalTok{(}\AttributeTok{seed=}\DecValTok{123}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\textbf{Remark:} The estimates are sensitive to the value we specify in
\textbf{control=list(seed=\ldots)}. In this course you may ignore the
sensitivity and focus on analyzing the results obtained.

\subsubsection*{Analyzing the estimates}\label{analyzing-the-estimates}
\addcontentsline{toc}{subsubsection}{Analyzing the estimates}

Having fitted the model, we want to explore the estimated model
parameters. Recall that we have

\begin{itemize}
\item
  \(K\) parameters per document, which represent how much the \(K\)
  topics feature in the document
\item
  \(M\) parameters per topic, which give insight on how frequent each
  word features within a topic
\end{itemize}

To extract the \(K\) proportions for each of the documents, we use the
following R code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dickens\_topics }\OtherTok{\textless{}{-}} \FunctionTok{tidy}\NormalTok{( Dickens\_LDA , }\AttributeTok{matrix =} \StringTok{"gamma"}\NormalTok{ ) }
\NormalTok{Dickens\_topics }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{slice\_head}\NormalTok{( }\AttributeTok{n=}\DecValTok{4}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 3
  document              topic gamma
  <chr>                 <int> <dbl>
1 Great Expectations_57     1 0.285
2 Great Expectations_7      1 0.185
3 Great Expectations_38     1 0.371
4 Great Expectations_17     1 0.259
\end{verbatim}

The values in the column \textbf{gamma} correspond to the estimated
proportions. We see that Chapter 7, 17, 38 and 57 from \emph{Great
Expectations} feature more of Topic 2 than Topic 1.

Given the high number of chapters, it's better to visualize the
estimates in order to discuss agreement between the estimated topics and
the underlying truth. One option is to create box plots:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dickens\_topics }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{separate}\NormalTok{( document, }\FunctionTok{c}\NormalTok{(}\StringTok{"title"}\NormalTok{, }\StringTok{"chapter"}\NormalTok{), }\AttributeTok{sep=}\StringTok{"\_"}\NormalTok{, }\AttributeTok{convert=}\ConstantTok{TRUE}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\FunctionTok{factor}\NormalTok{(topic), }\AttributeTok{y=}\NormalTok{gamma ) ) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{title ) }\SpecialCharTok{+} \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Topic"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Proportion"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{03-Text-Data-Analysis_files/figure-pdf/unnamed-chunk-35-1.pdf}

}

\caption{Illustration of the estimated proportions for each chapter
within each book.}

\end{figure}%

We see a clear difference in the plots for the two books. Most of the
chapters from \emph{A Tale of Two Cities} mostly consist of words that
belong to Topic 1, while for \emph{Great Expectations} the chapters are
composed of words from Topic 2. As such, the topics could be used to
classify the individual chapters.

Suppose we would label a chapter as ``A Tale of Two Cities'' if the
proportion of text estimated to be from Topic 1 exceeds 0.5. Such a
classification would work well, with only one chapter per book being
misclassified. This result may be due to some chapters rarely mentioning
important characters and given that both books were written by Charles
Dickens, correctly labeling these chapters is more challenging.

Let's look at the words most common in the different topics. We extract
the top five words for each topic:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tidy}\NormalTok{( Dickens\_LDA , }\AttributeTok{matrix =} \StringTok{"beta"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( topic ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_max}\NormalTok{( beta, }\AttributeTok{n=}\DecValTok{3}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
# Groups:   topic [2]
  topic term       beta
  <int> <chr>     <dbl>
1     1 lorry   0.00702
2     1 hand    0.00643
3     1 defarge 0.00574
4     2 joe     0.0143 
5     2 miss    0.00941
6     2 don     0.00765
\end{verbatim}

It seems that character names plan an important role when deciding
whether a text belongs to Topic 1 or Topic 2, which is kind of what we
expect. Let's produce a plot to compare the term frequencies for the
different words in more detail:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tidy}\NormalTok{( Dickens\_LDA , }\AttributeTok{matrix =} \StringTok{"beta"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{topic =} \FunctionTok{case\_when}\NormalTok{( topic}\SpecialCharTok{==}\DecValTok{1} \SpecialCharTok{\textasciitilde{}} \StringTok{"Topic1"}\NormalTok{, topic}\SpecialCharTok{==}\DecValTok{2} \SpecialCharTok{\textasciitilde{}} \StringTok{"Topic2"}\NormalTok{) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{( }\AttributeTok{names\_from =}\NormalTok{ topic, }\AttributeTok{values\_from =}\NormalTok{ beta, }\AttributeTok{values\_fill =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Topic1, }\AttributeTok{y=}\NormalTok{Topic2) ) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}  
  \FunctionTok{geom\_text}\NormalTok{( }\FunctionTok{aes}\NormalTok{(}\AttributeTok{label=}\NormalTok{term), }\AttributeTok{check\_overlap =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{vjust=}\DecValTok{1}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{coord\_trans}\NormalTok{( }\AttributeTok{x=}\StringTok{"sqrt"}\NormalTok{, }\AttributeTok{y=}\StringTok{"sqrt"}\NormalTok{ ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Term Frequency in Topic 1"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Term Frequency in Topic 2"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{03-Text-Data-Analysis_files/figure-pdf/unnamed-chunk-37-1.pdf}

}

\caption{Comparison of term frequencies for the two topics.}

\end{figure}%

The plot shows some very interesting patterns. In particular, the two
topics seem to mainly differ in terms of the term frequency of a subset
of words (the ones located close to the axes), while the term
frequencies are very similar for the majority of words.

\subsection{Example 2: Articles in the New York
Times}\label{example-2-articles-in-the-new-york-times}

Let's look at an example where there is no `truth' as such. The file
``NYT.csv'' provides metadata for 1289 articles published by the New
York Times in their ``Europe'' subsection between 01/01/2023 and
01/11/2024:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NYT }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/NYT.csv"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

Specifically, the data file gives for each article the lead paragraph
and the date it was published. We want to explore the results we obtain
when applying LDA.

\subsubsection*{Fitting the LDA model}\label{fitting-the-lda-model}
\addcontentsline{toc}{subsubsection}{Fitting the LDA model}

Since we take each article to be a separate document, we first assign a
unique identifier to each article:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NYT }\OtherTok{\textless{}{-}}\NormalTok{ NYT }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{( }\AttributeTok{ID=}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(NYT) )}
\end{Highlighting}
\end{Shaded}

Now we follow the same procedure as in the previous analysis. As such,
we start by splitting the sentences into individual words, remove any
stop words and count the number of occurrences for each article
individually:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NYT\_count }\OtherTok{\textless{}{-}}\NormalTok{ NYT }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{( word, lead\_paragraph ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{( stop\_words, }\AttributeTok{by=}\StringTok{"word"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{( ID, word, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

The next step is to convert the counts into a document term matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NYT\_dtm }\OtherTok{\textless{}{-}}\NormalTok{ NYT\_count }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{cast\_dtm}\NormalTok{( ID, word, n )}
\end{Highlighting}
\end{Shaded}

Finally, we are ready to run the LDA() function and we choose to set
\(K=2\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NYT\_LDA }\OtherTok{\textless{}{-}} \FunctionTok{LDA}\NormalTok{( NYT\_dtm, }\AttributeTok{k =} \DecValTok{2}\NormalTok{, }\AttributeTok{method=}\StringTok{"Gibbs"}\NormalTok{, }\AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{seed=}\DecValTok{2024}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\subsubsection*{Analysis of the results}\label{analysis-of-the-results}
\addcontentsline{toc}{subsubsection}{Analysis of the results}

Let's start by investigating the make-up of the articles. This requires
us to explore with which proportions the two topics feature in the
different articles. One option is to again create a box plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tidy}\NormalTok{( NYT\_LDA , }\AttributeTok{matrix =} \StringTok{"gamma"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( }\AttributeTok{y=}\NormalTok{gamma ) ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{( }\AttributeTok{y=}\StringTok{"Proportion"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.4\linewidth,height=\textheight,keepaspectratio]{03-Text-Data-Analysis_files/figure-pdf/unnamed-chunk-43-1.pdf}

}

\caption{Boxplot of the estimated proportions for the articles.}

\end{figure}%

The result seems a bit unsatisfying as the make-up only varies between a
65-35 and 35-65 split. Consequently, the estimates suggests that there
are very little differences in the language used within the topics.

To explore the topics in more detail, we plot the estimated term
frequencies for the two topics against each other:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tidy}\NormalTok{( NYT\_LDA , }\AttributeTok{matrix =} \StringTok{"beta"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{topic =} \FunctionTok{case\_when}\NormalTok{( topic}\SpecialCharTok{==}\DecValTok{1} \SpecialCharTok{\textasciitilde{}} \StringTok{"Topic1"}\NormalTok{, topic}\SpecialCharTok{==}\DecValTok{2} \SpecialCharTok{\textasciitilde{}} \StringTok{"Topic2"}\NormalTok{) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{( }\AttributeTok{names\_from =}\NormalTok{ topic, }\AttributeTok{values\_from =}\NormalTok{ beta, }\AttributeTok{values\_fill =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Topic1, }\AttributeTok{y=}\NormalTok{Topic2) ) }\SpecialCharTok{+}  \FunctionTok{coord\_trans}\NormalTok{( }\AttributeTok{x=}\StringTok{"sqrt"}\NormalTok{, }\AttributeTok{y=}\StringTok{"sqrt"}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{( }\FunctionTok{aes}\NormalTok{(}\AttributeTok{label=}\NormalTok{term), }\AttributeTok{check\_overlap =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{vjust=}\DecValTok{1}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Term Frequency in Topic 1"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Term Frequency in Topic 2"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{03-Text-Data-Analysis_files/figure-pdf/unnamed-chunk-44-1.pdf}

}

\caption{Comparison of term frequencies for the two topics.}

\end{figure}%

This plot provides some more insight. We see that Topic 1 seems to
feature more of the words related to the War in Ukraine, but it's not a
perfect plot - we note that for instance ``missile'' features more in
Topic 2 although this topic mainly occurred in the context of the war.

To summarize, we find that LDA produces two topics which seem quite
sensible, when considering the term frequencies of the words. However,
it's also important to point out that LDA does not really split the
documents into separate groups, due to each document only containing a
few words. This is quite a common feature, as the flexibility of the LDA
approach comes at the cost of it requiring a lot of data for model
fitting.

\section{Summary}\label{summary-2}

We considered several aspects related to the analysis of text data in R:

\begin{itemize}
\item
  The tidy text format is useful for the analysis of text data
\item
  Word frequency can be visualized using bar plots or word clouds
\item
  Sentiment analysis can be used to study the emotional intent. We
  considered one approach based on sentiment lexicons, but also
  highlighted that we have to make a strong assumption
\item
  Documents can be compared by plotting term frequencies against each
  other and analyzing the term frequency - inverse document frequency
\item
  Latent Dirichlet allocation is one method that may be used for topic
  modelling, but results need to be studied carefully
\end{itemize}

Some other aspects one should be aware of:

\begin{itemize}
\item
  We exclusively based our analyses on the individual words. However, in
  some applications it may be better to consider \textbf{n-grams}, which
  are compositions of \(n\) (usually consecutive) words. The
  unnest\_tokens() provides options to extract n-grams from a text.
\item
  The principles introduced in this chapter can be applied to any type
  of text data, such as social media messages or song lyrics, and are
  not limited to short texts and books.
\item
  You should be mindful of copyright when scraping text data from the
  internet. That's why we considered older books rather than recent
  bestsellers.
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Spatial Data Analysis}\label{spatial-data-analysis}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Attaching package: 'dplyr'
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:stats':

    filter, lag
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:base':

    intersect, setdiff, setequal, union
\end{verbatim}

In many applications we have information on the location where the data
were observed. Applications for which such spatial information is often
available include:

\begin{itemize}
\item
  \textbf{Disease modelling}: Knowledge of where infected patients are
  living allows us to identify the population at risk and to explore
  disease dynamics.
\item
  \textbf{Socioeconomic studies}: Data on income, access to schools and
  social activities are collected across administrative areas.
\item
  \textbf{Weather data}: Observations are collected at several weather
  stations and then used to predict the weather across the country.
\end{itemize}

In this chapter we explore how to analyze effectively data in such
applications. We start by introducing the three types of \textbf{spatial
data}:

\begin{description}
\item[\textbf{Point-referenced data:}]
Data is collected at a fixed set of spatial locations. Such data are,
for instance, analyzed in meteorology, hydrology, public health
(pollution levels), where the locations of the measurement stations are
given by their longitude and latitude coordinates (and possibly
altitude).
\item[\textbf{Point pattern data:}]
Differs from point-referenced data in that spatial locations themselves
are random. This type of data occurs, for instance, when monitoring
disease outbreaks and conducting ecological studies, where the locations
of the observations are unknown until we start collecting the data.
\item[\textbf{Lattice/Areal data:}]
Differs from point-referenced data in that observations represent
information about fixed spatial areas instead of spatial points. For
instance, when analyzing insurance claims, we may have access to the
number of claims for a postcode area, but no information on the
individual claims.
\end{description}

Sections \href{Visualizing\%20point-referenced\%20data}{4.1} to
\href{Principal\%20Component\%20Analysis}{4.4} focus on the analysis of
point-referenced data, and we will consider visualization of the other
types of spatial data in Sections
\href{Visualizing\%20point\%20pattern\%20data}{4.5} to
\href{Visualizing\%20lattice\%20data}{4.7}.

\section{Visualizing point-referenced
data}\label{visualizing-point-referenced-data}

\subsection{Motivation}\label{motivation}

The file ``Temperature Germany.csv'' contains the maximum temperature
recorded on 1 May 2020 for 75 weather stations across Germany. Let's
look at the provided variables

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Temperature }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/Temperature Germany.csv"}\NormalTok{ )}
\FunctionTok{names}\NormalTok{( Temperature )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "name"      "latitude"  "longitude" "altitude"  "max.temp" 
\end{verbatim}

Our variable of interest is \textbf{max.temp}, the recorded maximum
temperature, and \textbf{latitude}, \textbf{latitude} and
\textbf{altitude} specify the spatial locations of the 75 weather
stations.

We can already create a first plot of maximum daily temperature at
different locations using ggplot2:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{ggplot}\NormalTok{( Temperature, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{longitude, }\AttributeTok{y=}\NormalTok{latitude ) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{( }\FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\NormalTok{max.temp), }\AttributeTok{size=}\FloatTok{2.5}\NormalTok{ ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_distiller}\NormalTok{( }\AttributeTok{palette=}\StringTok{"Reds"}\NormalTok{, }\AttributeTok{trans=}\StringTok{"reverse"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Longitude"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Latitude"}\NormalTok{, }\AttributeTok{color=}\StringTok{"Temperature"}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{( }\AttributeTok{axis.title=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{15}\NormalTok{), }\AttributeTok{axis.text=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{15}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/Germany0-1.pdf}

}

\caption{Illustration of daily maximum temperature recorded at 75
weather stations across Germany. We use colour as a visual cue to
illustrate differences in maximum temperature.}

\end{figure}%

\textbf{Is this really the best way to plot the data?}

One may argue that the plot is not ideal since it does not provide any
spatial context, that is, we cannot relate the illustrated values and
locations to any known geographical features.

We now introduce two approaches for visualizing point-referenced data,
both of which aim to provide additional spatial context to aid
interpretation. The first approach uses \textbf{shapefiles} which allow
us to add the outline of Germany, that is its borders, to the plot. The
second approach will place the points upon a map.

\subsection{Shapefiles}\label{shapefiles}

Administrative boundaries can be described as a polygon with a
potentially very large number of edges. In some cases we may also deal
with several polygons. For instance, when considering the UK, we would
have one polygon per island. Such polygons are specified via a
\textbf{shapefile (.shp)} and can be used in R.

The administrative boundaries of countries are freely available from
www.gadm.org for educational and academic use. Let's download the data
for Germany and load the Level 1 data into R using the function
\textbf{read\_sf()} in the \textbf{sf} R package

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sf)}
\NormalTok{GER }\OtherTok{\textless{}{-}} \FunctionTok{read\_sf}\NormalTok{( }\StringTok{"Data/gadm41\_DEU\_shp/gadm41\_DEU\_1.shp"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\textbf{Important:} You need all the downloaded files, not just the .shp
file.

\textbf{Remark:} The Level 0 shapefile only contains the borders, while
higher levels (Level 1, etc.) also provide administrative boundaries
within the country - this applies to all countries listed on
www.gadm.org .

We plot the boundaries using \textbf{geom\_sf()} from the sf package in
combination with ggplot():

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( }\AttributeTok{data=}\NormalTok{GER ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_sf}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\caption{Administrative boundaries of the 16 German states.}

\end{figure}%

\textbf{IMPORTANT:} The shapefiles provided by GADM are very large, and
I noticed that some laptops may struggle to create the plot. Should you
have this issue, you may want to use the function
\textbf{st\_simplify()} to reduce the complexity of your polygon - the
function has a parameter \textbf{dTolerance} which specifies how much
the ``wiggles'' in the polygon will be straightened. Let's set
\textbf{dTolerance=2000} for the shapefile considered above

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GER\_simple }\OtherTok{\textless{}{-}} \FunctionTok{st\_simplify}\NormalTok{( GER, }\AttributeTok{dTolerance =} \DecValTok{2000}\NormalTok{, }\AttributeTok{preserveTopology=}\ConstantTok{TRUE}\NormalTok{ )}
\FunctionTok{ggplot}\NormalTok{( }\AttributeTok{data=}\NormalTok{GER\_simple ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_sf}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.65\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\caption{Approximation of the administrative boundaries of the 16 German
states with simplified borders.}

\end{figure}%

We see only very minor changes in the plot, and these are unlikely to
affect our analysis. If we set \textbf{dTolerance} too large, we may get
a shape that looks too different from the original file. As such, we
have to be a bit careful with the value we specify for
\textbf{dTolerance}.

Moving on with our analysis, the final step is to add the data points
displayed in Figure @ref(fig:Germany0) to the plot above. As such, our
data graphic is made up of two layers - one for the shapefile and one
for the data points. The order is important now because ggplot2 builds
the plot layer by layer:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( GER ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_sf}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{( }\AttributeTok{data=}\NormalTok{Temperature, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{longitude, }\AttributeTok{y=}\NormalTok{latitude, }\AttributeTok{color=}\NormalTok{max.temp), }\AttributeTok{size=}\DecValTok{3}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_distiller}\NormalTok{( }\AttributeTok{palette=}\StringTok{"Reds"}\NormalTok{, }\AttributeTok{trans=}\StringTok{"reverse"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Longitude"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Latitude"}\NormalTok{, }\AttributeTok{color=}\StringTok{"Temperature"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.77\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/unnamed-chunk-6-1.pdf}

}

\caption{Illustration of daily maximum temperature recorded at 75
weather stations across Germany. The visual cue colour is used to
represent maximum temperature.}

\end{figure}%

Note, we deviate from the format used in Chapter
\hyperref[data-visualization]{2}, because we are now working with two
data frames - one for the boundaries and another for the weather data.
Therefore, we have to specify the data for the points separately in
geom\_point().

\subsection{Projections}\label{projections}

So far we have used longitude and longitude within a Cartesian
coordinate system. However, the earth is a three-dimensional flattened
sphere and thus it is not flat. The process of converting locations in a
three-dimensional \textbf{geographic coordinate system} to a
two-dimensional representation is called \textbf{projection}.

There exists a wide range of projections because no projection can
preserve all properties at the same time, in particular
\textbf{shape/angle} and \textbf{area}.

We can change projections with the \textbf{coord\_sf()} function from
the sf package. Let's compare some possible projections applied to
Canada and Norway:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(patchwork)}
\NormalTok{CANADA   }\OtherTok{\textless{}{-}} \FunctionTok{read\_sf}\NormalTok{(}\StringTok{"Data/gadm41\_CAN\_shp/gadm41\_CAN\_1.shp"}\NormalTok{)}
\NormalTok{CANADA   }\OtherTok{\textless{}{-}} \FunctionTok{st\_simplify}\NormalTok{( CANADA, }\AttributeTok{dTolerance =} \DecValTok{5000}\NormalTok{ )}
\NormalTok{CANPlot1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{( }\AttributeTok{data=}\NormalTok{CANADA ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_sf}\NormalTok{()}
\NormalTok{CANPlot2 }\OtherTok{\textless{}{-}}\NormalTok{ CANPlot1 }\SpecialCharTok{+} \FunctionTok{coord\_sf}\NormalTok{( }\AttributeTok{crs=}\FunctionTok{st\_crs}\NormalTok{(}\DecValTok{3347}\NormalTok{) )}

\NormalTok{NORWAY   }\OtherTok{\textless{}{-}} \FunctionTok{read\_sf}\NormalTok{(}\StringTok{"Data/gadm41\_NOR\_shp/gadm41\_NOR\_1.shp"}\NormalTok{)}
\NormalTok{NORWAY   }\OtherTok{\textless{}{-}} \FunctionTok{st\_simplify}\NormalTok{( NORWAY, }\AttributeTok{dTolerance =} \DecValTok{2000}\NormalTok{ )}
\NormalTok{NORPlot1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{( }\AttributeTok{data=}\NormalTok{NORWAY ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_sf}\NormalTok{()}
\NormalTok{NORPlot2 }\OtherTok{\textless{}{-}}\NormalTok{ NORPlot1 }\SpecialCharTok{+} \FunctionTok{coord\_sf}\NormalTok{( }\AttributeTok{crs=}\FunctionTok{st\_crs}\NormalTok{(}\DecValTok{3346}\NormalTok{) )}

\NormalTok{( CANPlot1 }\SpecialCharTok{+}\NormalTok{ CANPlot2 ) }\SpecialCharTok{/}\NormalTok{ ( NORPlot1 }\SpecialCharTok{+}\NormalTok{ NORPlot2 )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/unnamed-chunk-7-1.pdf}

}

\caption{Administrative boundaries of Canada (top) and Norway (bottom).
The left plots preserve the angels, while the right plots do better at
highlighting that the countries are located on a sphere.}

\end{figure}%

In the left plots the meridians and parallels are orthogonal and, thus,
these projections are angle-preserving. This projection is also known as
the \textbf{WGS84 coordinate reference system}, which is the standard
for GPS systems and Google Earth. In the right plots, we do not preserve
the angels, but we better visualize that we are working with data on a
three-dimensional sphere. As data scientists, we should be aware that
our choice of projection may have a direct influence on how the plot is
perceived by others. While such aspects are negligible when analyzing
data across small spatial areas, we should keep this in mind when
considering spatial data collected across large spatial scales.

\textbf{Remark:} You have to experiment a bit with the value in
\textbf{st\_crs()} when creating plots. A value of 4326 corresponds to
the WGS84 system.

\subsection{Maps}\label{maps}

So far we added boundaries to a plot to aid the reader's orientation.
When we consider the German weather data, we find that the data set also
provides altitude. However, such information is difficult to incorporate
within a shape file. Therefore, it may be better to provide a map that
indicates larger water bodies and possibly elevation.

We use the \textbf{ggspatial} R package to import maps provided by
OpenStreetMap and then plot the points onto them. This requires us to
include the \textbf{annotation\_map\_tile()} function to our ggplot2
commands and replace geom\_point() by \textbf{geom\_spatial\_point()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggspatial)}
\FunctionTok{ggplot}\NormalTok{( Temperature, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{longitude, }\AttributeTok{y=}\NormalTok{latitude) ) }\SpecialCharTok{+} 
  \FunctionTok{annotation\_map\_tile}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_spatial\_point}\NormalTok{( }\FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\NormalTok{max.temp), }\AttributeTok{size=}\DecValTok{3}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_distiller}\NormalTok{( }\AttributeTok{palette=}\StringTok{"Reds"}\NormalTok{, }\AttributeTok{trans=}\StringTok{"reverse"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Longitude"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Latitude"}\NormalTok{, }\AttributeTok{color=}\StringTok{"Temperature"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/GERMap-1.pdf}

}

\caption{Temperature recorded at 75 weather stations across Germany,
with Open Street Map being used to illustrate geographical features.}

\end{figure}%

\textbf{Remark:} There are different types of maps provided by the
ggspatial package, but they are not relevant for the examples we
consider in this course.

While this type of map is not great at visualizing altitude, we can
observe that the temperature tends to be lower for higher altitudes and
for locations close to the sea (this is what we would expect).

\section{Inverse distance weighting}\label{inverse-distance-weighting}

After visualizing the data we now consider the task of making a
prediction at an unobserved location. Let \(x_1,\ldots,x_n\) and
\(\mathbf{s}_1,\ldots,\mathbf{s}_n\) denote the observed values and
locations respectively; \(\mathbf{s}_1,\ldots,\mathbf{s}_n\) are often
described by their longitude and latitude and are located within a
bounded area \(\mathcal{S}\subset\mathbb{R}^2\).

Consider the German temperature data in Figure @ref(fig:GERMap). How
would you use the data across the 75 weather stations if you were asked
to provide a prediction \(x^*\) at an unobserved location
\(\mathbf{s}^*\)?

Intuitively, the values at locations geographically close to
\(\mathbf{s}^*\) may provide the most useful information.
\textbf{Inverse distance weighting} puts this idea into a mathematical
framework, and it falls into the category of \textbf{spatial
interpolation} techniques. The approach postulates that the predicted
value \(x^*\) at \(\mathbf{s}^*\) is a weighted average of the observed
data points, with higher weighting given to locations that are
geographically close to \(\mathbf{s}^*\). We will formalize this
approach in the following and see how to perform it in R.

\subsection{Mathematical framework}\label{mathematical-framework}

Let \(d:\mathbb{R}^2 \times \mathbb{R}^2 \to \mathbb{R}_+\) be a metric
that defines a distance between any two points
\(\mathbf{s}_1=(s_{11},s_{12})\in\mathbb{R}^2\) and
\(\mathbf{s}_2=(s_{21}, s_{22})\in \mathbb{R}^2\). The most commonly
used distance metric is the L\(_2\) norm/Euclidian distance:

\begin{equation}
d(\mathbf{s}_1, \mathbf{s}_2) = ||\mathbf{s}_1-\mathbf{s}_2||_2= \sqrt{(s_{11}-s_{21})^2 + (s_{12}-s_{22})^2}.
(\#eq:Euclidian)
\end{equation}

Inverse distance weighting defines the predicted value \(x^*\) at the
unobserved location \(\mathbf{s}^*\) as \begin{equation}
x^* = 
\begin{cases}
\dfrac{\sum_{i=1}^n w(\mathbf{s}_i,\mathbf{s}^*) x_i}{\sum_{i=1}^n, w(\mathbf{s}_i,\mathbf{s}^*)} & \mbox{if}~d(\mathbf{s}^*, \mathbf{s}_i)>0~\mbox{for all } i=1,\ldots,n,\\
x_i & \mathrm{if}~d(\mathbf{s}^*, \mathbf{s}_i)=0,
\end{cases}
(\#eq:IDW)
\end{equation} where
\(w(\mathbf{s}_i,\mathbf{s}^*) = \left[d(\mathbf{s}_i,\mathbf{s}^*)\right]^{-p}\)
and \(p>0\) is called the \textbf{power parameter} and has to be
selected by us. One important rule to remember is that the weight given
to the observation closest to \(\mathbf{s}^*\) increases with \(p\); we
will see this when plotting the calculated predicted values later for
different values of \(p\).

\subsection{Performing inverse distance weighting in
R}\label{performing-inverse-distance-weighting-in-r}

There exist R functions to perform inverse distance weighting, but we
will implement our own function \textbf{IDW()} combines equations
@ref(eq:Euclidian) and @ref(eq:IDW). This will make it easier to plot
our predictions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{IDW }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( X, S, s\_star, p)\{}
\NormalTok{  d }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{( (S[,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{{-}}\NormalTok{s\_star[}\DecValTok{1}\NormalTok{])}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ (S[,}\DecValTok{2}\NormalTok{]}\SpecialCharTok{{-}}\NormalTok{s\_star[}\DecValTok{2}\NormalTok{])}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ )}
\NormalTok{  w }\OtherTok{\textless{}{-}}\NormalTok{ d}\SpecialCharTok{\^{}}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{p)}
  \ControlFlowTok{if}\NormalTok{( }\FunctionTok{min}\NormalTok{(d) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{ )}
    \FunctionTok{return}\NormalTok{( }\FunctionTok{sum}\NormalTok{( X }\SpecialCharTok{*}\NormalTok{ w ) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{( w ) )}
  \ControlFlowTok{else} 
    \FunctionTok{return}\NormalTok{( X[d}\SpecialCharTok{==}\DecValTok{0}\NormalTok{] )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Suppose we want to use the German weather data to predict the maximum
temperature on 1 May 2020 at an unobserved location with longitude 13.1
and latitude 51.0, \(\mathbf{s}^* = (13.1, 51.0)\). We consider the
settings \(p=0.5\) and \(p=2\) for the power parameter and obtain the
following values for \(x^*\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coord }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{( Temperature}\SpecialCharTok{$}\NormalTok{longitude, Temperature}\SpecialCharTok{$}\NormalTok{latitude )}
\NormalTok{s\_star }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{( }\FloatTok{13.1}\NormalTok{, }\FloatTok{51.0}\NormalTok{ )}
\FunctionTok{IDW}\NormalTok{( }\AttributeTok{X=}\NormalTok{Temperature}\SpecialCharTok{$}\NormalTok{max.temp, }\AttributeTok{S=}\NormalTok{coord, s\_star, }\AttributeTok{p=}\FloatTok{0.5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 23.4665
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{IDW}\NormalTok{( }\AttributeTok{X=}\NormalTok{Temperature}\SpecialCharTok{$}\NormalTok{max.temp, }\AttributeTok{S=}\NormalTok{coord, s\_star, }\AttributeTok{p=}\FloatTok{2.0}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 22.67209
\end{verbatim}

We see that the two settings for \(p\) give predictions that differ by
almost 1 degree Celsius. Consequently, the selection of \(p\) is indeed
crucial and we will consider its selection in more detail in the next
part.

\subsection{Creating spatial plots with inverse distance
weighting}\label{creating-spatial-plots-with-inverse-distance-weighting}

We want to make predictions across the whole study space, instead of a
single location \(\mathbf{s}^*\). Here we visualize these predictions
across a map, which is also useful to identify suitable values for
\(p\).

The first step is to define a grid that covers the study space
\(\mathcal{S}\). Here, we will simply define \(\mathcal{S}\) as a
rectangle (based on longitude and latitude coordinates):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{points\_lat }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{( }\FloatTok{47.1}\NormalTok{, }\FloatTok{55.1}\NormalTok{, }\AttributeTok{by=}\FloatTok{0.05}\NormalTok{ )}
\NormalTok{points\_lon }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{( }\FloatTok{5.8}\NormalTok{, }\FloatTok{15.6}\NormalTok{, }\AttributeTok{by=}\FloatTok{0.05}\NormalTok{ )}
\NormalTok{pixels }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{( }\FunctionTok{expand.grid}\NormalTok{( points\_lon, points\_lat ) )}
\end{Highlighting}
\end{Shaded}

We then apply our implemented \textbf{IDW()} function to the grid points
with the power parameter being set to \(p=0.5\), \(p=2.0\) and \(p=20\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p05 }\OtherTok{\textless{}{-}}\NormalTok{ p2 }\OtherTok{\textless{}{-}}\NormalTok{ p20 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{( j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(pixels[,}\DecValTok{1}\NormalTok{]) )\{}
\NormalTok{  p05[j] }\OtherTok{\textless{}{-}} \FunctionTok{IDW}\NormalTok{( }\AttributeTok{X=}\NormalTok{Temperature}\SpecialCharTok{$}\NormalTok{max.temp, }\AttributeTok{S=}\NormalTok{coord, }\AttributeTok{s\_star=}\NormalTok{pixels[j,], }\AttributeTok{p=}\FloatTok{0.5}\NormalTok{ )}
\NormalTok{  p2[j]  }\OtherTok{\textless{}{-}} \FunctionTok{IDW}\NormalTok{( }\AttributeTok{X=}\NormalTok{Temperature}\SpecialCharTok{$}\NormalTok{max.temp, }\AttributeTok{S=}\NormalTok{coord, }\AttributeTok{s\_star=}\NormalTok{pixels[j,], }\AttributeTok{p=}\FloatTok{2.0}\NormalTok{ )}
\NormalTok{  p20[j] }\OtherTok{\textless{}{-}} \FunctionTok{IDW}\NormalTok{( }\AttributeTok{X=}\NormalTok{Temperature}\SpecialCharTok{$}\NormalTok{max.temp, }\AttributeTok{S=}\NormalTok{coord, }\AttributeTok{s\_star=}\NormalTok{pixels[j,], }\AttributeTok{p=}\DecValTok{20}\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The next step is to collate the estimates into a data frame so that we
can plot the predictions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyr)}
\NormalTok{Predict }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{( }\StringTok{"Lon"}\OtherTok{=}\NormalTok{pixels[,}\DecValTok{1}\NormalTok{], }\StringTok{"Lat"}\OtherTok{=}\NormalTok{pixels[,}\DecValTok{2}\NormalTok{], }
                       \StringTok{"p0.5"}\OtherTok{=}\NormalTok{p05, }\StringTok{"p2"}\OtherTok{=}\NormalTok{p2, }\StringTok{"p20"}\OtherTok{=}\NormalTok{p20 )}
\NormalTok{Predict }\OtherTok{\textless{}{-}}\NormalTok{ Predict }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pivot\_longer}\NormalTok{( }\AttributeTok{cols=}\NormalTok{p0}\FloatTok{.5}\SpecialCharTok{:}\NormalTok{p20, }\AttributeTok{names\_to =} \StringTok{"Power"}\NormalTok{ )}
\NormalTok{Predict }\OtherTok{\textless{}{-}}\NormalTok{ Predict }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{Power =} \FunctionTok{case\_when}\NormalTok{( Power }\SpecialCharTok{==} \StringTok{"p0.5"} \SpecialCharTok{\textasciitilde{}} \StringTok{"p=0.5"}\NormalTok{, Power }\SpecialCharTok{==} \StringTok{"p2"} \SpecialCharTok{\textasciitilde{}} \StringTok{"p=2.0"}\NormalTok{,}
                             \AttributeTok{.default =} \StringTok{"p=20"}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

Finally, we plot the grid and the calculated predictions. We add the
boundaries of Germany and the locations of the weather stations to the
plot to provide some spatial context:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( }\AttributeTok{data=}\NormalTok{Predict ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_raster}\NormalTok{( }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{Lon, }\AttributeTok{y=}\NormalTok{Lat, }\AttributeTok{fill=}\NormalTok{value ) ) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{Power ) }\SpecialCharTok{+} 
  \FunctionTok{scale\_fill\_distiller}\NormalTok{( }\AttributeTok{palette=}\StringTok{"Reds"}\NormalTok{, }\AttributeTok{trans=}\StringTok{"reverse"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{( }\AttributeTok{data=}\NormalTok{GER, }\AttributeTok{alpha=}\FloatTok{0.0}\NormalTok{, }\AttributeTok{color=}\StringTok{"white"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{( }\AttributeTok{data=}\NormalTok{Temperature, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{longitude, }\AttributeTok{y=}\NormalTok{latitude) ) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{fill=}\StringTok{"C"}\NormalTok{, }\AttributeTok{x=}\StringTok{"Longitude"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Latitude"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/unnamed-chunk-13-1.pdf}

}

\caption{Predicted values of maximum daily temperature on May 1, 2020
for Germany and adjacent regions using inverse distance weighting with
three values for the power paramter.}

\end{figure}%

\textbf{What do we notice?}

When considering the range of the colour bars, we notice that \(p=0.5\)
leads to the estimated temperatures being much less varied than for
\(p=2\). Further, \(p=20\) produces unrealistic predictions and should
not be selected, since we predict sudden changes in temperature across
space, with the predicted value being almost only determined by the
value observed at the location closest to \(\mathbf{s}^*\); the latter
is also referred to as a tessellation of the region.

The difference in the range of predicted values for the different values
of \(p\) can be explained by that the weight given to the observation
closest to \(\mathbf{s}^*\) increases with \(p\) - we highlighted this
aspect earlier. These findings give us some guidance on the selection of
\(p\):

\begin{itemize}
\item
  \(p\) should be small enough to avoid tessellation, unless we believe
  such a feature is realistic;
\item
  \(p\) should be large enough to give predictions that are similar in
  range to the observed values.
\end{itemize}

The range of predicted values for \(p=0.5\) is in fact substantially
smaller than the range of observed values in Figure @ref(fig:GERMap) -
we predict temperatures above 20 degrees Celsius even for the highest
mountains, although the true value is probably much lower. This suggests
that \(p=0.5\) is too small and that \(p=2\) performs best amongst the
considered values, because we have both a range of predictions similar
to the observed data and the produced prediction map still appears
relatively smooth.

\textbf{Remark 1:} We ignored that temperature depends on altitude in
our analysis. If we had access to altitude for each point on the grid,
we could extend the distance measure in Equation @ref(eq:Euclidian) to
incorporate latitude, longitude and altitude in the inverse distance
weighting estimates.

\textbf{Remark 2:} We selected \(p\) ``by eye''. We may use
cross-validation (which some of you may have seen in MA22018) to find
the value \(p\) that performs best in terms of prediction.

\textbf{Remark 3:} Care should be taken when using inverse distance
weighting to make predictions for a location \(\mathbf{s}^*\) that is
not close to \(\mathbf{s}_1,\ldots,\mathbf{s}_n\). For instance, we
should not use the temperature data from Germany to predict the
temperature for Bath.

\section{Variogram}\label{variogram}

\subsection{Motivation}\label{motivation-1}

We now move on to exploring and quantifying spatial dependence, that is,
how informative is the data at one location to say something about
near-by locations. This is a key aspect underlying the majority of
spatial data analysis methods. For instance, inverse distance weighting
makes the assumption that data collected at the locations
\(\mathbf{s}_1,\ldots,\mathbf{s}_n\) is useful to make predictions at
unobserved locations, and that locations closest to the unobserved
location provide the most information.

In the examples considered so far, such as the German weather data in
Figure @ref(fig:GERMap), we found that spatially close sites tend to
have similar values.

As a second motivating data example, let's consider the recorded sea
surface temperature anomalies for the North Sea for 1 November 2024
(provided in the file ``SeaSurface.csv'') as recorded by NOAA:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SSTA }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"Data/Sea Surface Temperature Anomalies.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Observations are on a 5km resolution. To create a plot of the data, we
first have to convert the data frame providing longitude, latitude and
sea surface temperature into a spatial data object using
\textbf{st\_as\_sf()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SSTA\_sf }\OtherTok{\textless{}{-}} \FunctionTok{st\_as\_sf}\NormalTok{( SSTA, }\AttributeTok{coords=}\FunctionTok{c}\NormalTok{(}\StringTok{"lon"}\NormalTok{, }\StringTok{"lat"}\NormalTok{) ) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{st\_set\_crs}\NormalTok{( }\DecValTok{4326}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

We can now create the plot with geom\_sf():

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( }\AttributeTok{data=}\NormalTok{SSTA\_sf ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_sf}\NormalTok{( }\FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\NormalTok{Anomaly) ) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Longitude"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Latitude"}\NormalTok{, }\AttributeTok{color=}\StringTok{"SST Anomaly"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/SST-1.pdf}

}

\caption{Sea surface temperature anomalies on 1 November 2024 for the
North Sea as recorded by NOAA.}

\end{figure}%

The plot shows that locations which are spatially close observe a
similar value. We now want to quantify this similarity of values of
spatially close sites using the \textbf{variogram} which is based on
concepts we are familiar with from Year 1 Probability and Statistics.

\subsection{Mathematical framework}\label{mathematical-framework-1}

We already came across dependence of two random variables in Year 1. To
explore dependence between two random variables \(X\) and \(Y\), we
considered the covariance {[} \mathrm{Cov}(X,Y) =
\mathbb{E}\left[(X-\mathbb{E}X)(Y-\mathbb{E}Y)\right] = \mathbb{E}(XY) -
\mathbb{E}X \mathbb{E}Y. {]} In a spatial context, we now consider the
observation at a spatial site as the realization of a random variable,
with the random variables at any two sites being potentially dependent.
This leads to the concept of a \textbf{random field}, which is also
referred to as a \textbf{spatial random process}.

\textbf{Definition:} A random field
\(\{X(\mathbf{s}):\mathbf{s}\in\mathcal{S}\}\) is a family of random
variables that are defined on the same probability space and indexed by
the spatial location \(\mathbf{s}\in\mathcal{S}\).

For the sea surface data, \(X(\mathbf{s})\) describes the distribution
of the sea surface temperature anomaly at location
\(\mathbf{s}\in\mathcal{S}\). Further, Figure @ref(fig:SST) corresponds
to a single sample from the random field.

Consider an arbitrary pair \((\mathbf{s},\tilde{\mathbf{s}})\) of sites
across the region \(\mathcal{S}\). While dependence of \(X(\mathbf{s})\)
and \(X(\tilde{\mathbf{s}})\) may be quantified via their covariance, we
usually use the \textbf{semi-variogram} in spatial data analysis:

\textbf{Definition:} The semi-variogram for the locations \(\mathbf{s}\)
and \(\tilde{\mathbf{s}}\) is defined as {[}
\gamma(\mathbf{s},\tilde{\mathbf{s}}) =
\frac{1}{2}\mathbb{E}\left[\left(\{X(\mathbf{s})-\mathbb{E} X(\mathbf{s})\}-\{X(\tilde{\mathbf{s}})-\mathbb{E}X(\tilde{\mathbf{s}})\}\right)^2\right].
{]}

From now on we make the assumption that
\(\mathbb{E}\left[X(\mathbf{s})\right]\) is the same for all
\(\mathbf{s}\in\mathcal{S}\). Consequently, the expression for the
semi-variogram simplifies to half the average squared difference between
the values at these locations,

\begin{equation}
\gamma(\mathbf{s},\tilde{\mathbf{s}}) = \frac{1}{2} \mathbb{E}\left[\{X(\mathbf{s})-X(\tilde{\mathbf{s}})\}^2\right] = \frac{1}{2} \mathrm{Var}\left[X(\mathbf{s})-X(\tilde{\mathbf{s}})\right].
(\#eq:Semivariogram)
\end{equation}

The semi-variogram has several important properties:

\begin{itemize}
\item
  \(\gamma(\mathbf{s},\tilde{\mathbf{s}})\geq 0\) since it is the
  expectation of a square, with smaller values corresponding to stronger
  dependence.
\item
  If \(\mathbf{s}=\tilde{\mathbf{s}}\), we have
  \(\gamma(\mathbf{s},\tilde{\mathbf{s}})=0\).
\item
  If \(X(\mathbf{s})\) and \(X(\tilde{\mathbf{s}})\) are independent and
  identically distributed, {[}
  \gamma(\mathbf{s},\tilde{\mathbf{s}})=\mathrm{Var}{[}X(\mathbf{s}){]}=\mathrm{Var}{[}X(\tilde{\mathbf{s}}){]}.
  {]}
\end{itemize}

\subsection{Estimation of the
semi-variogram}\label{estimation-of-the-semi-variogram}

We want to estimate \(\gamma(\mathbf{s},\tilde{\mathbf{s}})\) in
expression @ref(eq:Semivariogram) using the observations
\(x_1,\ldots,x_n\) and locations \(\mathbf{s}_1,\ldots,\mathbf{s}_n\).

The main challenge is that we only have a single observation at each of
the \(n\) locations. For instance, if we were to derive an estimate for
\(\gamma(\mathbf{s}_1,\mathbf{s}_2)\) using \(x_1\) and \(x_2\) only, we
would have \(\gamma(\mathbf{s}_1,\mathbf{s}_2)=\frac{1}{2}(x_1-x_2)^2\).
However, such an estimate is likely to be poor. This is a common problem
in spatial data analysis and applies to all the examples considered so
far.

To make use of all the data available, we make the assumptions that the
random field is \textbf{stationary} and \textbf{isotropic}. Simply
speaking, these assumptions imply that for any pair
(\(\mathbf{s},\tilde{\mathbf{s}}\)) of locations
\(\gamma(\mathbf{s},\tilde{\mathbf{s}})\) is fully specified by their
spatial distance. As such, there exists a function
\(\tilde{\gamma}:\mathbb{R}\to\mathbb{R}_+\) such that for all
\(\mathbf{s},\tilde{\mathbf{s}}\in\mathcal{S}\) {[}
\gamma(\mathbf{s},\tilde{\mathbf{s}}) = \tilde{\gamma}(
\textbar\textbar{}\mathbf{s} - \tilde{\mathbf{s}}\textbar\textbar{} ).
{]}

\textbf{Important:} You have to carefully consider whether the
assumption holds that dependence is fully described by spatial distance.
If this is not the case, you should not apply the following estimation
procedure.

\textbf{Remark:} The concepts of stationarity and isotropy will be
introduced more formally in MA32024 Statistical Modelling and Data
Analytics 3B. In this course it's only important to discuss the general
assumptions we make when estimating the semi-variogram.

For a stationary and isotropic random process, we can estimate
\(\tilde{\gamma}(h)\) at distance \(h>0\) as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Find all pairs of sites with a distance similar to \(h\). This gives
  the set
  \(\mathcal{N}_h =\{(i,j):||\mathbf{s}_i - \mathbf{s}_j||\approx h\}\).
\item
  Calculate the estimate for \(\tilde\gamma(h)\) as {[} \hat\gamma(h) =
  \frac{1}{2|\mathcal{N}_h|} \sum\_\{(i,j) \in\mathcal{N}\_h\}
  (x\_i-x\_j)\^{}2. {]}
\end{enumerate}

The R package \textbf{gstat} provides the function \textbf{variogram()}
to perform the estimation. We will see, however, in the following
examples that some data wrangling is required.

\subsection{Analysis of sea surface temperature
anomalies}\label{analysis-of-sea-surface-temperature-anomalies}

The first step is to remove any locations with missing data (represented
by \textbf{NA}) from the data frame - we have no data for land areas and
the variogram() function cannot handle such data sets:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SSTA\_gamma }\OtherTok{\textless{}{-}} \FunctionTok{drop\_na}\NormalTok{( SSTA, Anomaly )}
\end{Highlighting}
\end{Shaded}

Before we can use the variogram() function, we have to carefully
consider whether all our key assumptions are reasonable:

\begin{description}
\item[\textbf{\(\mathbb{E}\left[X(\mathbf{s})\right]\) is constant
across \(\mathcal{S}\):}]
Since we consider anomalies, i.e., deviations from the mean, we may
assume that the mean anomaly is 0 (or at least very close to it) for all
locations in the North Sea.
\item[\textbf{Dependence only depends on spatial distance:}]
Figure @ref(fig:SST) shows a similar level of variability in the values
of close-by sites for all areas of the North Sea. While this is not a
sufficient condition, there is no clear reason for us to reject the
assumption.
\end{description}

Now that we verified that our assumptions are not unreasonable, we are
ready to estimate the semi-variogram for the random field
\(\{X(\mathbf{s}):\mathbf{s}\in\mathcal{S}\}\). The first step is to
convert the data frame \textbf{SST\_gamma} to make it clear that the
spatial locations are specified by their latitude and longitude
coordinates:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sp)}
\FunctionTok{coordinates}\NormalTok{( SSTA\_gamma ) }\OtherTok{\textless{}{-}} \ErrorTok{\textasciitilde{}}\NormalTok{lon}\SpecialCharTok{+}\NormalTok{lat}
\end{Highlighting}
\end{Shaded}

We can now use the variogram() function to calculate and plot the
estimated function \(\hat\gamma(h)\):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gstat)}
\NormalTok{gamma\_hat }\OtherTok{\textless{}{-}} \FunctionTok{variogram}\NormalTok{( Anomaly}\SpecialCharTok{\textasciitilde{}}\DecValTok{1}\NormalTok{, SSTA\_gamma, }\AttributeTok{width=}\FloatTok{0.1}\NormalTok{, }\AttributeTok{cutoff=}\DecValTok{2}\NormalTok{  )}
\FunctionTok{ggplot}\NormalTok{( gamma\_hat, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{dist, }\AttributeTok{y=}\NormalTok{gamma}\SpecialCharTok{/}\DecValTok{2}\NormalTok{ ) ) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{( }\AttributeTok{size=}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Distance"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Semi{-}variogram"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/unnamed-chunk-18-1.pdf}

}

\caption{Estimated semi-variogram for sea surface temperature anomalies
in the North Sea based on the data for 1 November 2024.}

\end{figure}%

From the plotted semi-variogram we conclude that the dependence between
pairs of sites decreased with increasing spatial distance. It's also
important to consider whether the function levels off at some distance,
which would indicate independence at large spatial distances. This seems
not to be the case here, and thus observations are dependent even at a
distance of \(h=2\).

\textbf{Remark:} The options \texttt{width} and \texttt{cutoff} in the
variogram function() specify the size of \(\mathcal{N}_h\) and the
maximum distance \(h\) to be considered in the estimation.

\section{Principal Component
Analysis}\label{principal-component-analysis}

In many applications, interest lies in identifying the key data
structures and to perform dimension reduction. There exists a range of
techniques, such as cluster analysis, in data science and machine
learning to achieve this. To conclude our trip through the world of
point-referenced data, we introduce \textbf{principal component
analysis} (PCA) and illustrate how it may be used to explore the spatial
structure in the data. You may learn more about this technique in the
Year 3 unit Applied Data Science.

\subsection{Motivation}\label{motivation-2}

Let's consider a toy example with two sites and \(T\) data points for
each site. We use \(x_{i,t}\) to denote the observation for site
\(i\in\{1,2\}\) on day \(t\in\{1,\ldots,T\}\). For illustration, we
simulate some data with \(T=200\) and visualize it using a scatter plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2025}\NormalTok{)}
\NormalTok{x1 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{( }\DecValTok{200}\NormalTok{, }\AttributeTok{mean=}\DecValTok{2}\NormalTok{, }\AttributeTok{sd=}\DecValTok{4}\NormalTok{ )}
\NormalTok{x2 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{( }\DecValTok{200}\NormalTok{, }\AttributeTok{mean=}\NormalTok{x1, }\AttributeTok{sd=}\DecValTok{1}\NormalTok{ )}
\NormalTok{X  }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{( }\StringTok{"Site1"}\OtherTok{=}\NormalTok{x1, }\StringTok{"Site2"}\OtherTok{=}\NormalTok{x2 )}
\FunctionTok{ggplot}\NormalTok{( X, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Site1, }\AttributeTok{y=}\NormalTok{Site2) ) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Data for Site 1"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Data for Site 2"}\NormalTok{ ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.38\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/ToyExample-1.pdf}

}

\caption{Toy data set with two spatial sites and 200 observations per
site.}

\end{figure}%

We can see that the simulated data is positively correlated, with most
of the points lying close to the line \(y=x\). Principal component
analysis is concerned with finding these important directions in cases
where we have many variables / a large number of spatial sites.

In this course, we focus on deriving these directions as they give
insight into the spatial structure of the data. Another important aspect
of PCA, which we will not cover, is that of dimension reduction, which
is particularly useful for modelling. For the data above, we may use PCA
to project the two-dimensional data onto a one-dimensional subspace,
which is illustrated below.

\begin{figure}[H]

{\centering \includegraphics[width=0.38\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/unnamed-chunk-19-1.pdf}

}

\caption{Original data (black) and the data points projected onto a
one-dimensional linear subspace using PCA (red).}

\end{figure}%

We won't cover the mathematical background on PCA, and more details will
be covered in the Year 3 Applied Data Science unit.

\subsection{Analyzing spatial
structure}\label{analyzing-spatial-structure}

Suppose we have \(T\) observations \(x_{i,1},\ldots,x_{i,T}\) for the
\(i\)-th site with location \(\mathbf{s}_i\) (\(i=1,\ldots,n\)), and
that \(x_{1,t},\ldots,x_{n,t}\) (\(t=1,\ldots,T\)) are recorded at the
same time. We have to perform the following steps which are done in one
go by the R function \textbf{prcomp()}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Calculate \(\tilde{x}_{i,t} = (x_{i,t} - \bar{x}_i)/\hat\sigma_i\),
  where {[} \bar\{x\}\_i =
  \frac{x_{i,1} + x_{i,2} + \cdots + x_{i,T}}{T} {]} denotes the average
  value for the observations at the location \(\mathbf{s}_i\), and
  \(\hat\sigma_i\) is the sample standard deviation.
\item
  Given the vectors
  \(\left\{\tilde{\mathbf{x}}_t =(\tilde{x}_{1,t},\ldots,\tilde{x}_{n,t}):t=1,\ldots,T \right\}\),
  we derive the matrix {[} \Sigma = \frac{1}{T-1}\sum\_\{t=1\}\^{}T
  \tilde{\mathbf{x}}\_t \tilde{\mathbf{x}}\_t\^{}\{\mathrm{T}\}. {]}
  This matrix is also known as the \textbf{empirical covariance matrix},
  and it is a multivariate extension of the covariance covered in Year 1
  Probability and Statistics.
\item
  Derive the eigenvalues and eigenvectors of \(\Sigma\), i.e., we are
  seeking the matrix \(\mathbf{U}\) of eigenvectors and the diagonal
  matrix \(\mathbf{D}\) of eigenvalues \(\lambda_1,\ldots,\lambda_n\)
  such that \(\Sigma = \mathbf{UDU}^{\mathrm{T}}\).
\end{enumerate}

\textbf{How do we use the eigenvectors and eigenvalues to analyze the
structure in the spatial data?}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We visualize the eigenvectors using the techniques covered so far.
  Each entry of an eigenvector corresponds to one of the sites and we
  study their spatial structure, starting from the eigenvector with the
  largest eigenvalue.
\item
  We use the eigenvalues \(\lambda_1,\ldots,\lambda_n\) to determine how
  many eigenvectors we need to explore. The eigenvalues are ordered,
  i.e., \(\lambda_1 > \lambda_2 > \cdots > \lambda_n\). For each
  \(m=1,\ldots,n\), we compute the ratio \begin{equation}
  \frac{\sum_{j=1}^m \lambda_j}{\sum_{j=1}^n \lambda_j}.
   (\#eq:Ratio)
  \end{equation} We then look for the first value of \(m\) for which the
  ratio exceeds 0.9. This corresponds to these \(m\) components
  explaining at least 90\% of the variation in the data. This is a
  widely used rule-of-thumb in PCA.
\end{enumerate}

\textbf{Important:} For the eigenvalues and eigenvectors to accurately
reflect the spatial structure, we have to make the assumption of
\textbf{linearity}. This means that for any \(i=1,\ldots,n\), we can
approximate \(X_{i,t}\) by a linear combination of the (\(n-1\))
remaining variables
\(X_{1,t},\ldots,X_{i-1,t},X_{i+1,t},\ldots,X_{n,t}\).

PCA may seem very abstract at this point, but the following example with
illustrate how to use it in practice.

\subsection{Example: Precipitation across Colorado, United
States}\label{example-precipitation-across-colorado-united-states}

The file ``PrecipitationColorado.csv'' contains the amount of
precipitation per month for 30 cities in the state and the period
2010-2023. So we have \(n=30\) sites and \(T=168\) observations per
site.

Let's import the precipitation data and the information on the spatial
sites.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ColoradoPrecip }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"Data/PrecipitationColorado.csv"}\NormalTok{)}
\NormalTok{ColoradoCities }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"Data/Cities Colorado.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Before we perform PCA, we have to consider the assumption of linearity,
which can be difficult to verify in practice. In this example it is
sufficient to just plot the data for pairs of sites against each other
and to see whether there is a linear pattern

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( ColoradoPrecip, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{VALDEZ, }\AttributeTok{y=}\NormalTok{WETMORE) ) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} 
\FunctionTok{ggplot}\NormalTok{( ColoradoPrecip, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{TOWNER, }\AttributeTok{y=}\NormalTok{STONINGTON) ) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} 
\FunctionTok{ggplot}\NormalTok{( ColoradoPrecip, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{CARDIFF, }\AttributeTok{y=}\NormalTok{NORTHGLENN) ) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.95\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/unnamed-chunk-21-1.pdf}

}

\caption{Scatter plots of the observed amounts of precipitation for
three pairs of sites}

\end{figure}%

There is clearly some positive correlation between all pairs of sites
and so we can conclude that linearity is a reasonable assumption. So we
can now proceed with the analysis and use the R function prcomp() to
perform the computations in
Section\href{Analyzing\%20spatial\%20structure}{4.4.2}. This function
requires the data to be in a \(T\times n\) matrix and thus we first have
to remove the column \textbf{date} before calling prcomp():

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Precip }\OtherTok{\textless{}{-}}\NormalTok{ ColoradoPrecip }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{( }\SpecialCharTok{{-}}\NormalTok{date )}
\NormalTok{PCA }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{( Precip, }\AttributeTok{scale. =} \ConstantTok{TRUE}\NormalTok{ ) }\DocumentationTok{\#\# Combines all 3 steps}
\end{Highlighting}
\end{Shaded}

The object \textbf{PCA} contains the computed eigenvalues and
eigenvectors:

\begin{itemize}
\item
  \textbf{sdev} - square roots of the eigenvalues.
\item
  \textbf{rotation} - eigenvectors
\end{itemize}

We now have to decide which eigenvectors should be visualized. The
eigenvalues in \textbf{sdev} are ordered in terms of magnitude, i.e.,
the first eigenvectors are more important than the latter. So we are
computing and visualizing the ratio @ref(eq:Ratio) for \(m=1,\ldots,30\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ColoradoEV }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{( }\AttributeTok{m=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{30}\NormalTok{, }\AttributeTok{Ratio =} \FunctionTok{cumsum}\NormalTok{(PCA}\SpecialCharTok{$}\NormalTok{sdev}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(PCA}\SpecialCharTok{$}\NormalTok{sdev}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) )}
\FunctionTok{ggplot}\NormalTok{( ColoradoEV, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{m, }\AttributeTok{y=}\NormalTok{Ratio) ) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_abline}\NormalTok{( }\AttributeTok{intercept =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{slope =} \DecValTok{0}\NormalTok{, }\AttributeTok{linewidth=}\FloatTok{1.5}\NormalTok{ ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.45\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/unnamed-chunk-23-1.pdf}

}

\caption{Proportion of variance explained by the first m eigenvectors}

\end{figure}%

The plot shows that the first three eigenvectors explain more than 90\%
of the variance in the data. Therefore, it is sufficient to visualize
the first three eigenvectors, which we achieve by combining the values
in the eigenvectors with the spatial sites:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ColoradoCities }\OtherTok{\textless{}{-}}\NormalTok{ ColoradoCities }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{PC1 =}\NormalTok{ PCA}\SpecialCharTok{$}\NormalTok{rotation[,}\DecValTok{1}\NormalTok{], }\AttributeTok{PC2 =}\NormalTok{ PCA}\SpecialCharTok{$}\NormalTok{rotation[,}\DecValTok{2}\NormalTok{], }\AttributeTok{PC3 =}\NormalTok{ PCA}\SpecialCharTok{$}\NormalTok{rotation[,}\DecValTok{3}\NormalTok{] ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{( }\AttributeTok{cols=}\NormalTok{PC1}\SpecialCharTok{:}\NormalTok{PC3, }\AttributeTok{names\_to=}\StringTok{"PC"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

Finally, we can plot the values of the first three eigenvectors using
the visualizaion techniques we have seen before:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( ColoradoCities, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{LONG, }\AttributeTok{y=}\NormalTok{LAT) ) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{PC ) }\SpecialCharTok{+}
  \FunctionTok{annotation\_map\_tile}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_spatial\_point}\NormalTok{( }\FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\NormalTok{value), }\AttributeTok{size=}\DecValTok{5}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_distiller}\NormalTok{( }\AttributeTok{palette=}\StringTok{"RdYlBu"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Longitude"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Latitude"}\NormalTok{, }\AttributeTok{color=}\StringTok{"value"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/Colorado-1.pdf}

}

\caption{Illustration of the first three eigenvectors for the
precipitation data from Colorado.}

\end{figure}%

Now we have to interpret the plots:

\begin{itemize}
\item
  All entries in the first eigenvector have the same sign. This suggests
  that given monthly precipitation is high at one location, it also
  tends to be high at the other locations. We further observe a
  west-east trend in the values. Looking at the geography of Colorado,
  this direction likely represents the effect of the Rocky Mountains,
  which run through the western part of the state and stretch from north
  to south. As such, spatial variations in the amount of precipitation
  are driven by the different climates of Western and Eastern Colorado.
\item
  The second eigenvector shows a west-east trend. This may reflect again
  the effect of the Rocky Mountains on the spatial variation in the
  amount of precipitation.
\item
  The third eigenvector indicates a south-east to north-west trend
  across Colorado. Due to its size, there is quite a difference in
  climate between Northern and Southern Colorado. Northern Colorado
  tends to observe more snowfall in winter while Southern Colorado
  experiences more thunderstorms in the summer. So we may conclude that
  the third eigenvector captures this difference in climate
\end{itemize}

\section{Visualizing point pattern
data}\label{visualizing-point-pattern-data}

Recall that the difference between point-referenced and point pattern
data is that for the latter the points are randomly distributed across
space, while they are fixed when analyzing point-referenced data. When
visualizing point pattern data, there is no large difference to
point-referenced data. This is due to both data types requiring us to
add points (often provided by their latitude and longitude) to a
shapefile or map.

We illustrate this aspect using the data in ``Wildfires.csv''. The data
file provides the locations of wildfires recorded for California between
2013 and 2020:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{WildFires }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/Wildfires.csv"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

Let's add the points in the Californian wildfire data set to a shapefile
and a map, and also highlight the size of the burned area:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{USA }\OtherTok{\textless{}{-}} \FunctionTok{read\_sf}\NormalTok{( }\StringTok{"Data/Shapefile USA/gadm41\_USA\_1.shp"}\NormalTok{ )}
\NormalTok{California }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{( USA, NAME\_1}\SpecialCharTok{==}\StringTok{"California"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{st\_simplify}\NormalTok{(}\AttributeTok{dTolerance =} \DecValTok{2000}\NormalTok{)}
\NormalTok{shape }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{( }\AttributeTok{data=}\NormalTok{California ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_sf}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{( }\AttributeTok{data=}\NormalTok{WildFires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Longitude,}\AttributeTok{y=}\NormalTok{Latitude, }\AttributeTok{color=}\FunctionTok{log}\NormalTok{(AcresBurned) ) ) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_distiller}\NormalTok{( }\AttributeTok{palette=}\StringTok{"Reds"}\NormalTok{, }\AttributeTok{trans=}\StringTok{"reverse"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{( }\AttributeTok{legend.position=}\StringTok{"none"}\NormalTok{ )}
\NormalTok{map }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{( WildFires, }\FunctionTok{aes}\NormalTok{( Longitude, }\AttributeTok{y=}\NormalTok{Latitude ) ) }\SpecialCharTok{+} 
  \FunctionTok{annotation\_map\_tile}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_spatial\_point}\NormalTok{( }\FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\FunctionTok{log}\NormalTok{(AcresBurned)) ) }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_distiller}\NormalTok{( }\AttributeTok{palette=}\StringTok{"Reds"}\NormalTok{, }\AttributeTok{trans=}\StringTok{"reverse"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Longitude"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Latitude"}\NormalTok{, }\AttributeTok{color=}\StringTok{"Log( Acres burned)"}\NormalTok{ )}
\NormalTok{shape }\SpecialCharTok{+}\NormalTok{ map }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/Wildfires-1.pdf}

}

\caption{Locations of wildfires in California between 2013 and 2020
highlighting state boundaries (left) and with an underlying map (right).
The size of the burned area is visualized using colour.}

\end{figure}%

The plots highlight that there are areas of California that did not
record any wildfires. A look on the map reveals that the south-east
consists of the Mojave and Colorado Deserts and thus we would not expect
to see wildfires there.

\section{Intensity of a point
process}\label{intensity-of-a-point-process}

\subsection{Motivation}\label{motivation-3}

When analyzing point pattern data across a region \(\mathcal{S}\), we
want to identify areas with a high density of points and explore
possible reasons for the observed point patterns. For the wildfire data
in Figure @ref(fig:Wildfires), we already found that some variation in
the density of the points can be explained by the varied landscape and
climate.

However, such conclusions may be difficult in some applications. Let's
consider a data set on tornadoes across the United States (excluding
Alaska and Hawaii) between 1950 and 2021:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tornadoes }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/Tornadoes.csv"}\NormalTok{ )}
\FunctionTok{ggplot}\NormalTok{( Tornadoes, }\FunctionTok{aes}\NormalTok{( Lon, }\AttributeTok{y=}\NormalTok{Lat ) ) }\SpecialCharTok{+} \FunctionTok{annotation\_map\_tile}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_spatial\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Longitude"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Latitude"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.55\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/TORNADO-1.pdf}

}

\caption{Locations of tornadoes across the United States}

\end{figure}%

From this plot we cannot identify where the most tornadoes occur. In
such cases a common approach is to consider the \textbf{intensity} of
the point process, which is directly related to the expected number of
points over any subregion \(\mathcal{B}\subseteq\mathcal{S}\).

\textbf{Important:} When working with point pattern data, both the
number of points and their locations are random. For instance, we do not
know how many wildfires will we observe and where they will occur.

Let's formally define the intensity of a point process. For a subregion
\(\mathcal{B}\subseteq\mathcal{S}\), we introduce the random variable
\(N(\mathcal{B})\) as the number of points of our sample that lie within
\(\mathcal{B}\). The \textbf{intensity function}
\(\lambda:\mathcal{S}\to\mathbb{R}_+=\{x\in\mathbb{R}:x\geq0\}\) then
describes the expectation of \(N(\mathcal{B})\) with {[}
\mu(\{\mathcal{B}\}) = \mathbb{E}\left[N(\mathcal{B})\right] =
\int\_\{\mathcal{B}\} \lambda(\mathbf{s}) \mathrm{d} \mathbf{s}. {]}
When \(\lambda(\cdot)\) is constant across \(\mathcal{S}\), we term the
point process \textbf{homogeneous}. Otherwise, we call the point process
\textbf{non-homogeneous}.

In what follows we describe two methods for visualizing the intensity:
quadrat counting (Section \hyperref[quadrat-counting]{4.6.2}) and the
kernel smoothed intensity function (Section
\hyperref[kernel-smoothed-intensity]{4.6.3}). These plots may also help
us to explore whether a process is homogeneous or non-homogeneous, but
we will see that it is not straightforward.

\subsection{Quadrat counting}\label{quadrat-counting}

Let \(\mathbf{s}_1,\ldots,\mathbf{s}_n\in\mathcal{S}\) denote the
locations of the observed points. We then partition \(\mathcal{S}\) into
disjoint subregions \(\mathcal{B}_1,\ldots,\mathcal{B}_m\), with
\(\bigcup_{j=1}^m \mathcal{B}_j = \mathcal{S}\), and count the number of
points lying within each of them. This gives us an estimate
\(\widehat{\mu(\mathcal{B}_j)}\) for
\(\mu(\mathcal{B}_j)~(j=1,\ldots,m)\), with \begin{equation}
\widehat{\mu(\mathcal{B}_j)} = \sum_{i=1}^n \mathbb{I}\{\mathbf{s}_i \in \mathcal{B}_j\},
(\#eq:Quadrant)
\end{equation} where \(\mathbb{I}\{\mathbf{s}_i \in \mathcal{B}_j\}=1\)
if \(\mathbf{s}_i \in \mathcal{B}_j\) and
\(\mathbb{I}\{\mathbf{s}_i \in \mathcal{B}_j\}=0\) when
\(\mathbf{s}_i \notin \mathcal{B}_j\). One common choice is to set
\(\mathcal{B}_1,\ldots,\mathcal{B}_m\) to rectangles or quadrats. This
is why this approach is called \textbf{quadrat counting}.

If we assume that \(\lambda(\mathbf{s})\) is constant for all
\(\mathbf{s}\in\mathcal{B}_j\), we can estimate the value of
\(\lambda(\mathbf{s})~(\mathbf{s}\in\mathcal{B}_j)\) as \begin{equation}
\hat{\lambda}^{(Q)}(\mathbf{s}) = \frac{\widehat{\mu(\mathcal{B}_j)}}{|\mathcal{B}_j|},
(\#eq:Quadrat)
\end{equation} where \(|\mathcal{B}_j|\) is the area of the subregion
\(\mathcal{B}_j\).

When performing this approach, we have to balance two aspects:

\begin{itemize}
\item
  \(\mathcal{B}_1,\ldots,\mathcal{B}_m\) should be small enough such
  that \(\lambda(\mathbf{s})\) is approximately constant for all
  \(\mathbf{s}\in\mathcal{B}_j\)
\item
  \(\mathcal{B}_1,\ldots,\mathcal{B}_m\) should be large enough such
  that \(\widehat{\mu(\mathcal{B}_j)}\) in @ref(eq:Quadrant) provides a
  reliable estimate for \(\mu(\mathcal{B}_j)\).
\end{itemize}

Let's again consider the wildfire data set. While we may implement the
approach from scratch, we us the \textbf{spatstat} R package. The first
step is to combine the shapefile and observed points into a \textbf{ppp}
object:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(spatstat)}
\NormalTok{WildFires\_ppp }\OtherTok{\textless{}{-}} \FunctionTok{ppp}\NormalTok{( WildFires}\SpecialCharTok{$}\NormalTok{Longitude, WildFires}\SpecialCharTok{$}\NormalTok{Latitude,}
                      \AttributeTok{poly =}\NormalTok{ California}\SpecialCharTok{$}\NormalTok{geometry[[}\DecValTok{1}\NormalTok{]][[}\DecValTok{1}\NormalTok{]] )}
\end{Highlighting}
\end{Shaded}

We now use the \textbf{quadratcount()} function and compare results for
two choices for the number of subregions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{( }\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{mai=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{,}\FloatTok{0.01}\NormalTok{,}\FloatTok{0.5}\NormalTok{,}\FloatTok{0.01}\NormalTok{) )}
\NormalTok{WildFireQuadrants }\OtherTok{\textless{}{-}} \FunctionTok{quadratcount}\NormalTok{( WildFires\_ppp, }\AttributeTok{nx=}\DecValTok{2}\NormalTok{, }\AttributeTok{ny=}\DecValTok{2}\NormalTok{ )}
\FunctionTok{plot}\NormalTok{( WildFireQuadrants )}
\NormalTok{WildFireQuadrants }\OtherTok{\textless{}{-}} \FunctionTok{quadratcount}\NormalTok{( WildFires\_ppp, }\AttributeTok{nx=}\DecValTok{6}\NormalTok{, }\AttributeTok{ny=}\DecValTok{6}\NormalTok{ )}
\FunctionTok{plot}\NormalTok{( WildFireQuadrants )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/quadrat-1.pdf}

}

\caption{Quadrat counting estimates based on a 2x2 grid (left) and a 6x6
grid (right).}

\end{figure}%

The plots indicate that the highest intensities are observed close to
San Francisco and in the south-west, close to the Mexican border. We may
argue that setting \(m=4\) is too small as we cannot identify a clear
structure, while the \(6\times6\) grid offers a good balance between the
two aspects outlined above. Due to the large differences in the
estimated intensities, we may conclude that the point process is
non-homogeneous.

\textbf{Important:} The quadratcount() function counts the number of
points, but does not calculate the intensity. So we would need to divide
by the size of the subregions to obtain \(\hat{\lambda}^{(Q)}(\cdot)\).

\subsection{Kernel smoothed intensity}\label{kernel-smoothed-intensity}

Our second method for exploring the intensity of a point process is
based on the concept of density plots we studied in Section
\hyperref[histograms-and-density-plots]{2.2.4}. Given a probability
density function (kernel) \(K\), we define the kernel smoothed intensity
estimate for \(\lambda(\mathbf{s})\), \(\mathbf{s}\in\mathcal{S}\), as
\begin{equation}
\hat\lambda^{(K)}(\mathbf{s}) = \sum_{i=1}^{n} K\left(||\mathbf{s}-\mathbf{s}_i||_2\right),
(\#eq:SKI)
\end{equation} where \(||\mathbf{s}-\mathbf{s}_i||_2\) denotes the
Euclidian distance of \(\mathbf{s}\) and \(\mathbf{s}_i\). The
difference to Section \hyperref[histograms-and-density-plots]{2.2.4} is
that \(K\) is the density of a random vector and not of a random
variable.

The estimate \(\hat{\lambda}^{(K)}\) defined in equation @ref(eq:SKI)
satisfies {[} \int\_\{\mathbb{R}\^{}2\}
\hat{\lambda}\^{}\{(K)\}(\mathbf{s}) \mathrm{d}\mathbf{s} = n. {]}
However, we may have
\(\int_{\mathcal{S}} \hat{\lambda}^{(K)}(\mathbf{s}) \mathrm{d}\mathbf{s} \neq n\),
because \(\hat{\lambda}^{(K)}(\mathbf{s})\) may be positive outside the
region \(\mathcal{S}.\) This is also referred to as the \textbf{edge
effect bias}, because it is usually caused by points that lie close to
the boundary of \(\mathcal{S}\).

This leads to the \textbf{uniformly corrected kernel smoothed intensity
function} {[} \hat\lambda\^{}\{(C)\}(\mathbf{s}) =
\frac{1}{g(\mathbf{s})}\sum\_\{i=1\}\^{}\{n\}
K\left(\textbar\textbar{}\mathbf{s}-\mathbf{s}\_i\textbar\textbar{}\emph{2\right),
\quad\mathrm{with}\quad g(\mathbf{s}) = \int}\{\mathcal{S}\}
K(\mathbf{s}-\tilde{\mathbf{s}}) \mathrm{d}\tilde{\mathbf{s}}. {]} This
alternative estimate corrects for the edge effect bias.

There are multiple choices for the kernel \(K\) and the correction, but
we will limit our analysis to \(\hat\lambda^{(K)}\) and
\(\hat\lambda^{(C)}\), because they are the default settings for the
\textbf{density.ppp()} function in the spatstat R package:

\textbf{Example:} Let's consider the Californian wildfire data set
again, and we want to compare the two estimates \(\hat\lambda^{(K)}\)
and \(\hat\lambda^{(C)}\). We can directly apply the density.ppp()
function to the ppp object we computed earlier:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lambdaK }\OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{( WildFires\_ppp, }\AttributeTok{edge=}\ConstantTok{FALSE}\NormalTok{ )}
\FunctionTok{plot}\NormalTok{( lambdaK, }\AttributeTok{main=}\StringTok{"Uncorrected"}\NormalTok{ )}
\NormalTok{lambdaC }\OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{( WildFires\_ppp, }\AttributeTok{edge=}\ConstantTok{TRUE}\NormalTok{ )}
\FunctionTok{plot}\NormalTok{( lambdaC, }\AttributeTok{main=}\StringTok{"Corrected"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/unnamed-chunk-27-1.pdf}

}

\caption{Uncorrected (left) and corrected (right) kernel smoothed
intensity for Californian wildfires.}

\end{figure}%

The range of values in the legend shows that the estimates for
\(\lambda^{(C)}\) are higher than for \(\lambda^{(K)}\) due to the
correction and we also see some differences in the colour pattern close
to the boundaries. Further, the spatial structure is similar to that of
\(\hat{\lambda}^{(Q)}\) in Figure @ref(fig:quadrat) - so we again
conclude that the point process describing the occurrence of wildfires
is non-homogeneous.

\textbf{Remark:} Similar to density plots we can also introduce the
notion of bandwidth in equation @ref(eq:SKI). We can manually set the
bandwidth in the density.ppp() function using the argument
\textbf{sigma}, with a smaller value leading to the kernels being more
concentrated around the observed locations, as the following plot
illustrates:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lambda\_sigma0}\FloatTok{.1} \OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{( WildFires\_ppp, }\AttributeTok{edge=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{sigma =} \FloatTok{0.1}\NormalTok{ )}
\FunctionTok{plot}\NormalTok{( lambda\_sigma0}\FloatTok{.1}\NormalTok{, }\AttributeTok{main=}\StringTok{"sigma=0.1"}\NormalTok{ )}
\NormalTok{lambda\_sigma10 }\OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{( WildFires\_ppp, }\AttributeTok{edge=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{sigma =} \DecValTok{10}\NormalTok{ )}
\FunctionTok{plot}\NormalTok{( lambda\_sigma10, }\AttributeTok{main=}\StringTok{"sigma=10"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/unnamed-chunk-28-1.pdf}

}

\caption{Corrected kernel smoothed intensity estimates for Californian
wildfires with sigma=0.1 (left) and sigma=10 (right).}

\end{figure}%

Note, both of these choices for the bandwidth are poor. A bandwidth of
\(\sigma=0.1\) gives a too strong concentration, with wildfires
predicted with very high probability at the same locations as the
observed fires. On the other hand, a bandwidth of \(\sigma=10\) leads to
the estimated intensity function being smoothed too much over space,
with all areas of California predicted to have a similar risk of
wildfires, which is in contrast to our previous findings.

\subsection{Tornadoes across the United
States}\label{tornadoes-across-the-united-states}

Let's return to the example in Figure @ref(fig:TORNADO). While there is
a strong indication that the point process is non-homogeneous, we need
to analyze the data in more detail. We start by again loading the data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tornadoes }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/Tornadoes.csv"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

There are a total of 68663 tornadoes in the data set. In the next
Section \hyperref[visualizing-lattice-data]{4.7} we will count the
number of storms per state and visualize the extracted numbers. Here we
focus on the states of Kansas and Texas and estimate the intensity
function. For this, we first need to load a shapefile for the USA, which
includes the state boundaries:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{USA }\OtherTok{\textless{}{-}} \FunctionTok{read\_sf}\NormalTok{( }\StringTok{"Data/gadm41\_USA\_shp/gadm41\_USA\_1.shp"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

When analyzing the California wildfire data, we created the ppp object
using the largest polygon in the shapefile. However, this may not be
ideal in all cases, in particular, when we are working with multiple
islands. Here we introduce an alternative approach for constructing the
ppp object, which performs better in cases where we want to include the
full shapefile.

The first step is to extract the shapefiles for Kansas and Texas from
the shapefile for the USA, reduce their complexity and then change the
projection (\textbf{we cannot use the WGS84 coordinate system here}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{KS }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{( USA, NAME\_1}\SpecialCharTok{==}\StringTok{"Kansas"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{st\_simplify}\NormalTok{( }\AttributeTok{dTolerance =} \DecValTok{2000}\NormalTok{ )}\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{st\_transform}\NormalTok{( }\AttributeTok{crs=}\DecValTok{3857}\NormalTok{ )}
\NormalTok{TX }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{( USA, NAME\_1}\SpecialCharTok{==}\StringTok{"Texas"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{st\_simplify}\NormalTok{( }\AttributeTok{dTolerance =} \DecValTok{2000}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{st\_transform}\NormalTok{( }\AttributeTok{crs=}\DecValTok{3857}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

Now we have to transform the observed locations of Tornadoes to the same
coordinate system as the shapefiles:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tornadoes\_transformed }\OtherTok{\textless{}{-}}\NormalTok{ Tornadoes }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{st\_as\_sf}\NormalTok{( }\AttributeTok{coords=}\FunctionTok{c}\NormalTok{(}\StringTok{"Lon"}\NormalTok{,}\StringTok{"Lat"}\NormalTok{), }\AttributeTok{crs=}\DecValTok{4326}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{st\_transform}\NormalTok{( }\AttributeTok{crs=}\DecValTok{3857}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{st\_coordinates}\NormalTok{( )}
\end{Highlighting}
\end{Shaded}

Finally, we create the ppp objects for Kansas and Texas:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Texas\_ppp }\OtherTok{\textless{}{-}} \FunctionTok{ppp}\NormalTok{( Tornadoes\_transformed[,}\DecValTok{1}\NormalTok{], Tornadoes\_transformed[,}\DecValTok{2}\NormalTok{], }
                  \AttributeTok{window =} \FunctionTok{as.owin}\NormalTok{(TX) )}
\NormalTok{Kansas\_ppp }\OtherTok{\textless{}{-}} \FunctionTok{ppp}\NormalTok{( Tornadoes\_transformed[,}\DecValTok{1}\NormalTok{], Tornadoes\_transformed[,}\DecValTok{2}\NormalTok{],}
                   \AttributeTok{window =} \FunctionTok{as.owin}\NormalTok{(KS) )}
\end{Highlighting}
\end{Shaded}

\textbf{Remark:} The warning messages we received are due to not all
tornadoes we observed being located in these states.

Having converted the data to a ppp object, we derive and plot the
corrected kernel smoothed intensity function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lambdaC\_KS }\OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{( Kansas\_ppp, }\AttributeTok{edge=}\ConstantTok{TRUE}\NormalTok{ )}
\FunctionTok{plot}\NormalTok{( lambdaC\_KS, }\AttributeTok{main=}\StringTok{"Kansas"}\NormalTok{ )}
\NormalTok{lambdaC\_TX }\OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{( Texas\_ppp, }\AttributeTok{edge=}\ConstantTok{TRUE}\NormalTok{ )}
\FunctionTok{plot}\NormalTok{( lambdaC\_TX, }\AttributeTok{main=}\StringTok{"Texas"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/unnamed-chunk-34-1.pdf}

}

\caption{Corrected smoothed kernel intensity for the occurrence of
tornadoes across Kansas and Texas.}

\end{figure}%

While the intensity values are very small, there are clear conclusions
we can draw from the plots. For Texas, the intensity is much higher in
the north and east than along the border to Mexico. So it's reasonable
to conclude that the point process describing tornadoes across Texas is
non-homogeneous. When comparing the results of Texas and Kansas, the
peak intensities are comparable, but Kansas shows less variance in the
values of the intensity function. Consequently, we may argue that the
point process describing the locations of tornadoes across Kansas is
homogeneous.

\textbf{Important:} When discussing whether a point process is
homogeneous or not, we also have to consider whether the locations of
points depend on each other. For instance for wildfires, this may
correspond to the probability of a wildfire being dependent on whether
wildfires occurred recently and in close proximity. It's presumably fine
to assume that tornadoes occur independently of each other, and thus our
conclusions are reasonable. However, in other cases we may be unable to
make such conclusions as the techniques we covered in this course do not
allow us to distinguish between whether (a) the point process is
non-homogeneous and (b) the point process is homogeneous but dependent.
In these cases we need to account for the context of the study.

\section{Visualizing lattice data}\label{visualizing-lattice-data}

When working with lattice data we have to plot values for a fixed number
of areas instead of a fixed number of points. These areas are usually
defined by shapefiles. We will consider two examples to demonstrate how
to visualize lattice/areal data using shapefiles.

\subsection{Population density across London
boroughs}\label{population-density-across-london-boroughs}

Suppose we have observations which refer to the different boroughs of
London, such as average income for each borough. Let's load the
shapefiles for the boroughs (and the City of London):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{London }\OtherTok{\textless{}{-}} \FunctionTok{read\_sf}\NormalTok{(}\StringTok{"Data/London.shp"}\NormalTok{)}
\FunctionTok{class}\NormalTok{( London )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "sf"         "tbl_df"     "tbl"        "data.frame"
\end{verbatim}

We see that the object \textbf{London} consists of: (1) a \textbf{sf}
part which includes the data to produce the shapefiles and (2) a data
frame to store the information for each borough. The information
provided in the data frame includes, for instance, area size in
hectares.

We want to explore and visualize the population density in 2020. For
this we import the data on the number of residents in 2020 for each
borough provided in the file ``London.csv''. We load the population data
and create a new variable which represents population density in the
data frame of the object \textbf{London}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{London\_population }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/London.csv"}\NormalTok{ )}
\NormalTok{London\_population }\OtherTok{\textless{}{-}}\NormalTok{ London\_population }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{Population =} \FunctionTok{as.numeric}\NormalTok{( }\FunctionTok{gsub}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{,"}\NormalTok{,}\StringTok{""}\NormalTok{,Population) ) )}
\NormalTok{London }\OtherTok{\textless{}{-}} \FunctionTok{inner\_join}\NormalTok{( }\AttributeTok{x=}\NormalTok{London, }\AttributeTok{y=}\NormalTok{London\_population, }\AttributeTok{by=}\FunctionTok{c}\NormalTok{(}\StringTok{"NAME"}\OtherTok{=}\StringTok{"Borough"}\NormalTok{) )}
\NormalTok{London }\OtherTok{\textless{}{-}}\NormalTok{ London }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{( }\AttributeTok{Density =}\NormalTok{ Population }\SpecialCharTok{/}\NormalTok{ HECTARES )}
\end{Highlighting}
\end{Shaded}

To visualize the calculated population densities, we have two options:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Use color as a visual cue
\item
  Place points within the boroughs, with their size depending on the
  density
\end{enumerate}

Let's create both options:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( }\AttributeTok{data=}\NormalTok{London, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{Density) ) }\SpecialCharTok{+} \FunctionTok{geom\_sf}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_distiller}\NormalTok{( }\AttributeTok{palette=}\StringTok{"Reds"}\NormalTok{, }\AttributeTok{trans=}\StringTok{"reverse"}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{( }\AttributeTok{axis.title=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{15}\NormalTok{), }\AttributeTok{axis.text=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{15}\NormalTok{) ) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Longitude"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Latitude"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/unnamed-chunk-37-1.pdf}

}

\caption{Population density in people per hectare for the 33 London
boroughs.}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( }\AttributeTok{data=}\NormalTok{London ) }\SpecialCharTok{+} \FunctionTok{geom\_sf}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{( }\AttributeTok{data=}\FunctionTok{st\_centroid}\NormalTok{(London), }\FunctionTok{aes}\NormalTok{( }\AttributeTok{size =}\NormalTok{ Density, }\AttributeTok{geometry =}\NormalTok{ geometry), }
              \AttributeTok{stat =} \StringTok{"sf\_coordinates"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Longitude"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Latitude"}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{( }\AttributeTok{axis.title=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{15}\NormalTok{), }\AttributeTok{axis.text=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{15}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: st_centroid assumes attributes are constant over geometries
\end{verbatim}

\begin{verbatim}
Warning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not
give correct results for longitude/latitude data
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/unnamed-chunk-38-1.pdf}

}

\caption{Population density for the 33 London boroughs, with size of
points used as visual cure.}

\end{figure}%

We see that Central London (with the exception of the City of London)
records the highest population densities. The outer boroughs tend to
have a population density between 40 and 80 people per hectare.

\textbf{Remark:} When we download shapefiles from gadm.org, we can get
the different administrative levels and assign values in the same way.
We will demonstrate this in the next example.

\subsection{Tornadoes across the United
States}\label{tornadoes-across-the-united-states-1}

We want to visualize the number of tornadoes for each US state in the
continental United States. Let's load the data from ``Tornadoes.csv''
and extract the number of tornadoes for each state between 1950 and
2021:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tornadoes }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"Data/Tornadoes.csv"}\NormalTok{ )}
\NormalTok{TornadoesNumbers }\OtherTok{\textless{}{-}}\NormalTok{ Tornadoes }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{count}\NormalTok{( State )}
\end{Highlighting}
\end{Shaded}

We then load the shapefile and combine it with the extracted number of
tornadoes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{USA }\OtherTok{\textless{}{-}} \FunctionTok{read\_sf}\NormalTok{(}\StringTok{"Data/gadm41\_USA\_shp/gadm41\_USA\_1.shp"}\NormalTok{)}
\NormalTok{USA }\OtherTok{\textless{}{-}} \FunctionTok{st\_simplify}\NormalTok{( USA, }\AttributeTok{dTolerance =} \DecValTok{5000}\NormalTok{, }\AttributeTok{preserveTopology =} \ConstantTok{TRUE}\NormalTok{ )}
\NormalTok{USA}\SpecialCharTok{$}\NormalTok{HASC\_1 }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{( }\StringTok{".*}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{."}\NormalTok{, }\StringTok{""}\NormalTok{, USA}\SpecialCharTok{$}\NormalTok{HASC\_1)}
\NormalTok{USA }\OtherTok{\textless{}{-}} \FunctionTok{inner\_join}\NormalTok{( USA, TornadoesNumbers, }\AttributeTok{by=}\FunctionTok{c}\NormalTok{(}\StringTok{"HASC\_1"}\OtherTok{=}\StringTok{"State"}\NormalTok{) ) }
\end{Highlighting}
\end{Shaded}

\textbf{Remark:} The third line of code is necessary because the states
in \textbf{TornadoesCount} are represented as ``TX'', ``KS'', etc., but
these abbreviations do not exist in \textbf{USA}. Therefore, we change
the entries \textbf{HASC\_1} column to use it for combining the data
frames.

For this analysis we Alaska and Hawaii from the analysis as we have no
data for it. The observations can be removed using the filter function
and we rename some of the variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{USA }\OtherTok{\textless{}{-}}\NormalTok{ USA }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{( }\SpecialCharTok{!}\NormalTok{NAME\_1 }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Alaska"}\NormalTok{, }\StringTok{"Hawaii"}\NormalTok{) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{( }\AttributeTok{State =}\NormalTok{ NAME\_1, }\AttributeTok{Number=}\NormalTok{n )}
\end{Highlighting}
\end{Shaded}

We can now plot the data as we did in the example considering the data
for London. We make one addition and also add a map to the plot using
the ggspatial R package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( USA, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{Number) ) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{annotation\_map\_tile}\NormalTok{( }\AttributeTok{zoom=}\DecValTok{5}\NormalTok{ ) }\SpecialCharTok{+} \FunctionTok{geom\_sf}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_distiller}\NormalTok{( }\AttributeTok{palette=}\StringTok{"Reds"}\NormalTok{, }\AttributeTok{trans=}\StringTok{"reverse"}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{fill=}\StringTok{"Number"}\NormalTok{, }\AttributeTok{x=}\StringTok{"Longitude"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Latitude"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{( }\AttributeTok{axis.title=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{15}\NormalTok{), }\AttributeTok{axis.text=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{15}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{04-SpatialDataAnalysis_files/figure-pdf/TornadoesUSA-1.pdf}

}

\caption{Number of tornadoes for each state in the continential US for
1950-2021.}

\end{figure}%

Using the map we conclude that the highest number of tornadoes occur in
Texas, Oklahoma, Kansas and Florida, while the Western United States
observes a relatively small number of tornadoes.

\section{Summary}\label{summary-3}

In this chapter we considered the analysis of spatial data:

\begin{itemize}
\item
  There are three types of spatial data: point-referenced, point pattern
  and lattice
\item
  All types of spatial data can be visualized using shapefiles (using
  the \textbf{sf} package) and/or maps (using \textbf{ggspatial}). In
  these cases we often have to resort to colour as a visual cue.
\item
  When the data covers a large spatial area/region, we should carefully
  select the projection we use for our shapefiles
\item
  When working with point-referenced data, we can

  \begin{itemize}
  \item
    Employ inverse distance weighting to make predictions at unobserved
    sites
  \item
    Estimate the semi-variogram or perform principal component analysis
    to analyse spatial dependence
  \item
    Use principal component analysis to analyze the spatial structure in
    the data
  \end{itemize}
\item
  Quadrat counting and kernel intensity function for analyzing point
  pattern data;
\end{itemize}

These techniques allow us to explore features of the spatial data and
are essential for developing models for spatial data - these models are
however too complex to be fully considered in a Year 2 unit. Some of
them will be covered in MA32024 Statistical Modelling and Data Analytics
3B.


\backmatter
\bibliography{book.bib,packages.bib}



\end{document}
