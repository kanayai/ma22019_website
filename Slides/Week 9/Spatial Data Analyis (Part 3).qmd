---
title: "MA22019 - Spatial Data Analysis (Part 3)"
author: "Christian Rohrbeck"
date: "2 April 2025"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, warning=FALSE, message=FALSE}
colorize <- function(x, color="blue") {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

## Plan for Today

We finish the remaining content in the lecture notes:

* Comments on semi-variogram and PCA

* Visualization of point pattern data (Sections 4.5)

* Quadrat counting and kernel smoothed intensity function (Section 4.6)

* Visualizing lattice/areal data (Section 4.7)

There will be a revision class in Week 10 (after the Easter break). Look out for a message about a poll on Moodle.

# Some comments on last week's content

## Semi-variogram - width and cutoff 


The **width** in the variogram() function determines the number of points used to estimate $\hat\gamma(h)$:

* A too small value means that estimates are highly uncertain

* A too large value means that important aspects may be missed



The **cutoff** determines which range of distances we consider: 

* A too small value may imply that we miss important features

* A too large value increases the risk of spurious relations affecting our conclusions



## What to look out for? 


It may be useful to consider the number of points per estimate:

```{r, eval=FALSE, echo=TRUE}
gamma_hat$np
```

If the number of points is too small, it may be worth changing the width or cutoff.



Certain patterns in the semi-variogram may, for instance, be due to the constant mean assumption not being satisfied. 

**However**, such features may also be caused by randomness while all assumptions hold.



## PCA - when does it work well?

Ideally we want the data to lie in a low-dimensional linear subspace:

* Only a few eigenvectors need to be considered for exploration

* Good potential for dimension reduction

We have not really covered the topic in enough depth to introduce methods for exploring whether the data lies in a linear subspace in our examples.

Instead, we only explore whether a linear relation between pairs of variables is realistic - we should pick spatially close sites.

## Illustration

```{r, out.width='95%', fig.height=4, fig.width=8, fig.align='center', warning=FALSE, message=FALSE, echo=FALSE}
library(ggplot2)
library(patchwork)
ColoradoPrecip <- read.csv("PrecipitationColorado.csv")
ggplot( ColoradoPrecip, aes( x=VALDEZ, y=WETMORE) ) + geom_point() + theme_bw() + 
ggplot( ColoradoPrecip, aes( x=TOWNER, y=STONINGTON) ) + geom_point() + theme_bw()
```

We find quite a strong correlation and thus there is potential.

# Analysis of point pattern data

## Visualization 


We can use the same techniques as for point-referenced data:

* Placing points onto a shapefile

* Creating maps with points on top

Let's look at an example</br>**`r colorize("-> R Markdown file WildFires.Rmd")`**



**But**: We may not always see the structure clearly</br>**`r colorize("-> R Markdown file Tornadoes.Rmd")`**

This leads us to the concept of **intensity** of a point process.

## Intensity of a point process 


Remember that the locations and their number are **random** when working with point pattern data.

Let $N(\mathcal{B})$ be the random variable representing the number of points located in $\mathcal{B}\subseteq\mathcal{S}$.



The **`r colorize("intensity")`** then describes $\mathbb{E}\left[N(\mathcal{B})\right]$ for any $\mathcal{B}\subseteq\mathcal{S}$ with
\[
\mu({\mathcal{B}}) = \mathbb{E}\left[N(\mathcal{B})\right] = \int_{\mathcal{B}} \lambda(\mathbf{s}) \mathrm{d} \mathbf{s}.
\]
We refer to $\lambda:\mathcal{S}\to\mathbb{R}_+=\{x\in\mathbb{R}:x\geq0\}$ as the **intensity function**.


## Analysis of the intensity function


We say that that the point process is 

* **homogeneous** when $\lambda(\cdot)$ is constant

* **non-homogeneous** when $\lambda(\cdot)$ is not constant




In what follows, we descrive two techniques for visualizing the intensity function:

* Quadrat counting

* Kernel smoothed intensity function



# Quadrat Counting

## Overview {.smaller}

Given observed locations $\mathbf{s}_1,\ldots,\mathbf{s}_n$, we perform three steps:

1. Split $\mathcal{S}$ into disjoint areas $\mathcal{B}_1,\ldots,\mathcal{B}_m$ which are set as rectangles (**quadrats**).

2. Count the number of points in $\mathcal{B}_j$ as an estimate for $\mathbb{E}\left[N(\mathcal{B}_j)\right]$,
\[
\widehat{\mu(\mathcal{B}_j)} = \sum_{i=1}^n \mathbb{I}\{\mathbf{s}_i \in \mathcal{B}_j\}.
\]

3. The intensity function is then estimated as
\[
\hat{\lambda}^{(Q)}(\mathbf{s}) = \frac{\widehat{\mu(\mathcal{B}_j)}}{|\mathcal{B}_j|},\quad\mathbf{s}\in\mathcal{B}_j,
\]
where $|\mathcal{B}_j|$ denotes the area of $\mathcal{B}_j$.

## Details

The **quadratcount()** function in the spatstat R package performs steps 1) and 2).

We have to balance two aspects:

- $\mathcal{B}_j$ should be small enough such that $\lambda(\mathbf{s})$ is approximately constant for all $\mathbf{s}\in\mathcal{B}_j$.

- $\widehat{\mu(\mathcal{B}_j)}$ should be a reliable estimate for $\mu(\mathcal{B}_j)$. 

**`r colorize("-> R Markdown file WildFires.Rmd")`**


# Kernel Smoothed Intensity Function

## Overview 


Suppose we have a probability density (kernel) $K(\cdot)$ and, similar to a density plot, we define
\[
\hat\lambda^{(K)}(\mathbf{s}) = \sum_{i=1}^{n} K\left(||\mathbf{s}-\mathbf{s}_i||_2\right),
\]
where $||\mathbf{s}-\mathbf{s}_i||_2$ is the Euclidian distance.



However, this approach assigns positive probability to areas outside $\mathcal{S}$ and thus
\[
\int_{\mathcal{S}} \hat{\lambda}^{(K)}(\mathbf{s}) \mathrm{d}\mathbf{s} \neq n.
\]


## Edge correction

The **uniformly corrected smoothed kernel intensity function** is defined as
\[
\hat\lambda^{(C)}(\mathbf{s}) = \frac{1}{g(\mathbf{s})}\sum_{i=1}^{n} K\left(||\mathbf{s}-\mathbf{s}_i||_2\right),
\]
where 
\[
g(\mathbf{s}) = \int_{\mathcal{S}} K(\mathbf{s}-\tilde{\mathbf{s}}) \mathrm{d}\tilde{\mathbf{s}}.
\]

This correction is available in the spatstat R package.</br>**`r colorize("-> R Markdown file WildFires.Rmd")`**

## Analysis of Tornadoes

Let's consider a second example</br>**`r colorize("-> R Markdown file Tornadoes.Rmd")`**


# Visualizing Lattice Data

## Overview

We will consider two examples:

* Population density across London boroughs

* Number of tornadoes per US state

In both of these cases, we will exploit an aspect of shapefiles we have not considered so far.

Specifically, we make use of the shapefile coming with a data frame, which permits data wrangling.


## Example 1 - Boroughs in London 

Let's load a shapefile for London

```{r, echo=-1, message=FALSE, warning=FALSE}
library(sf)
London <- read_sf("London.shp")
class( London )
```

We see that **London** includes a data frame. 

**All entries except for the "geometry" can be manipulated.**

Let's visualize population density for the different boroughs.</br>**`r colorize("-> R Markdown file")`**


## Example 2 - Tornadoes in the US

The data in "Tornadoes.csv" provides observations on tornadoes for 1950-1921 for the whole US.

Let's visualize the number of tornadoes recorded for each state.
</br>**`r colorize("->R Markdown file Tornadoes.Rmd")`**

## Summary

* We covered all the relevant material:

  + Visualization of point pattern and lattice data

  + Exploration of the intensity of a point process

* Problem Sheet 8 provides some practice and will be considered in the tutorial in Week 10.

* Revision lecture in Week 10:

  + We will revisit two topics of your choice

  + Selection via a feedback form on Moodle (the two most popular choices win)



