knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(tidyr)
library(tidytext)
library(patchwork)
library(dplyr)
library(ggplot2)
library(tidyr)
library(tidytext)
library(patchwork)
Fakes_raw <- read.csv("Dickens_Fakes.csv", encoding = "UTF-8")
Originals_raw <- read.csv("Dickens_Originals.csv", encoding = "UTF-8")
Fakes_words <- Fakes_raw %>%
unnest_tokens( word, Text ) %>%
mutate( word = gsub( "_", "", word ) ) %>%
count( word, sort = TRUE ) %>%
mutate( tf = n / sum(n) ) %>%
anti_join( stop_words, by="word" )
Originals_words <- Originals_raw %>%
unnest_tokens( word, Text ) %>%
mutate( word = gsub( "_", "", word ) ) %>%
count( word, sort = TRUE ) %>%
mutate( tf = n / sum(n) ) %>%
anti_join( stop_words, by="word" )
plot_Fakes <- Fakes_words %>%
slice_head( n=10 ) %>%
mutate( word = reorder(word,tf) ) %>%
ggplot( aes( x=tf, y=word ) ) + geom_col() +
labs( x="Term frequency", y="Word", title="AI generated" ) + theme_bw() +
theme( axis.title=element_text(size=17), axis.text=element_text(size=15) )
plot_Originals <- Originals_words %>%
slice_head( n=10 ) %>%
mutate( word = reorder(word,tf) ) %>%
ggplot( aes( x=n, y=word ) ) + geom_col() +
labs( x="Term frequency", y="Word", title="Original" ) + theme_bw() +
theme( axis.title=element_text(size=17), axis.text=element_text(size=15) )
plot_Fakes + plot_Originals
Words <- full_join( Fakes_words, Originals_words, by="word" ) %>%
rename( Fakes='term frequency.x', Originals='term frequency.y' ) %>%
mutate( Fakes = case_when( is.na(Fakes) == TRUE ~ 0, .default = Fakes),
Originals = case_when( is.na(Originals) == TRUE ~ 0, .default = Originals) )
Words <- full_join( Fakes_words, Originals_words, by="word" ) %>%
rename( Fakes=tf.x, Originals=tf.y ) %>%
mutate( Fakes = case_when( is.na(Fakes) == TRUE ~ 0, .default = Fakes),
Originals = case_when( is.na(Originals) == TRUE ~ 0, .default = Originals) )
ggplot( Words, aes( x=Fakes, y=Originals ) ) +
geom_point() +
geom_text( aes(label=word), check_overlap = TRUE, vjust=1.5 ) +
coord_trans( x="sqrt", y="sqrt" ) + theme_bw() +
labs( x="Term Frequency in AI generated texts",
y="Term Frequency in original texts" )
AFINN <- read.csv("AFINN Sentiment Lexicon.csv")
Fakes_sentiment <- Fakes_raw %>%
unnest_tokens( word, Text ) %>%
mutate( word = gsub( "_", "", word ) ) %>%
inner_join( AFINN, by="word" ) %>%
group_by( Title ) %>%
summarise( sentiment = mean(value) )
Originals_sentiment <- Originals_raw %>%
unnest_tokens( word, Text ) %>%
mutate( word = gsub( "_", "", word ) ) %>%
inner_join( AFINN, by="word" ) %>%
group_by( Title, Chapter ) %>%
summarise( sentiment = mean(value) )
Fakes_sentiment
Originals_sentiment
Fakes_sentiment %>% summarise( Mean=mean(sentiment), Median=median(sentiment) )
Originals_sentiment %>% summarise( Mean=mean(sentiment), Median=median(sentiment) )
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(tidyr)
library(tidytext)
library(patchwork)
library(lubridate)
library(sf)
library(gstat)
library(sp)
library(patchwork)
library(spatstat)
Water     <- read.csv( "Tesremos_Water.csv" )
Tesremos  <- read_sf( "Tesremos_Shapefile/Tesremos.shp" )
Locations <- read.csv("Tesremos_Locations.csv" )
Water <- Water %>% mutate( Date = as_date(Date, format="%Y-%m-%d" ) )
WaterMarch  <- filter( Water, month(Date) == 3 )
WaterAugust <- filter( Water, month(Date) == 8 )
WaterMarch <- WaterMarch %>%
pivot_longer( cols = Site_1:Site_102, names_to = "Site" ) %>%
group_by( Site ) %>%
summarise( Average = mean( value ) ) %>%
full_join( Locations, by=c("Site"="ID") )
WaterAugust <- WaterAugust %>%
pivot_longer( cols = Site_1:Site_102, names_to = "Site" ) %>%
group_by( Site ) %>%
summarise( Average = mean( value ) ) %>%
full_join( Locations, by=c("Site"="ID") )
IDW <- function( X, S, s_star, p){
d <- sqrt( (S[,1]-s_star[1])^2 + (S[,2]-s_star[2])^2 )
w <- d^(-p)
if( min(d) > 0 )
return( sum( X * w ) / sum( w ) )
else
return( X[d==0] )
}
points_lon <- seq( 98.5, 98.7, length.out=50 )
points_lat <- seq( 15.65, 15.75, length.out=30 )
pixels <- as.matrix( expand.grid( points_lon, points_lat ) )
min(WaterMarch$Average)
max(WaterMarch$Average)
predictMarch<- predictAugust <- c()
coord <- cbind( Locations$Longitude, Locations$Latitude )
for( j in 1:length(pixels[,1]) ){
predictMarch[j] <- IDW( X=WaterMarch$Average, S=coord, s_star=pixels[j,], p=1 )
predictAugust[j] <- IDW( X=WaterAugust$Average, S=coord, s_star=pixels[j,], p=2.3 )
}
min(predictMarch)
max(predictMarch)
knitr::opts_knit$set(root.dir = "/Users/submissions/Data Coursework 2")
knitr::opts_chunk$set(echo = TRUE)
library( dplyr )
library( ggplot2 )
library( tidytext )
library( wordcloud )
library( stringr )
library(patchwork)
library(tidyr)
library(topicmodels)
library(sf)
library(lubridate)
library(gstat)
library(spatstat)
library(tm)
Dickens_fake<-read.csv("Dickens_Fakes.csv")
Dickens_original <- read.csv("Dickens_Originals.csv")
AFINN <- get_sentiments( "afinn" )
Dickens_fake_raw <- Dickens_fake %>%
unnest_tokens( word, Text ) %>%
mutate( word = gsub( "_", "", word ) )
Dickens_original_raw <- Dickens_original %>%
unnest_tokens( word, Text ) %>%
mutate( word = gsub( "_", "", word ) )
Dickens_fake_count <- Dickens_fake_raw %>%
count( word, sort=TRUE ) %>%
mutate( 'term frequency' = n / sum(n), rank = row_number() )
Dickens_original_count <- Dickens_original_raw %>%
count( word, sort=TRUE ) %>%
mutate( 'term frequency' = n / sum(n), rank = row_number() )
data( stop_words )
Dickens_original_count <- Dickens_original_count %>%
anti_join(stop_words)
Dickens_fake_count <- Dickens_fake_count %>%
anti_join(stop_words)
Dickens_fake_count %>%
with(wordcloud(word, n, max.words=40,
colors=rainbow(40),
scale=c(2.2, 0.5), rot.per=0.3))
Dickens_fake_AFINN <- inner_join(Dickens_fake_raw, AFINN )
Dickens_fake_AFINN <- Dickens_fake_AFINN %>%
mutate(sentiments=cumsum(value)) %>%
mutate(type="fake")
Dickens_original_AFINN <- inner_join(Dickens_original_raw, AFINN )
Dickens_original_AFINN <- Dickens_original_AFINN %>%
mutate(sentiments=cumsum(value)) %>%
mutate(type="original")
Dickens_AFINN<- bind_rows(Dickens_fake_AFINN, Dickens_original_AFINN)
Dickens_AFINN <- Dickens_AFINN %>%
group_by(type) %>%
mutate(index = row_number())
ggplot(Dickens_AFINN , aes(x=index , y=value , color=type)) +
geom_smooth(method = "loess", se = FALSE) +
facet_wrap(~type, scales = "free_x")
head(Dickens_fake_AFINN)
head(Dickens_original_AFINN)
head(Dickens_AFINN)
dim(Dickens_AFINN)
data(stop_words)
combined_raw <- bind_rows(
Dickens_fake_raw %>% mutate(type = "Fake"),
Dickens_original_raw %>% mutate(type = "Original")
) %>%
anti_join(stop_words, by = "word")  # Remove stop words here
combined_dtm <- combined_raw %>%
count(type, word) %>%
cast_dtm(type, word, n)
Dickens_LDA <- LDA(combined_dtm, k = 2, method = "Gibbs", control = list(seed = 123))
tidy(Dickens_LDA, matrix = "beta") %>%
mutate(topic = case_when(topic == 1 ~ "Topic 1", topic == 2 ~ "Topic 2")) %>%
pivot_wider(names_from = topic, values_from = beta, values_fill = 0) %>%
ggplot(aes(x = `Topic 1`, y = `Topic 2`)) +
geom_point() +
geom_text(aes(label = term), check_overlap = TRUE, vjust = 1) +
coord_trans(x = "sqrt", y = "sqrt") +
theme_bw() +
labs(x = "Term Frequency in Topic 1", y = "Term Frequency in Topic 2",
title ="Most frequent words in Topic 1 and Topic 2 ")
dim(combined_dtm)
predictMarch<- predictAugust <- c()
coord <- cbind( Locations$Longitude, Locations$Latitude )
for( j in 1:length(pixels[,1]) ){
predictMarch[j] <- IDW( X=WaterMarch$Average, S=coord, s_star=pixels[j,], p=2.3 )
predictAugust[j] <- IDW( X=WaterAugust$Average, S=coord, s_star=pixels[j,], p=2.3 )
}
predictMarch<- predictAugust <- c()
coord <- cbind( Locations$Longitude, Locations$Latitude )
for( j in 1:length(pixels[,1]) ){
predictMarch[j] <- IDW( X=WaterMarch$Average, S=coord, s_star=pixels[j,], p=1 )
predictAugust[j] <- IDW( X=WaterAugust$Average, S=coord, s_star=pixels[j,], p=1 )
}
min(predictMarc)
min(predictMarch)
min(predictAugust)
max(predictMarch)
max(WaterMarch$Average)
min(WaterMarch$Average)
knitr::opts_chunk$set(echo = TRUE)
library( dplyr )
library( ggplot2 )
library( tidytext )
library( wordcloud )
library( stringr )
library( tidyr )
library( sf )
library( ggspatial )
library( prettymapr )
library( lubridate )
library( sp )
library( gstat )
library( spatstat )
library( tm )
library( topicmodels )
library( reshape2 )
library(patchwork)
AFINN <- read.csv("AFINN Sentiment Lexicon.csv")
Bing <- read.csv("Bing Sentiment Lexicon.csv")
Dickens_Fakes_Raw <- read.csv("Dickens_Fakes.csv")
Dickens_Originals_Raw <- read.csv("Dickens_Originals.csv")
Tesremos_shape <- read_sf( "Tesremos_Shapefile" )
Tesremos_Locations <- read.csv( "Tesremos_Locations.csv" )
Tesremos_Water <- read.csv("Tesremos_Water.csv")
Tesremos_Water_Long <- Tesremos_Water %>%
pivot_longer(
cols = -Date,
names_to = "ID",
values_to = "observations"
)
mean( is.na( Tesremos_Water_Long$observations ) )
Tesremos_Data <- Tesremos_Water_Long %>%
left_join(Tesremos_Locations, by="ID")
Tesremos_Data <- Tesremos_Data %>%
group_by( month(Date), ID ) %>%
mutate( average_month = mean(observations))
ggplot( Tesremos_shape ) + geom_sf() + theme_bw() +
geom_point( data=Tesremos_Data, aes(x=Longitude, y=Latitude, color=average_month ) ) +
facet_wrap(~month(Date)) +
scale_color_distiller( palette="Blues", trans="reverse" ) +
labs( x="Longitude", y="Latitude", color="Average observation per month" ) +
theme(axix.text.x = element_text(size=3))
Tesremos_Data_gamma <- Tesremos_Data
coordinates(Tesremos_Data_gamma) <- ~ Longitude + Latitude
gamma_hat <- variogram( observations~1, Tesremos_Data_gamma, cutoff=0.14  )
ggplot( gamma_hat, aes( x=dist, y=gamma/2 ) ) + geom_point( size=2 ) +
theme_bw() + labs( x="Distance", y="Semi-variogram" )
gamma_hat
dim(Tesremos_Data)
dim(Tesremos_Data_gamma)
head(Tesremos_Data_gamma)
0.4 * 33 + 0.6 * 45
0.4 * 33 + 0.6 * 46
0.4 * 76 + 0.6 * 65
0.4 * 76 + 0.6 * 66
