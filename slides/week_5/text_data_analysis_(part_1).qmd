---
title: "MA22019 - Text Data Analysis (Part 1)"
author: "Christian Rohrbeck"
date: "5 March 2025"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, warning=FALSE, message=FALSE}
colorize <- function(x, color="blue") {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
library(dplyr)
```

## Plan for Today

We start Chapter 3 on **Text Data Analysis**:

- What is text data and why is it important?

- Analysis of word frequency (Section 3.1)

- Sentiment Analysis (Section 3.2)

- Comparing term frequencies (Section 3.3.1)


## Summary of Week 4 Feedback

* 19 submissions (out of 165)

* Majority of students seems happy with how the course is run

* Concerns that were raised by more than one student:

  + Vagueness and lack of context in the coursework

  + Lack of complex examples and a fully written report

  + Amount of questions on the problem sheets


# Why analyse texts?

## What is text data? 

![](textdata.png){#id .class width=100% height=100%}

## Applications 


Text data is collected widely, for instance, for

* Optimizing search engines 

* Crime prevention

* Customer service


In all these applications, we are interested in 

* The words used within the text

* The intention of the text


## Loading text data from a .txt file

One way to store text data is as .txt file.

The file "Hesse quote.txt" contains a short quote that we load with **readLines()**

```{r, echo=TRUE}
text_Hesse <- readLines( "hesse_quote.txt" )
text_Hesse
```

`r colorize("Is this data in a useful format?")`

# Extracting text data

## Transforming text data

Let's define a data frame
```{r, echo=TRUE}
quote_Hesse <- data.frame( line=1:2, text=text_Hesse )
quote_Hesse
```

`r colorize("Does this help?")`


## The tidy text format I 


We focus on analysing the individual words in a text.



As such, 

- each word is an individual observation

- we do not care about punctuation




We use the **unnest_tokens()** function in the tidytext package to extract the individual words
```{r, warning=FALSE, echo=TRUE}
library(tidytext)
Hesse_tidy <- quote_Hesse %>% unnest_tokens( word, text )
```


## The tidy text format II

```{r, echo=TRUE}
head( Hesse_tidy )
```

The **`r colorize("tidy text")`** format propagates:

- Each variable is a column: our variable of interest is **word**

- Each observation is a row: we have one word per row


# Analysis of *Jane Eyre*

## Charlotte Brontë

```{r, echo=FALSE, fig.align='center', out.width = '70%'}
knitr::include_graphics("CharlotteBronte.jfif")
```

*Jane Eyre* is a novel that was published in 1847.


## Loading the data 


We download the full text from Project Gutenberg.</br> 

**`r colorize("-> R Markdown file")`**



The book has about 21,000 lines of text, and we separate them into individual words:

```{r, echo=T, eval=FALSE}
JaneEyre <- JaneEyre_raw %>% unnest_tokens( word, text )
```

There is some data cleaning to be done! </br> 

**`r colorize("-> R Markdown file")`**


## Calcualting term frequency

Exploring the frequency of the different words in a text is a common approach in text data analysis.

We use functions in the dplyr R package for this

```{r, eval=FALSE, echo=TRUE}
library(dplyr)
JaneEyre_Count <- JaneEyre %>% 
  count( word, sort=TRUE ) %>%
  mutate( 'term frequency'=n/sum(n), rank=row_number() )
slice_head( JaneEyre_Count, n=10 )
```

**`r colorize("-> R Markdown file")`**

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(tidytext)
library(dplyr)
URL <- "https://www.gutenberg.org/cache/epub/1260/pg1260.txt"
JaneEyre_raw <- readLines( URL, encoding = "UTF-8" )
ind <- grep( "EBOOK", JaneEyre_raw )
JaneEyre_raw<- data.frame( text=JaneEyre_raw[ (ind[1]+1):(ind[2]-1) ] )
JaneEyre <- JaneEyre_raw %>% unnest_tokens( word, text )
JaneEyre$word <- gsub( "_", "", JaneEyre$word )
JaneEyre_Count <- JaneEyre %>% 
  count( word, sort=TRUE ) %>%
  mutate( 'term frequency' = n / sum(n), rank = row_number() )
```

## Illustration

Let's create a plot of term frequency versus rank on log scale

```{r, warning=FALSE, message=FALSE, fig.align='center', out.width='60%'}
library(ggplot2)
ggplot( JaneEyre_Count, aes( x=rank, y=`term frequency` ) ) + 
  geom_line(size=1.5) + coord_trans( x="log10", y="log10" ) + 
  theme( axis.text=element_text(size=15), 
         axis.title=element_text(size=16))
```

`r colorize("What do we notice?")`


## Zipf's Law

- Relationship between rank and term frequency on logarithmic scale is close to linear 

- The non-linear shape in the top-left is driven by only six words. 

- **`r colorize("Zipf's Law")`** states that empirically a word’s frequency is inversely proportional to its rank.

## Stop words 


The words "the", "I" and "and" appear most often in *Jane Eyre*.

`r colorize("Is this information useful?")`



It is quite common in text data analysis to specify **stop words** which are ignored in the analysis.



The tidytext package comes with its own list of stop words:
```{r, echo=TRUE}
library( tidytext )
data( stop_words )
```

**`r colorize("-> R Markdown file")`**


## Visualizing word frequency 


There are two options for visualizing the most frequent words:

1) `r colorize("Bar plots")` - good to display the actual counts

2) `r colorize("Word clouds")` - good for many words



Let's produce these plots for *Jane Eyre*</br>

**`r colorize("-> R Markdown file")`**



# Sentiment Analysis

## What do we mean by sentiment?

The emotional intent of the text:

- Is a text more positive or negative?

- Is a review positive or negative?

- How does the story evolve throughout the book?

## Measuring sentiment 


We start by assigning a sentiment to each individual word using a **`r colorize("sentiment lexicon")`**.

The following two lexicons are provided by tidytext:

* **AFINN**: Sentiment score between -5 and +5.

* **Bing**: Words are categorized as "positive" or "negative".

The sentiment of a text is then the sum (or mean) of the sentiment of the words.


**`r colorize("Is this a weak, moderate or strong assumption?")`**

**`r colorize("For which pieces of text may our approach perform poorly?")`**



## Sentiment analysis for *Jane Eyre*

We already analysed word frequency for the book.

Let's see what we can say about the sentiment.</br>

**`r colorize("-> R Markdown file")`**


## What to remember

Sentiment analysis assesses whether a text has a positive or negative emotional intent.

We measure sentiment via the individual words. </br> **`r colorize("We make an assumption here!")`**

**Important:** Stop words are not removed, because they may be important.

**Remark:** The relative change in sentiment is often more accurate than the actual values.


# Comparing text documents

## Motivation 

In many applications we are interested in comparing two (or more) pieces of text.

`r colorize("Can you think of examples?")`

Using the methods we introduced, we may explore two aspects:

* Word frequency

* Sentiment

## Comparison using term frequency 


Interest in exploring two pieces of text in terms of term frequency of the individual words.



`r colorize( "Why should we not compare the actual counts?" )`



Let's compare the novels *Jane Eyre* and *Wuthering Heights*</br>

**`r colorize("-> R Markdown file")`**


