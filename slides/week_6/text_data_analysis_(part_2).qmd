---
title: "MA22019 - Text Data Analysis (Part 2)"
author: "Christian Rohrbeck"
date: "12 March 2025"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, warning=FALSE, message=FALSE}
colorize <- function(x, color="blue") {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
library(dplyr)
```

## Plan for Today

We complete the chapter on text data analysis:

* Term frequency - inverse document frequency (Section 3.3.2)

* Problem Class 4

* Topic modelling (Section 3.4)

## Motivation 


We learned last week that a text can be analysed based on

* Frequency of words within it

* Its emotional intent

These aspects can also be considered when comparing texts.



**However:** This may not be the best way to:

* Identify the key words that best describe a text

* Group a large number of texts into subgroups 

Today we will address these aspects.



# Term frequency - inverse document frequency

## Motivation 


Suppose $D$ is a large number of documents / texts / websites, also referred to as a **corpus**.

**`r colorize("How do we decide which words are specific to a text?")`**



Such information is useful for 

- Designing search engines

- Document classification

- Text summarization



## tf and idf 

We have two components:

* **term frequency** of the word $t$ in the document $d$
\[
\mathrm{tf}(t,d)=\frac{\mathrm{Counts~of}~t~\mathrm{within}~d}{\mathrm{Number~of~words~within}~d}
\]

* **inverse document frequency** of $t$ across $D$
\[
\mathrm{idf}(t,D) = 
\log \left(\frac{\mathrm{Number~of~documents}}{\mathrm{Number~of~documents~with}~t}\right).
\]

## tf-idf 


The `r colorize("term frequency - inverse document frequency")` is then defined as
\[
\mathrm{tf.idf}(t,d,D) = \mathrm{tf}(t,d) \times \mathrm{idf}(t,D).
\]

**`r colorize("What are the properties of tf.idf?")`**


- $\mathrm{tf.idf}(t,d,D)=0$ if $t$ occurs in all documents

- $\mathrm{tf.idf}(t,d,D)$ is large if $t$ occurs very often and only in one document

Whether stop words should be removed or not depends on the application.




## Example - Books by Charles Dickens

Let's look at a corpus containing only four books

* *A Christmas Carol*

* *A Tale of Two Cities*

* *Great Expectations* 

* *Oliver Twist*

`r colorize("Which words would we expect to have the highest tf-idf?")`

**`r colorize("-> R Markdown file")`**

# Problem Class 4

## Overview

We want to illustrate the techniques covered so far by analyzing the books by Jane Austen:

* *Emma*

* *Mansfield Park*  

* *Northanger Abbey*

* *Persuasion*

* *Pride and Prejudice*  

* *Sense and Sensibility* 

**`r colorize("-> R Markdown file")`**

# Topic modelling

## Motivation - How do write a text? 


Consider the following aspects:

1) Will the words we use depend on the topic we want to write about?

2) Can we have several topics in a (longer) piece of text?

`r colorize("How can we adjust for such aspects in a model?")`




## Framework and Principles


We now have a corpus $D$ 

* $N$ documents and

* $M$ unique words across all documents.

The topics nor the term frequency with which words appear within a topic are known to us.



Our framework should allow for the following principles: 

* **Every document is a mixture of topics.**

* **Every topic is a mixture of words.**



## Latent Dirichlet Allocation (LDA) 

Introduced in the early 2000s, the key aspects are:

1) We specify the number $K$ of topics.

2) The proportions $(\psi_{i,1},\ldots,\psi_{i,K})$ describe how much each topic features in document $i$.  

3) The proportions $\left(\theta_{k,1},\ldots,\theta_{k,M}\right)$ describe the term frequency (distribution of words) for a text from topic $k$.

The proportions are estimated using the **topicmodels** package, and we should remove the stop words. 


## Using the topicmodels package

There are few steps that need to be performed:

1) Construct the document term matrix $A\in\mathbb{R}^{N\times M}$, where $A_{i,j}$ denotes the number of counts of word $j$ in document $i$

2) Fit the LDA model with a fixed value for $K$

3) Analyse the proportions:

    a) $(\psi_{i,1},\ldots,\psi_{i,K}),~i=1,\ldots,N$ 
    
    b) $\left(\theta_{k,1},\ldots,\theta_{k,M}\right),~k=1,\ldots,K$



## Examples

Let's look at two examples:

1) Recovering the books by Charles Dickens

    **`r colorize("-> R Markdown file")`**

2) Classifying news articles by the New York Times

    **`r colorize("-> R Markdown file")`**


## Summary

We learned how to:

* Identify the most common words within a text

* Analyse the sentiment of a text

When dealing with several a corpus, we can:

* Make comparisons between texts

* Identify the words that best represent each text (if-idf)

* Estimate a topic model using Latent Dirichlet allocation

## Remark

The methods we introduced are still popular, but they are slowly being replaced by deep learning and transformer-based models in practice:

* Bidirectional Encoder Representations from Transformers (Bert)

* Neural Topic Models

* Word2Vec and Top2Vec

Nevertheless, for the purposes of an initial analysis, the methods we introduced do a good job.
