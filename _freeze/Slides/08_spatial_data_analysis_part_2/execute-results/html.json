{
  "hash": "39d52880e32521355f4931533d912367",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Spatial Data Analysis (Part 2)\"\nsubtitle: \"Lecture\"\ndate: \"26 March 2025\"\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolorize <- function(x, color = \"blue\") {\n    if (knitr::is_latex_output()) {\n        sprintf(\"\\\\textcolor{%s}{%s}\", color, x)\n    } else if (knitr::is_html_output()) {\n        sprintf(\n            \"<span style='color: %s;'>%s</span>\", color,\n            x\n        )\n    } else {\n        x\n    }\n}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\n```\n:::\n\n\n## Plan for Today\n\nWe introduce two more techniques for analysing point-referenced data:\n\n* Semi-variogram (Section 4.3)\n\n* Principal component analysis (Section 4.4)\n\nThe two other types of spatial data will be considered in more detail in Week 9.\n\n# Spatial Dependence\n\n## Motivation\n\nIn terms of analyzing point-referenced data, we explored \n\n- Visualization\n\n- Inverse distance weighting\n\nThe latter makes certain assumptions to predict values at unobserved locations.\n\n**<span style='color: blue;'>Which assumption may we make?</span>**\n\n\n## Sea Surface Temperature Anomalies\n\nLet's consider a data set similar to Problem Class 5:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08_spatial_data_analysis_part_2_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=65%}\n:::\n:::\n\n\n**What do we observe?**\n\n## Spatial dependence\n\nLocations which are spatially close appear to have similar values.\n\nThis suggests that observations are **dependent**!\n\nToday we introduce two techniques for exploring this **<span style='color: blue;'>spatial dependence</span>.**\n\n# The (semi-)variogram\n\n## Mathematical definition \n\n\nLet $X(\\mathbf{s})$ denote random variable at $\\mathbf{s}\\in\\mathcal{S}$. \n\nWe assume that $E[X(\\mathbf{s})]$ is constant.\n\n</br> **<span style='color: blue;'>Is this reasonable for our data?</span>**\n\n\n\nWe measure dependence between $X(\\mathbf{s})$ and $X(\\tilde{\\mathbf{s}})$ via the **semi-variogram**\n\n$$\n\\gamma(\\mathbf{s},\\tilde{\\mathbf{s}}) = \\frac{1}{2} \\mathrm{E}\\left[\\{X(\\mathbf{s})-X(\\tilde{\\mathbf{s}})\\}^2\\right].\n$$\n\n\n\n## Properties\n\nThe semi-variogram satisfies \n\n- $\\gamma(\\mathbf{s},\\tilde{\\mathbf{s}})\\geq 0$ and $\\gamma(\\mathbf{s},\\mathbf{s})=0$.\n\n- For $X(\\mathbf{s})$ and $X(\\tilde{\\mathbf{s}})$ i.i.d., $\\gamma(\\mathbf{s},\\tilde{\\mathbf{s}})=\\mathrm{Var}[X(\\mathbf{s})].$\n\n\n## Estimation I \n\n\n**Why can we not easily estimate $\\gamma(\\mathbf{s},\\tilde{\\mathbf{s}})$?**\n\n\n\nTo estimate $\\gamma(\\mathbf{s},\\tilde{\\mathbf{s}})$, we assume that the spatial random process is $X(\\mathbf{s})$ **stationary** and **isotropic**.\n  \n\n\nAs such there exists $\\tilde{\\gamma}:\\mathbb{R}_+\\to\\mathbb{R}_+$ with\n\n$$\n\\gamma(\\mathbf{s} , \\tilde{\\mathbf{s}}) = \\tilde{\\gamma}(||\\mathbf{s}-\\tilde{\\mathbf{s}}||),\n$$\n\nwhere $||\\mathbf{s}-\\tilde{\\mathbf{s}}||$ denotes the distance between the sites.\n\n\n## Estimation II\n\nIf our assumptions hold, we can estimate $\\tilde{\\gamma}(h)$, $h>0$, as follows:\n\n1. Find all pairs of sites with a distance similar to $h$. This gives the set $\\mathcal{N}_h =\\{(i,j):||\\mathbf{s}_i - \\mathbf{s}_j||\\approx h\\}$.    \n\n2. Calculate the estimate for $\\tilde\\gamma(h)$ as\n\n$$\n\\hat\\gamma(h) = \\frac{1}{2|\\mathcal{N}_h|} \\sum_{i,j \\in\\mathcal{N}_h} (x_i-x_j)^2.\n$$\n\nWe use the function **variogram()** in the **gstat** R package for this. \n\n**Remark**: We have to convert the data into a specific format.\n\n## What are we looking for?\n\nIn many applications we find the following features:\n\n1. **$\\hat\\gamma(h)$ increases with increasing distance.**</br> This suggests that spatial dependence becomes weaker the further sites are apart.\n\n2. **$\\hat\\gamma(h)$ levels off at a certain distance.** </br> This marks the point when sites are so far apart that the realizations are close to independent.\n\n## Example\n\nLet's look at the example on sea surface temperature anomalies\n\n**<span style='color: blue;'>-> Quarto Document</span>**\n\n## Summary\n\nTo analyse spatial dependence using the semi-variogram, we need to \n\n1. Consider whether it is reasonable to assume that dependence is fully described by spatial distance and that all sites have the same mean\n\n2. Estimate the semi-variogram using the **sp** and **gstat** package\n\n3. Interpret the estimated semi-variogram\n\nYou will learn more about the mathematical details in Year 3 Statistics.\n\n# Principal Component Analysis (PCA)\n\n## Motivation \n\n\nLet's assume we have multiple observations per spatial location.\n\nWe use $x_{i,t}$ to denote the $t$-th observation for site $i$.\n\n**<span style='color: blue;'>How could we visualize such data?</span>**\n\n\n\nIn many applications we want to \n\n* Understand the spatial structure of the data\n\n* Perform dimension reduction\n\n\n## Motivation - Data for two sites\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08_spatial_data_analysis_part_2_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n## Can we reduce it to a single variable?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08_spatial_data_analysis_part_2_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n## How does PCA work?\n\n1. Calculate $\\tilde{x}_{i,t} = (x_{i,t} - \\bar{x}_i)/\\hat\\sigma_i$.\n\n2. Derive the matrix\n\n$$\n\\Sigma = \\frac{1}{T-1}\\sum_{t=1}^T \\tilde{\\mathbf{x}}_t \\tilde{\\mathbf{x}}_t^{\\mathrm{T}}.\n$$\n\nThis matrix is also known as the **empirical covariance matrix**.\n\n3. Derive and study the eigenvalues and eigenvectors of $\\Sigma$, $\\Sigma = \\mathbf{UDU}^{\\mathrm{T}}$. \n\n## Analysis of the eigenvectors \n\n\nEach eigenvector provides insight on the spatial structure in the data.\n\n-> We create plots of the eigenvectors and interpret them sequentially\n\n**<span style='color: blue;'>Do we need to consider all eigenvectors?</span>**\n\n\nNo (usually), the eigenvalues will help us with this.\n \n## Analysis of the eigenvalues\n\n\nWe have eigenvalues $\\lambda_1 > \\lambda_2 > \\cdots > \\lambda_n$\n\nConsider the ratio\n\n$$\n\\frac{\\sum_{j=1}^m \\lambda_j}{\\sum_{j=1}^n \\lambda_j}.\n$$\n\nThe smallest $m$ for which the ratio exceeds 0.9 gives the number of eigenvectors we should plot.\n\n\nThis is one of several rules-of-thumb, but you only need to know this one.\n\n## Example\n\nLet's analyse rainfall data for sites across the US state Colorado.\n\nWe have\n\n* $n=30$ cities\n\n* Monthly amount of precipitation across 2010-2023. So we have $T=168$ observations per site. \n\n**<span style='color: blue;'>-> Quarto Document</span>**\n\n\n\n\n## Summary\n\nWhile PCA may seem like a black box, it is very powerful method and has many applications beyond spatial data analysis.\n\nThe key steps are\n\n1. Apply the prcomp() function to the matrix of observations\n\n2. Study the eigenvalues to determine how many eienvectors need to be considered\n\n3. Visualize the eigenvectors and make conclusions about the spatial structure in the data\n\nWhen does PCA work? -> **Linearity of variables**\n\n\n## Looking ahead\n\n* We will conclude the content next week (Week 9).\n\n* There will be a problem sheet which will be considered in the tutorial after Easter (Tuesday in Week 10).\n\n* The lecture in Week 10 will be a revision lecture.</br> Look out for an announcement on Moodle!\n\n* Coursework 2 will be released on the Friday in Week 10\n\n\n\n\n\n\n\n",
    "supporting": [
      "08_spatial_data_analysis_part_2_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}