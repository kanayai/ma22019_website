{
  "hash": "008bc129aeddde0a5f30546f1fe898b8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Text Data Analysis (Part 2)\"\nsubtitle: \"Lecture\"\ndate: \"12 March 2025\"\n---\n\n\n\n\n::: {.cell}\n\n:::\n\n\n## Plan for Today\n\nWe complete the chapter on text data analysis:\n\n* Term frequency - inverse document frequency (Section 3.3.2)\n\n* Problem Class 4\n\n* Topic modelling (Section 3.4)\n\n## Motivation \n\n\nWe learned last week that a text can be analysed based on\n\n* Frequency of words within it\n\n* Its emotional intent\n\nThese aspects can also be considered when comparing texts.\n\n\n\n**However:** This may not be the best way to:\n\n* Identify the key words that best describe a text\n\n* Group a large number of texts into subgroups \n\nToday we will address these aspects.\n\n\n\n# Term frequency - inverse document frequency\n\n## Motivation \n\n\nSuppose $D$ is a large number of documents / texts / websites, also referred to as a **corpus**.\n\n**<span style='color: blue;'>How do we decide which words are specific to a text?</span>**\n\n\n\nSuch information is useful for \n\n- Designing search engines\n\n- Document classification\n\n- Text summarization\n\n\n\n## tf and idf \n\nWe have two components:\n\n* **term frequency** of the word $t$ in the document $d$ .\n\n$$\n\\text{tf}(t,d)=\\frac{\\text{Counts~of}~t~\\text{within}~d}{\\text{Number~of~words~within}~d}\n$$\n\n* **inverse document frequency** of $t$ across $D$\n\n$$\n\\text{idf}(t,D) = \n\\log \\left(\\frac{\\text{Number~of~documents}}{\\text{Number~of~documents~with}~t}\\right).\n$$\n\n## tf-idf \n\n\nThe <span style='color: blue;'>term frequency - inverse document frequency</span> is then defined as\n\n$$\n\\text{tf.idf}(t,d,D) = \\text{tf}(t,d) \\times \\text{idf}(t,D).\n$$ \n\n**<span style='color: blue;'>What are the properties of tf.idf?</span>**\n\n\n- $\\text{tf.idf}(t,d,D)=0$ if $t$ occurs in all documents\n\n- $\\text{tf.idf}(t,d,D)$ is large if $t$ occurs very often and only in one document\n\nWhether stop words should be removed or not depends on the application.\n\n\n\n\n## Example - Books by Charles Dickens\n\nLet's look at a corpus containing only four books\n\n* *A Christmas Carol*\n\n* *A Tale of Two Cities*\n\n* *Great Expectations* \n\n* *Oliver Twist*\n\n<span style='color: blue;'>Which words would we expect to have the highest tf-idf?</span>\n\n**<span style='color: blue;'>-> Quarto Document</span>**\n\n# Problem Class 4\n\n## Overview\n\nWe want to illustrate the techniques covered so far by analyzing the books by Jane Austen:\n\n* *Emma*\n\n* *Mansfield Park*  \n\n* *Northanger Abbey*\n\n* *Persuasion*\n\n* *Pride and Prejudice*  \n\n* *Sense and Sensibility* \n\n**<span style='color: blue;'>-> Quarto Document</span>**\n\n# Topic modelling\n\n## Motivation - How do write a text? \n\n\nConsider the following aspects:\n\n1) Will the words we use depend on the topic we want to write about?\n\n2) Can we have several topics in a (longer) piece of text?\n\n<span style='color: blue;'>How can we adjust for such aspects in a model?</span>\n\n\n\n\n## Framework and Principles\n\n\nWe now have a corpus $D$ \n\n* $N$ documents and\n\n* $M$ unique words across all documents.\n\nThe topics nor the term frequency with which words appear within a topic are known to us.\n\n\n\nOur framework should allow for the following principles: \n\n* **Every document is a mixture of topics.**\n\n* **Every topic is a mixture of words.**\n\n\n\n## Latent Dirichlet Allocation (LDA) \n\nIntroduced in the early 2000s, the key aspects are:\n\n1) We specify the number $K$ of topics.\n\n2) The proportions $(\\psi_{i,1},\\ldots,\\psi_{i,K})$ describe how much each topic features in document $i$.  \n\n3) The proportions $\\left(\\theta_{k,1},\\ldots,\\theta_{k,M}\\right)$ describe the term frequency (distribution of words) for a text from topic $k$.\n\nThe proportions are estimated using the **topicmodels** package, and we should remove the stop words. \n\n\n## Using the topicmodels package\n\nThere are few steps that need to be performed:\n\n1) Construct the document term matrix $A\\in\\mathbb{R}^{N\\times M}$, where $A_{i,j}$ denotes the number of counts of word $j$ in document $i$\n\n2) Fit the LDA model with a fixed value for $K$\n\n3) Analyse the proportions:\n\n    a) $(\\psi_{i,1},\\ldots,\\psi_{i,K}),~i=1,\\ldots,N$ \n    \n    b) $\\left(\\theta_{k,1},\\ldots,\\theta_{k,M}\\right),~k=1,\\ldots,K$\n\n\n\n## Examples\n\nLet's look at two examples:\n\n1) Recovering the books by Charles Dickens\n\n    **<span style='color: blue;'>-> Quarto Document</span>**\n\n2) Classifying news articles by the New York Times\n\n    **<span style='color: blue;'>-> Quarto Document</span>**\n\n\n## Summary\n\nWe learned how to:\n\n* Identify the most common words within a text\n\n* Analyse the sentiment of a text\n\nWhen dealing with several a corpus, we can:\n\n* Make comparisons between texts\n\n* Identify the words that best represent each text (if-idf)\n\n* Estimate a topic model using Latent Dirichlet allocation\n\n## Remark\n\nThe methods we introduced are still popular, but they are slowly being replaced by deep learning and transformer-based models in practice:\n\n* Bidirectional Encoder Representations from Transformers (Bert)\n\n* Neural Topic Models\n\n* Word2Vec and Top2Vec\n\nNevertheless, for the purposes of an initial analysis, the methods we introduced do a good job.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}