{
  "hash": "5f18aa95477905e12861b4862d1bebe5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Analysis of Jane Eyre\"\nauthor: \"Christian Rohrbeck\"\ndate: '05/03/2025'\nformat: html\n---\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidytext)\nlibrary(wordcloud)\nlibrary(stringr)\n```\n:::\n\n\n### **Loading and cleaning the text data**\n\nWe can either download the text from Project Gutenberg (see lecture notes), or directly via the text file provided on Moodle\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJaneEyre_raw <- readLines( \"data/jane_eyre.txt\" )\nJaneEyre_raw <- data.frame( text = JaneEyre_raw )\n```\n:::\n\n\nLet's split the text into words:\n\n::: {.cell}\n\n```{.r .cell-code}\nJaneEyre <- JaneEyre_raw %>% unnest_tokens( word, text )\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnrow(JaneEyre)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 188417\n```\n\n\n:::\n:::\n\n\nWe have to do some data cleaning. Let's look at the first 10 words\n\n::: {.cell}\n\n```{.r .cell-code}\nslice_head( JaneEyre, n=10 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            word\n1           jane\n2           eyre\n3             an\n4  autobiography\n5             by\n6      charlotte\n7  bront<U+00EB>\n8   _illustrated\n9             by\n10             f\n```\n\n\n:::\n:::\n\n\nWe remove the underscores from any words, as they are used to indicate words in italics\n\n::: {.cell}\n\n```{.r .cell-code}\nJaneEyre <- JaneEyre %>% mutate( word = gsub( \"\\\\_\", \"\", word ) )\n```\n:::\n\n\n\n## **Word frequency analysis**\n\n### Calculating term frequency\n\nWe now count how often each word appears in the book:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJaneEyre_Count <- JaneEyre %>% count( word, sort=TRUE )\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslice_head(JaneEyre_Count, n=10 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   word    n\n1   the 7856\n2     i 7172\n3   and 6632\n4    to 5238\n5     a 4475\n6    of 4368\n7   you 2970\n8    in 2769\n9   was 2526\n10   it 2406\n```\n\n\n:::\n:::\n\n\n\nOne important measure is *term frequency*:\n\n::: {.cell}\n\n```{.r .cell-code}\nJaneEyre_Count <- JaneEyre_Count %>%\n  mutate( 'term frequency'=n/sum(n), rank=row_number() ) %>%\n  rename( Count = n )\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslice_head(JaneEyre_Count, n=10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   word Count term frequency rank\n1   the  7856     0.04169475    1\n2     i  7172     0.03806451    2\n3   and  6632     0.03519852    3\n4    to  5238     0.02780004    4\n5     a  4475     0.02375051    5\n6    of  4368     0.02318262    6\n7   you  2970     0.01576291    7\n8    in  2769     0.01469613    8\n9   was  2526     0.01340643    9\n10   it  2406     0.01276955   10\n```\n\n\n:::\n:::\n\n\n\n\n### Does Zipf's Law hold?\n\nPlot rank against term frequency (both on logarithmic scale):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot( JaneEyre_Count, aes( x=rank, y=`term frequency` ) ) + \n  geom_line( linewidth=1.5 ) + \n  coord_trans( x=\"log10\", y=\"log10\" )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `coord_trans()` was deprecated in ggplot2 4.0.0.\ni Please use `coord_transform()` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](analysis_of_jane_eyre_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=40%}\n:::\n:::\n\n\nWe see a linear relationship in the plot, which supports Zipf's Law that postulates an inverse proportionality between rank and term frequency.\n\n### Removing stop words\n\nWe use the data set **stop_words** in the tidytext package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"stop_words\")\nJaneEyre_Count <- JaneEyre_Count %>% \n  anti_join(stop_words, by=\"word\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslice_head(JaneEyre_Count, n=10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        word Count term frequency rank\n1       jane   341   0.0018098155   69\n2  rochester   317   0.0016824384   71\n3        sir   316   0.0016771310   72\n4       miss   310   0.0016452868   73\n5       time   244   0.0012949999   98\n6        day   232   0.0012313114  102\n7     looked   221   0.0011729303  106\n8      night   218   0.0011570081  109\n9       eyes   187   0.0009924794  120\n10      john   184   0.0009765573  125\n```\n\n\n:::\n:::\n\n\n### Visualization\n\n**Option 1 Bar plot:** we order the words based on frequency\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nJaneEyre_Count %>%\n  slice_max( Count, n=10 ) %>% \n  mutate( word = reorder(word,Count) ) %>%\n  ggplot( aes( x=Count, y=word ) ) + \n    geom_col() + labs( x=\"Count\", y=\"\" )\n```\n\n::: {.cell-output-display}\n![](analysis_of_jane_eyre_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=50%}\n:::\n:::\n\n\n\n**Option 2 Word cloud:**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nJaneEyre_Count %>%\n  with( wordcloud( word, Count, max.words=40, colors=topo.colors(n=40) ) )\n```\n\n::: {.cell-output-display}\n![](analysis_of_jane_eyre_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=65%}\n:::\n:::\n\n\n## **Sentiment Analysis for** *Jane Eyre*\n\n### Start with the raw data and split it into words\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJaneEyre <- JaneEyre_raw %>%\n  mutate( line = row_number() ) %>%\n  unnest_tokens( word, text ) %>%\n  mutate( word = gsub( \"_\", \"\", word ) )\n```\n:::\n\n\n### Load the sentiment lexicon\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAFINN <- read.csv(\"data/afinn_sentiment_lexicon.csv\" )\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslice_head(AFINN, n=5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       word value\n1   abandon    -2\n2 abandoned    -2\n3  abandons    -2\n4  abducted    -2\n5 abduction    -2\n```\n\n\n:::\n:::\n\n\n\n### Only keep the words both in the book and the sentiment lexicon\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJaneEyre_AFINN <- inner_join( JaneEyre, AFINN, by=\"word\" )\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslice_head(JaneEyre_AFINN, n=5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  line    word value\n1   33 demands    -1\n2   36  thanks     2\n3   41    fair     2\n4   41  honest     2\n5   48   vague    -2\n```\n\n\n:::\n:::\n\n\n\n### Sentiment across lines\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nJaneEyre_AFINN %>% \n  filter( word != \"miss\" ) %>%\n  mutate( sentiment = cumsum( value ) ) %>%\n  ggplot( aes( x=line, y=sentiment ) ) +\n  geom_line( linewidth=1.2 ) + \n  labs( x=\"Line number\", y=\"Sentiment\" )\n```\n\n::: {.cell-output-display}\n![](analysis_of_jane_eyre_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=50%}\n:::\n:::\n\n\n### Sentiment for the individual chapters \n\nDetect which chapter each line belongs to:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJaneEyre_chapters <- JaneEyre_raw %>%\n  mutate( chapter = cumsum( str_detect(\n    text, regex(\"^chapter \", ignore_case = TRUE)\n  ) ) )\n```\n:::\n\n\nRemove any text before Chapter 1 and split the text into words:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJaneEyre_chapters <- JaneEyre_chapters %>%\n  filter( chapter > 0 ) %>%\n  unnest_tokens( word, text ) %>%\n  mutate( word = gsub( \"_\", \"\", word ) )\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslice_head(JaneEyre_chapters, n=5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  chapter    word\n1       1 chapter\n2       1       i\n3       1   there\n4       1     was\n5       1      no\n```\n\n\n:::\n:::\n\n\nWe can now calculate and visualize the aggregated sentiment for each chapter\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nJaneEyre_chapters %>%\n  inner_join( AFINN, by=\"word\" ) %>%\n  group_by( chapter ) %>%\n  summarise( sentiment = mean(value) ) %>%\n  ggplot( aes( x=chapter, y=sentiment ) ) + \n  geom_col() + labs( x=\"Chapter\", y=\"AFINN sentiment score\" )\n```\n\n::: {.cell-output-display}\n![](analysis_of_jane_eyre_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=50%}\n:::\n:::\n\n",
    "supporting": [
      "analysis_of_jane_eyre_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}