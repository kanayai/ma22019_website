{
  "hash": "e95170f51f1384c6af243a999c57a1ae",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"MA22019 2025 - Solutions for Quiz 6\"\nsubtitle: \"Mise en place\"\ncategories: [\"Quiz\", \"Solutions\"]\n---\n\n\n\nYou may want to load the following packages before starting the exercise:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary( dplyr )\nlibrary( ggplot2 )\nlibrary( tidytext )\nlibrary( stringr )\nlibrary( tidyr )\nlibrary( topicmodels )\n```\n:::\n\n\nWhen working on a University PC, you have to first install the tidytext package and any dependencies using\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages( \"tidytext\", dependencies = TRUE )\n```\n:::\n\n\nFor the sentiment analysis you can load the sentiment lexicons using\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAFINN <- read.csv(\"data/AFINN Sentiment Lexicon.csv\")\nBing <- read.csv(\"data/Bing Sentiment Lexicon.csv\")\n```\n:::\n\n\n### **Exercise 1 - Comparing Moby Dick and Robinson Crusoe**\n\nWe want to consider the books *Moby Dick* and *The Life and Adventures of Robinson Crusoe*. The text for both books is provided in the file \"AdventureBooks.csv\" and we load it using \n\n\n::: {.cell}\n\n```{.r .cell-code}\nBooks <- read.csv( \"data/AdventureBooks.csv\" )\n```\n:::\n\n\nConsider the following two questions:\n\na) Which five words (excluding stop words) are the most common in *Moby Dick*?\n\n<p style=\"color:blue\"> *Let's start by extracting the lines from the book Moby Dick:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMobyDick_raw <- filter( Books, title == \"Moby Dick\" )\n```\n:::\n\n\n<p style=\"color:blue\"> *We then combine the different steps from Section 3.1.2 and extract the five most common words(excluding stop words) using the following code:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMobyDick_raw %>% \n  unnest_tokens( word, text ) %>%\n  mutate( word = gsub( \"_\", \"\", word ) ) %>%\n  count( word, sort=TRUE ) %>%\n  anti_join( stop_words, by=\"word\" ) %>%\n  slice_head( n=5 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   word    n\n1 whale 1031\n2  ahab  436\n3   sea  436\n4  ship  429\n5    ye  426\n```\n\n\n:::\n:::\n\n\n<p style=\"color:blue\"> *We find that \"whale\", \"ahab\", \"sea\", \"ship\" and \"ye\" are the five most common words excluding stop words).*</p>\n\nb) When considering a corpus which only includes *Moby Dick* and *The Life and Adventures of Robinson Crusoe*, which five words have the highest term frequency - inverse document frequency (tf-idf)?\n\n<p style=\"color:blue\"> *We combine the different steps from Section 3.3.2 and extract the five words with the highest term frequency -inverse document frequency using the following code:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBooks %>%\n  unnest_tokens( word, text ) %>% \n  mutate( word = gsub( \"_\", \"\", word ) ) %>% \n  count( title, word, sort=TRUE ) %>%\n  bind_tf_idf( word, title, n ) %>%\n  arrange( desc(tf_idf) ) %>%\n  slice_head( n=6 ) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            title   word    n          tf       idf       tf_idf\n1       Moby Dick  whale 1031 0.004851194 0.6931472 0.0033625914\n2       Moby Dick   ahab  436 0.002051523 0.6931472 0.0014220076\n3 Robinson Crusoe friday  183 0.001506111 0.6931472 0.0010439565\n4       Moby Dick whales  246 0.001157511 0.6931472 0.0008023254\n5       Moby Dick  sperm  236 0.001110458 0.6931472 0.0007697105\n6       Moby Dick  stubb  233 0.001096342 0.6931472 0.0007599261\n```\n\n\n:::\n:::\n\n\n<p style=\"color:blue\"> *The output shows that \"whale\", \"ahab\", \"friday\", \"sperm\" and \"stubb\" have the highest term frequency - inverse document frequency. Here we ignored the word \"whales\" because it's the plural of a word with higher tf-idf.*</p>\n\n\n### **Exercise 2 - Analysis of news articles**\n\nIn Section 4.4 we applied topic modelling to articles published in the New York Times. We now study another example. The file “Articles.csv” on Moodle provides the text for 2692 news articles from 2015. The articles were published either in the \"business\" or the \"sports\" category. In this exercise you will first repeat the steps from Section 4.4, and then explore how well your fitted model performs at identifying whether an article belongs to the \"business\" or \"sports\" category.\n\na) Treading each article as a separate document, derive the document term matrix for the set of articles and store it as **Articles_dtm**. \n\n<p style=\"color:blue\"> *We start by loading the text data:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nArticles <- read.csv( \"data/Articles.csv\" )\n```\n:::\n\n\n<p style=\"color:blue\"> *The next step is to create an identifier for each article. To aid the analysis in the next steps, we use the unite() function to create an identifier which includes the news category:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nArticles <- Articles %>% \n  mutate( ID=1:nrow(Articles) ) %>%\n  unite( col=document, NewsType, ID )\n```\n:::\n\n\n<p style=\"color:blue\"> *We are now ready to remove stop words and count the number of occurrences for each word and article*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nArticles_count <- Articles  %>%\n  unnest_tokens( word, Article ) %>%\n  anti_join( stop_words, by=\"word\" ) %>%\n  count( document, word )\n```\n:::\n\n\n<p style=\"color:blue\"> *The last step is create the document term matrix:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nArticles_dtm <- Articles_count %>% cast_dtm( document, word, n )\n```\n:::\n\n\nWith the document term matrix having been derived, we estimate the parameters of an LDA model with $K=2$ topics using:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nArticles_LDA <- LDA( Articles_dtm, k = 2, method=\"Gibbs\", control = list(seed=2024) )\n```\n:::\n\n\nb) For each article, extract the proportions with which the different topics feature. Is there a difference in proportions between \"business\" and \"sports\" articles?\n\n<p style=\"color:blue\"> *From the fitted model, we can extract the proportions, together with news category, using*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nArticles_topics <- tidy( Articles_LDA , matrix = \"gamma\" ) %>% \n  separate( document, c(\"NewsType\", \"ID\"), sep=\"_\", convert=TRUE )\n```\n:::\n\n\n<p style=\"color:blue\"> *All that is left is to create a facet plot, which visualizes the estimated proportions for the two news categories:*</p>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot( Articles_topics, aes( x=factor(topic), y=gamma ) ) + \n  facet_wrap( ~NewsType ) + geom_boxplot() + \n  labs( x=\"Topic\", y=\"Proportion\" )\n```\n\n::: {.cell-output-display}\n![](Quiz-6-Solutions_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n<p style=\"color:blue\"> *The box plots indicate that the two topics tend to reflect the differences in content. More precisely, documents which predominately feature Topic 1 belong to the \"sports\" category, while documents mostly made up of Topic 2 fall usually into the \"business\" category. Consequently, there is a difference in proportions between \"business\" and \"sports\" articles*</p>\n\nc) Which are the five most common words in each of the two topics?\n\n<p style=\"color:blue\"> *We adapt the code used in Section 4.3 to extract the most common words for each category:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy( Articles_LDA , matrix = \"beta\" ) %>%\n  group_by( topic ) %>%\n  slice_max( beta, n=5 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 x 3\n# Groups:   topic [2]\n   topic term        beta\n   <int> <chr>      <dbl>\n 1     1 pakistan 0.00935\n 2     1 world    0.00758\n 3     1 england  0.00722\n 4     1 test     0.00680\n 5     1 cricket  0.00677\n 6     2 percent  0.0145 \n 7     2 oil      0.00865\n 8     2 million  0.00612\n 9     2 prices   0.00607\n10     2 strong   0.00557\n```\n\n\n:::\n:::\n\n\n\n",
    "supporting": [
      "Quiz-6-Solutions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}