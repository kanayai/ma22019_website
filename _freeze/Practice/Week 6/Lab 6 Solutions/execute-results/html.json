{
  "hash": "7354702e6dcc9e669d337c0b9b446fbe",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"MA22019 2025 - Solutions for Computer Lab 6\"\nsubtitle: \"Mise en place\"\ncategories: [\"Lab\", \"Solutions\"]\n---\n\n\nYou may want to load the following packages before starting the exercise:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary( dplyr )\nlibrary( ggplot2 )\nlibrary( tidytext )\nlibrary( stringr )\nlibrary( tidyr )\nlibrary( topicmodels )\n```\n:::\n\n\nWhen working on a University PC, you have to first install the tidytext package and any dependencies using\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages( \"tidytext\", dependencies = TRUE )\n```\n:::\n\n\nFor the sentiment analysis you can load the sentiment lexicons using\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAFINN <- read.csv(\"data/AFINN Sentiment Lexicon.csv\")\nBing <- read.csv(\"data/Bing Sentiment Lexicon.csv\")\n```\n:::\n\n\n\n### **Tutorial Question 1 - Comparing books**\n\nWe are asked to explore and compare the books *Anne of Green Gables* by L.M. Montgomery and *Rebecca of Sunnybrook Farm* by Kate Douglas Wiggin. The two books are provided together in the file \"Books Tutorial Question 1.csv\" and the following information is provided:\n\n* **text** - Text as printed in the book\n\n* **title** - Book the text comes from\n\n* **chapter** - Chapter the text belongs to\n\nPerform the following tasks using the techniques described in Chapter 3 of the lecture notes. \n\na) Extract the two words with the highest term frequency-inverse document frequency for each book, with the corpus only containing *Anne of Green Gables* and *Rebecca of Sunnybrook Farm*.\n\n<p style=\"color:blue\"> *Let's start by loading the data:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBooks_raw <- read.csv( \"data/Books Tutorial Question 1.csv\" )\n```\n:::\n\n\n<p style=\"color:blue\"> *We first split the text into individual words and remove any underscores, before counting the number of occurrences of each word per book:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBooks_count <- Books_raw %>%\n  unnest_tokens( word, text ) %>%\n  mutate( word = gsub( \"\\\\_\", \"\", word ) ) %>%\n  count( title, word )\n```\n:::\n\n\n<p style=\"color:blue\"> *We can now use the bind_tf_idf() function to derive the tf-idf and look for the top 2 for each book:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBooks_count %>% \n  bind_tf_idf( word, title, n ) %>%\n  group_by( title ) %>%\n  arrange( desc(tf_idf) ) %>%\n  slice_head( n=3 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 6\n# Groups:   title [2]\n  title                      word          n      tf   idf   tf_idf\n  <chr>                      <chr>     <int>   <dbl> <dbl>    <dbl>\n1 Anne of Green Gables       anne       1102 0.0107  0.693 0.00740 \n2 Anne of Green Gables       marilla     795 0.00770 0.693 0.00534 \n3 Anne of Green Gables       diana       385 0.00373 0.693 0.00258 \n4 Rebecca of Sunnybrook Farm rebecca     571 0.00771 0.693 0.00535 \n5 Rebecca of Sunnybrook Farm rebecca's   105 0.00142 0.693 0.000983\n6 Rebecca of Sunnybrook Farm cobb         90 0.00122 0.693 0.000843\n```\n\n\n:::\n:::\n\n\n<p style=\"color:blue\"> *The output shows that \"Anne\" and \"Marilla\" have the highest tf-idf for \"Anne of Green Gables\". When looking at \"Rebecca of Sunnybrook Farm\", \"Rebecca\" and \"Rebecca's\" should not be considered different words. As such, \"Rebecca\" and \"Cobb\" are the two words with the highest tf-idf in that book.*</p>\n\nb) Use sentiment analysis to explore how the emotional intent has evolved over the two books. How do the two books compare?\n\n<p style=\"color:blue\"> *To begin with, we require a sentiment lexicon, such as AFINN:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAFINN <- read.csv(\"data/AFINN Sentiment Lexicon.csv\")\n```\n:::\n\n\n<p style=\"color:blue\"> *We want to consider sentiment per chapter. Before calculating the sentiment score, we need to split and clean the text data :*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBooks <- Books_raw %>%\n  unnest_tokens( word, text ) %>%\n  mutate( word = gsub( \"_\", \"\", word ) )\n```\n:::\n\n\n<p style=\"color:blue\"> *The next step is to calculate the sentiment for each chapter and book. We can adapt the code from Section 4.2 for this purpose:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBooks <- Books %>%\n  inner_join( AFINN, by=\"word\" ) %>%\n  group_by( title, chapter ) %>%\n  summarise( sentiment = mean(value) )\n```\n:::\n\n\n<p style=\"color:blue\"> *Finally, we visualize the sentiment per chapter for each book:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot( Books, aes( x=chapter, y=sentiment ) ) + \n  facet_wrap( ~title, scales = \"free_x\" ) + geom_col() + theme_bw() +\n  labs( x=\"Chapter\", y=\"AFINN sentiment score\" )\n```\n\n::: {.cell-output-display}\n![](Lab-6-Solutions_files/figure-html/unnamed-chunk-10-1.png){width=100%}\n:::\n:::\n\n\n\n<p style=\"color:blue\"> *We find that both books used generally a positive sentiment in terms of the AFINN sentiment lexicon. Looking at the individual plots, it's difficult to identify a clear pattern. For \"Anne of Green Gables\", one may argue that the last chapters are slightly more positive than the beginning of the book, but it's not really conclusive. For \"Rebbeca of Sunnybrook Farm\", the plot shows a slight increase in sentiment before the book ending on a slightly less \"posiitve\" sentiment. Overall, apart from the general \"positive\" sentiment, there are little obvious similarities in terms of how the emotional intent evolves throughout the books.*</p>\n\n\n\nc) Suppose each book chapter is considered as a separate document (as we did in Section 3.4.3 in the lecture notes). Use Latent Dirichlet Allocation to derive $K=2$ topics, and then study the estimated proportions provided by the model. What do you conclude?\n\n<p style=\"color:blue\"> *The data is already in a nice format and we can thus perform the same steps as in the analysis of the two books by Charles Dickens:*</p>\n\n<p style=\"color:blue\"> *We first create the document term matrix*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBooks_chapters <- Books_raw %>%\n  unite( col=document, title, chapter ) %>%\n  unnest_tokens( word, text ) %>%\n  mutate( word = gsub( \"_\", \"\", word ) ) %>%\n  anti_join( stop_words, by=\"word\" ) %>%\n  count( document, word, sort = TRUE )\nBooks_dtm <- Books_chapters %>% cast_dtm( document, word, n )\n```\n:::\n\n\n<p style=\"color:blue\"> *Now we can fit the LDA model:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBooks_LDA <- LDA( Books_dtm, k=2, method = \"Gibbs\", control=list(seed=123) )\n```\n:::\n\n\n<p style=\"color:blue\"> *We start by analyzing the make-up of the individual chapters:*</p>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy( Books_LDA , matrix = \"gamma\" ) %>%\n  separate( document, c(\"title\", \"chapter\"), sep=\"_\", convert=TRUE ) %>%\n  ggplot( aes( x=factor(topic), y=gamma ) ) + \n  facet_wrap( ~title ) + geom_boxplot() + \n  labs( x=\"Topic\", y=\"Proportion\" )\n```\n\n::: {.cell-output-display}\n![Illustration of the estimated proportions for each chapter within each book.](Lab-6-Solutions_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n<p style=\"color:blue\"> *The results suggest that the fitted model does a good job at identifying which book the indiviudal chapters come from. The proportion of Topic 1 is high for chapters from \"Anne of Green Gables\", while Topic 2 features predominately in \"Rebecca of Sunnybrook Farm\".*</p>\n\n<p style=\"color:blue\"> *Finally, we look at proportions for the individual words within th two topics:*</p>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy( Books_LDA , matrix = \"beta\" ) %>%\n  mutate( topic = case_when( topic==1 ~ \"Topic1\", topic==2 ~ \"Topic2\") ) %>%\n  pivot_wider( names_from = topic, values_from = beta, values_fill = 0 ) %>%\n  ggplot( aes(x=Topic1, y=Topic2) ) + geom_point() +  \n  geom_text( aes(label=term), check_overlap = TRUE, vjust=1 ) + \n  coord_trans( x=\"sqrt\", y=\"sqrt\" ) + theme_bw() +\n  labs( x=\"Term Frequency in Topic 1\", y=\"Term Frequency in Topic 2\" )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `coord_trans()` was deprecated in ggplot2 4.0.0.\ni Please use `coord_transform()` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Comparison of term frequencies for the two topics.](Lab-6-Solutions_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=65%}\n:::\n:::\n\n\n<p style=\"color:blue\"> *We find a similar pattern to that in Section 4.4.3. The character names are mostly allocated to one of the two topics, while the majority of words features with a similar frequency in both topics.*</p>\n\nd) Some scholars claim that *Anne of Green Gables* is patterned after *Rebecca of Sunnybrook Farm*. Discuss whether your results in parts a)-c) support this claim or not. \n\n<p style=\"color:blue\"> *The results in part c) and e) give some support, as the clearest differences are observed for the character names, while the remaining words have a similar frequency and occur in both books. However, the differences in sentiment from part d) make this claim seem less likely. So we may conclude that the books may have some similarities in language, but the stories are significantly different in terms of their sentiment.*</p>\n\n<p style=\"color:blue\"> *Looking at the summaries of the two books, we find that both books desribe young girls and their coming-of-age stories, but that Anne and Rebecca are different in their personalities:*</p>\n\n* <p style=\"color:blue\">*\"Anne of Green Gables\" is about Anne's personal growth, her imaginative and often dramatic nature, and the challenges she faces in building relationships, particularly with Marilla and Diana.*</p>\n\n* <p style=\"color:blue\">*\"Rebecca of Sunnybrook Farm\" focuses on Rebecca who is optimistic, and tends to bring out the best in everyone around her.*</p>\n\n### **Tutorial Question 2 - The function grep()**\n\nWe so far only used the function grep() when extracting the text data from the files provided by Project Gutenberg. Specifically, we used it to identify the lines containing the word \"EBOOK\", signalling the beginning and end of the book. The following exercise will require you to use grep() to identify all lines which contain a specific phrase:\n\nThe Police of Utopia sent us data on burglaries which were reported between 2015 and 2021, including a short description providing information on the number of criminals and their victims. Victims are classified into six groups: \"young single\", \"young couple\", \"middle-aged single\", \"middle-aged couple\", \"elderly single\" and \"elderly couple\". The data are available in the file \"UtopiaCrimes.csv\". Extract the following information using the functions grep():\n\na) For which group of people did the police record the most burglaries?\n\n<p style=\"color:blue\">*We start by loading the date:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCrimes <- read.csv(\"data/UtopiaCrimes.csv\")\n```\n:::\n\n\n<p style=\"color:blue\">*Looking at the data, we see that the description states the group affected by the burglary. So we need for each group to extract the lines of text in which they are mentioned and count the number of occurrences. The function grep() extracts the indices of the lines and length() then counts them:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength( grep( Crimes$Description, pattern = \"young single\" ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2126\n```\n\n\n:::\n\n```{.r .cell-code}\nlength( grep( Crimes$Description, pattern = \"young couple\" ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1488\n```\n\n\n:::\n\n```{.r .cell-code}\nlength( grep( Crimes$Description, pattern = \"middle-aged single\" ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3043\n```\n\n\n:::\n\n```{.r .cell-code}\nlength( grep( Crimes$Description, pattern = \"middle-aged couple\" ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2017\n```\n\n\n:::\n\n```{.r .cell-code}\nlength( grep( Crimes$Description, pattern = \"elderly single\" ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4410\n```\n\n\n:::\n\n```{.r .cell-code}\nlength( grep( Crimes$Description, pattern = \"elderly couple\" ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3429\n```\n\n\n:::\n:::\n\n\n<p style=\"color:blue\">*The results show that the most burglaries were observed for the group \"elderly single\".*</p>\n\nb) What is the proportion of burglaries that involved more than two criminals?\n\n<p style=\"color:blue\">*We need to extract the descriptions which state \"Three criminals\", and \"More than 3 criminals\". This can again be achhieved using grep() and length():*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_3_burglars <- length( grep( Crimes$Description, pattern = \"Three Criminals\" ) )\nnum_4_burglars <- length( grep( Crimes$Description, pattern = \"More than 3 criminals\" ) )\n```\n:::\n\n\n<p style=\"color:blue\">*Finally, we calculate the proportion:*</p>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n( num_3_burglars + num_4_burglars ) / nrow( Crimes )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2440501\n```\n\n\n:::\n:::\n\n\n<p style=\"color:blue\">*We find that 24.4% of burglaries were committed by a group of three or more people.*</p>\n\n\n",
    "supporting": [
      "Lab-6-Solutions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}