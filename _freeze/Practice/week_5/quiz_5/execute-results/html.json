{
  "hash": "9574d7474d5657d1142d4a864e97a252",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"MA22019 2025 - Quiz 5\"\nsubtitle: \"Mise en place\"\ndescription: \"To work after the lecture\"\ncategories: \"Quiz\"\n---\n\n\n\n### **Overview**\n\nThis week's problem sheet focuses on the text data analysis techniques covered in Sections 3.1-3.3.1 in the lecture notes. Exercises 1-3 are designed to help you with revising the content of the lecture from Week 5. You can check your solutions to these questions yourself by answering the Moodle quiz.\n\n\nYou may want to load the following packages before starting the exercise:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary( dplyr )\nlibrary( ggplot2 )\nlibrary( tidytext )\nlibrary( wordcloud )\nlibrary( stringr )\n```\n:::\n\n\nWhen working on a University PC, you have to first install the tidytext and wordcloud packages. The installation of the textdata package is quite cumbersome, and thus I provide the AFINN and Bing sentiment lexicons as .csv files for your convenience. You can load them using\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAFINN <- read.csv( \"data/AFINN Sentiment Lexicon.csv\" )\nBing <- read.csv( \"data/Bing Sentiment Lexicon.csv\" )\n```\n:::\n\n\n\n\n### **Exercise 1 - Short stories by Edgar Allan Poe**\n\nThe file \"Poe.csv\" contains the book *The Works of Edgar Allan Poe - Volume 1* which includes eight short stories by the American author Edgar Allen Poe. Each row in the data set gives a line from the book and the name of the short story that line belongs to. To load the data, we use\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPoe_raw <- read.csv(\"data/poe.csv\" )\n```\n:::\n\n\nThe following exercises are designed to help you with revising the steps we used for extracting the most common words for *Jane Eyre* in the lecture.\n\na) Extract the individual words from the text and remove any underscores.\n\nb) Which are the five most common words (including stop words) across all stories?\n\nc) For each short story, identify the word most commonly used within it (excluding stop words).\n\n\n### **Exercise 2 - Baby names in the USA between 1880 and 2017**\n\nThe file “Babynames.csv” provides a comprehensive record of the names given to newborns in the United States between 1880 and 2017 according to the Social Security Administration. For each name, year and sex, we are given the number of newborn babies given that name (as long as the name was used at least five times in that year). We are further provided with the proportion of newborn babies who were given that name amongst babies of the same sex.\n\na) What was the most common baby name over the period 1880-2017?\n\nb) Create a word cloud which visualizes the 30 most common baby names (in terms of total number) for a girl between 1880 and 2017.\n\nc) Explore how the popularity of the girl name “Mary” evolved over the period 1880-2017. When did its popularity peak?\n\n\n### **Exercise 3 - Analysis of the book** *Frankenstein*\n\nThe novel *Frankenstein* by Mary Shelley tells the story of a young scientist who creates a creature via an experiment and is subsequently horrified by what he has made. The book is provided in the file \"Frankenstein.csv\" and can be loaded using\n\n\n::: {.cell}\n\n```{.r .cell-code}\nFrankenstein_raw <- read.csv(\"data/frankenstein.csv\", fileEncoding = \"UTF-8\" )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,\n: invalid input found on input connection 'data/frankenstein.csv'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,\n: EOF within quoted string\n```\n\n\n:::\n:::\n\n\nPerform an analysis that addresses the following questions for *Frankenstein*:\n\na) Which five words appear the most often (excluding stop words)?\n\nb) Calculate the AFINN sentiment score (sum of the AFINN score of all words) for each chapter. Which chapter has the highest/lowest sentiment score? \n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}