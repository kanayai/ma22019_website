{
  "hash": "f432bc4f6eeba879289921784269476c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Problem Class 4\"\nauthor: \"Dr. Karim Anaya-Izquierdo\"\nformat: html\n---\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidytext)\nlibrary(stringr)\n```\n:::\n\n\n\n## Background\n\nJane Austen wrote seven novels, and we consider six of these: *Emma*, *Mansfield Park*, *Northanger Abbey*, *Persuasion*, *Pride and Prejudice* and *Sense and Sensibility*. \n\nAll novels are available via Project Gutenberg and we will analyze similarities and differences of the different books in the following. We start by loading the text data which is available in the file \"Data/JaneAusten.csv\" on Moodle:\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nJaneAusten_raw <- read.csv(\"data/janeausten.csv\" )\n```\n:::\n\n\nWe will analyze the six books and\n\n1) Identify the most common words (except stop words) for each book\n\n2) Extract the words with the highest term frequency - inverse document frequency\n\n3) Compare the six books in terms of their sentiment\n\n\n## Word frequency analysis\n\nAs before, we first have to bring the data into a usable format and remove stop words.\n\n**Task 1:** Split the lines of text into individual words and remove all stop words and underscores.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJaneAusten <- JaneAusten_raw %>%\n  unnest_tokens( word, text ) %>%\n  mutate( word = gsub(\"_\", \"\", word) ) %>%\n  anti_join( stop_words, by=\"word\" )\n```\n:::\n\n\nWith the data in the desired format, we are ready to identify the most common words:\n\n**Task 2:** Extract the ten most common words (excluding stop words) for each book.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJaneAusten_Count <- JaneAusten %>%\n  group_by( title ) %>%\n  count( word, name = \"Count\", sort = TRUE ) %>%\n  slice_head( n=10 )\n```\n:::\n\n\nAfter identifying the most frequent words, let's visualize them. The following piece of code produces for each book a bar plot which visualizes the number of occurrences of the words identified in Task 2, i.e., the bar plot for a book provides information on the ten most common words (excluding stop words) contained in that book.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot( JaneAusten_Count, \n        aes( x=Count, y=reorder_within(word,Count,title), fill=title ) ) + \n  facet_wrap( ~title, scales = \"free\" ) + \n  geom_col( show.legend = FALSE ) + \n  scale_y_reordered() + theme_bw() + \n  labs( x=\"Count\", y=\"Word\" )\n```\n\n::: {.cell-output-display}\n![](analysis_of_jane_austen_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n**Task 3:** What is the benefit of using the functions **reorder_within()** and **scale_y_reordered()**? What do we conclude from the plot?\n\nTo conclude our analysis on the words used within the books, we want to calculate the tf-idf values for each term.\n\n**Task 4:** Calculate the tf-idf value for each word and book. What do you conclude when considering the words with the highest tf-idf?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJaneAusten_Count <- JaneAusten %>% count( title, word, sort=TRUE )\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJaneAusten_Count %>% \n  bind_tf_idf( word, title, n ) %>%\n  arrange( desc(tf_idf) ) %>%\n  slice_head(n=20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   title       word   n          tf       idf     tf_idf\n1  Sense and Sensibility     elinor 685 0.018815063 1.7917595 0.03371207\n2  Sense and Sensibility   marianne 566 0.015546461 1.7917595 0.02785552\n3             Persuasion     elliot 289 0.011328003 1.7917595 0.02029706\n4    Pride and Prejudice      darcy 432 0.011019565 1.7917595 0.01974441\n5                   Emma     weston 440 0.009446114 1.7917595 0.01692516\n6    Pride and Prejudice     bennet 339 0.008647297 1.7917595 0.01549388\n7             Persuasion  wentworth 218 0.008544998 1.7917595 0.01531058\n8                   Emma  knightley 389 0.008351224 1.7917595 0.01496338\n9                   Emma      elton 387 0.008308287 1.7917595 0.01488645\n10   Pride and Prejudice    bingley 310 0.007907558 1.7917595 0.01416844\n11      Northanger Abbey  catherine 487 0.020431280 0.6931472 0.01416188\n12        Mansfield Park   crawford 605 0.012739792 1.0986123 0.01399609\n13                  Emma       emma 865 0.018570202 0.6931472 0.01287188\n14                  Emma  woodhouse 315 0.006762559 1.7917595 0.01211688\n15 Sense and Sensibility   jennings 235 0.006454803 1.7917595 0.01156545\n16      Northanger Abbey    morland 148 0.006209095 1.7917595 0.01112521\n17 Sense and Sensibility willoughby 216 0.005932925 1.7917595 0.01063037\n18            Persuasion    russell 148 0.005801192 1.7917595 0.01039434\n19        Mansfield Park    bertram 272 0.005727642 1.7917595 0.01026256\n20      Northanger Abbey     tilney 221 0.009271690 1.0986123 0.01018599\n```\n\n\n:::\n:::\n\n\n## Sentiment analysis\n\nLet's study the sentiment of the books using the AFINN sentiment lexicon:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAFINN <- read.csv( \"Data/AFINN Sentiment Lexicon.csv\" )\n```\n:::\n\n\nWe want to derive the sentiment score for each chapter in each book. To do this, we first need to identify which chapter each line belongs to. We can do this by adapting the code from Section 3.2 in the lecture notes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJaneAusten_chapters <- JaneAusten_raw %>%\n  group_by( title ) %>%\n  mutate( chapter = cumsum( str_detect(\n    text, regex(\"^chapter \", ignore_case = TRUE)\n  ) ) ) %>%\n  filter( chapter > 0 ) %>%\n  ungroup()\n```\n:::\n\n\nThe next step is to split the lines of text into individual words and to match the words in the AFINN sentiment lexicon with the words in the books. As for Jane Eyre, we remove the word \"miss\" from the analysis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJaneAusten_AFINN <- JaneAusten_chapters %>%\n  filter( chapter > 0 ) %>%\n  unnest_tokens( word, text ) %>%\n  mutate( word = gsub( \"_\", \"\", word ) ) %>%\n  inner_join( AFINN, by = \"word\" ) %>% \n  filter( word != \"miss\" )\n```\n:::\n\n  \n**Task 5:** Derive the aggregated sentiment score for each chapter using the AFINN sentiment lexicon.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJaneAusten_AFINN <- JaneAusten_AFINN %>%\n  group_by( title, chapter ) %>%\n  summarise( sentiment = sum(value) )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'title'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n:::\n\n\n\nWe produce plots which illustrate the sentiment for the different chapters and books as follows:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot( JaneAusten_AFINN, aes( x=chapter, y=sentiment ) ) +\n  facet_wrap( ~title, scales=\"free_x\" ) +\n  geom_col( aes( fill=title ), show.legend = FALSE ) + \n  theme_bw() + labs( x=\"Chapter\", y=\"AFINN sentiment score\" ) \n```\n\n::: {.cell-output-display}\n![](analysis_of_jane_austen_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=95%}\n:::\n:::\n\n\n**Task 6:** What do we conclude?\n",
    "supporting": [
      "analysis_of_jane_austen_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}