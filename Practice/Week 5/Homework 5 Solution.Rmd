---
title: "MA22019 2025 - Solutions for Homework 5"
author: "Text data analysis - Word frequency and sentiment"
output:
  html_document: default
  pdf_document: default
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### **Overview**

This week's problem sheet focuses on the text data analysis techniques covered in Sections 3.1-3.3.1 in the lecture notes. Exercises 1-3 are designed to help you with revising the content of the lecture from Week 5. You can check your solutions to these questions yourself by answering the Moodle quiz.

Tutorial Question 1 is similar to Exercises 1-3, while Tutorial Question 2 explores how well the type of sentiment analysis we introduced performs at assessing sentiment for product reviews. Should time permit, you may want to work on the Homework Question during the tutorial. 

Your answer to the Homework Question can be submitted on Moodle to your tutor for feedback. The submission deadline is 17:00 on Thursday 13 March 2025. You should submit a single PDF or Word file that provides your R code, any created R output and all your comments.

You may want to load the following packages before starting the exercise:

```{r, message=FALSE, warning=FALSE}
library( dplyr )
library( ggplot2 )
library( tidytext )
library( wordcloud )
library( stringr )
```

When working on a University PC, you have to first install the tidytext and wordcloud packages. The installation of the textdata package is quite cumbersome, and thus I provide the AFINN and Bing sentiment lexicons as .csv files for your convenience. You can load them using

```{r}
AFINN <- read.csv("AFINN Sentiment Lexicon.csv")
Bing <- read.csv("Bing Sentiment Lexicon.csv")
```

### **Exercise 1 - Short stories by Edgar Allan Poe**

The file "Poe.csv" contains the book *The Works of Edgar Allan Poe - Volume 1* which includes eight short stories by the American author Edgar Allen Poe. Each row in the data set gives a line from the book and the name of the short story that line belongs to. To load the data, we use

```{r}
Poe_raw <- read.csv("Poe.csv")
```

The following exercises are designed to help you with revising the steps we used for extracting the most common words for *Jane Eyre* in the lecture.

a) Extract the individual words from the text and remove any underscores.

<p style="color:blue"> *Looking at Section 3.1.2, we identify that we need to use unnest_tokens() and gsub():*</p>

```{r}
Poe <- Poe_raw %>%
  unnest_tokens( word, text ) %>%
  mutate( word = gsub( "\\_", "", word ) ) 
```

b) Which are the five most common words (including stop words) across all stories?

<p style="color:blue"> *We apply the count() function we used for calculating the term frequency in Section 3.1.2 and output the five words with the highest count:*</p>

```{r}
Poe %>% 
  count( word, sort=TRUE ) %>% 
  slice_head(n=5)
```

<p style="color:blue"> *We find that "the", "of", "and", "to" and "a" are the most common words across the short stories.*</p>


c) For each short story, identify the word most commonly used within it (excluding stop words).

<p style="color:blue"> *We first remove the stop words in the list provided by the tidytext package:*</p>

```{r}
data( stop_words )
Poe <- Poe %>% anti_join( stop_words, by="word" )
```

<p style="color:blue"> *Finally, we apply the count() function again, but we need to do it for each story. So we combine count() with group_by():*</p>

```{r}
Poe %>%
  group_by( story ) %>%
  count( word, sort=TRUE ) %>% 
  slice_head( n=1 )
```


### **Exercise 2 - Baby names in the USA between 1880 and 2017**

The file “Babynames.csv” provides a comprehensive record of the names given to newborns in the United States between 1880 and 2017 according to the Social Security Administration. For each name, year and sex, we are given the number of newborn babies given that name (as long as the name was used at least five times in that year). We are further provided with the proportion of newborn babies who were given that name amongst babies of the same sex.

a) What was the most common baby name over the period 1880-2017?

<p style="color:blue"> *We start by loading the data set:*</p>

```{r}
Babies <- read.csv("Babynames.csv")
```

<p style="color:blue"> *To find the most common name, we have to sum up the counts across the different years:*</p>

```{r}
Babies %>%
  group_by( Name=name ) %>%
  summarise( Count = sum(n) ) %>%
  arrange( desc(Count) ) %>%
  slice_head( n=1 )
```

<p style="color:blue"> *We find that "James" was the most common name given to a baby over the period 1880-2017.*</p>


b) Create a word cloud which visualizes the 30 most common baby names (in terms of total number) for a girl between 1880 and 2017.

<p style="color:blue"> *We start by calculating for each name how many girls were given that name.*</p>

```{r}
Names_Girls <- Babies %>%
  filter( sex=="F" ) %>%
  group_by( name ) %>%
  summarize( Count=sum(n) )
```

<p style="color:blue"> *Using the code in Section 3.1.2, we create a word cloud as follows:*</p>

```{r, fig.align='center', out.width='60%'}
Names_Girls %>% with( wordcloud( name, Count, max.words=30 ) )
```

c) Explore how the popularity of the girl name “Mary” evolved over the period 1880-2017. When did its popularity peak?

<p style="color:blue"> *One way is to create a plot for the proportions over time*</p>

```{r, fig.align='center', out.width='50%', fig.height=3.5, fig.width=5}
Babies %>% 
  filter( sex=="F", name=="Mary" ) %>%
  ggplot( aes("x"=year, "y"=prop) ) + geom_line( linewidth=1 ) + 
  labs( x="Year", y="Proportion of newborn girls named Mary" ) + theme_bw()
```

<p style="color:blue">*From the plot, we conclude that "Mary" has become less popular over the period, falling from a peak of more than 7% in 1880 to 0.1-0.2% in the 2010s.*</p>


### **Exercise 3 - Analysis of the book** *Frankenstein*

The novel *Frankenstein* by Mary Shelley tells the story of a young scientist who creates a creature via an experiment and is subsequently horrified by what he has made. The book is provided in the file "Frankenstein.csv" and can be loaded using

```{r}
Frankenstein_raw <- read.csv( "Frankenstein.csv", fileEncoding = "UTF-8" )
```

Perform an analysis that addresses the following questions for *Frankenstein*:

a) Which five words appear the most often (excluding stop words)?

<p style="color:blue">*We first split the lines of text and remove any punctuation:*</p>

```{r}
Frankenstein <- Frankenstein_raw %>% unnest_tokens( word, text )
Frankenstein <- Frankenstein %>% mutate( word = gsub( "_", "", word ) )
```

<p style="color:blue">*We now count the number of occurrences for each word and remove any words that are categorized as stop words. Finally, we print the five most common words (excluding stop words):*</p>

```{r}
Frankenstein %>% 
  count( word, sort=TRUE ) %>%
  anti_join( stop_words, by=c("word") ) %>% 
  slice_head( n=5 )
```

<p style="color:blue">*We conclude that that five most common words (excluding stop words) are “life”, “father”, “eyes”, “time” and “night”.*</p>

b) Calculate the AFINN sentiment score (sum of the AFINN score of all words) for each chapter. Which chapter has the highest/lowest sentiment score?

<p style="color:blue">*We want to study chapters, and thus we take the code from Section 3.2.2 to derive the chapter the individual lines belong to:*</p>

```{r}
Frankenstein_chapters <- Frankenstein_raw %>%
  mutate( chapter = cumsum( str_detect(
    text, regex("^chapter ", ignore_case = TRUE)
  ) ) ) 
```

<p style="color:blue">*The next step is to remove all the text before the first chapter begins, and to then split the remaining text into the indiviudal words:*</p>

```{r}
Frankenstein_chapters <- Frankenstein_chapters %>%
  filter( chapter > 0 ) %>%
  unnest_tokens( word, text ) %>%
  mutate( word = gsub( "\\_", "", word ) )
```

<p style="color:blue">*Since we are asked to use the AFINN sentiment lexicon, we load it:*</p>

```{r}
AFINN <- read.csv( "AFINN Sentiment Lexicon.csv" )
```

<p style="color:blue">*We can now use the code from Section 3.2.2 to derive the sentiment score for each chapter:*</p>

```{r}
Frankenstein_sentiment <- Frankenstein_chapters %>%
  inner_join( AFINN, by="word" ) %>%
  group_by( chapter ) %>%
  summarise( sentiment = sum(value) )
```

<p style="color:blue">*Finally, let's extract the chapters with the lowest and highest AFINN sentiment score:*</p>

```{r}
slice_min( Frankenstein_sentiment, sentiment, n=1 )
slice_max( Frankenstein_sentiment, sentiment, n=1 )
```

<p style="color:blue">*We conclude that Chapter 6 has the highest score (157), while Chapter 24 has the lowest score (-290).*</p>


### **Tutorial Question 1 - Word frequency analysis for** *Les Misérables*

We want to analyse the book *Les Misérables* by Victor Hugo in terms of the words used therein. The file "LesMis.csv" contains the data as provided by Project Gutenberg, but with the metadata at the beginning and end already removed. Consider the following questions:

a) Extract the individual words from the text, and remove any stop words and underscores.

<p style="color:blue"> *We adapt the code from the lectures and use the functions unnest_tokens(), gsub() and anti_join():*</p>
  
```{r}
LesMis_raw <- read.csv( "LesMis.csv", fileEncoding="UTF-8" )
data( stop_words )

LesMis <- LesMis_raw %>%
  unnest_tokens( word, text ) %>%
  mutate( word = gsub( "\\_", "", word ) )  %>% 
  anti_join( stop_words, by="word" )
```

b) Find the 10 most common in *Les Misérables* (excluding stop words) and create a bar plot which visualizes their number of occurrences.

<p style="color:blue"> *We first create a data frame that gives the number of times each word appeared in the text and we sort the words based on this count*</p>

```{r}
LesMis_Count <- LesMis %>% count( word, sort=TRUE )
```

<p style="color:blue"> *We now extract and reorder the top 10 words before creating a bar plot as in Section 3.1.2 of the lecture notes:*</p>

```{r, out.width='50%', fig.align='center'}
LesMis_Count %>%
  slice_head( n=10 ) %>%
  mutate( word = reorder(word,n) ) %>%
  ggplot( aes( x=n, y=word) ) + geom_col() + 
  theme_bw() + labs( x="Count", y="" ) +
  theme( axis.text=element_text(size=17), axis.title=element_text(size=17) )
```

<p style="color:blue"> *We find that the four most frequent words (excluding stop words) refer to three of the protagonists in the book. We also see that the word "chapter" appears very often. This finding results from the chapter titles being labeled "Chapter.." and thus this is not really accurate (the book also provides a table of contents that uses "chapter" a lot).*</p>

c) Create a word cloud for the 30 most frequently used words (excluding stop words).

<p style="color:blue"> *We adapt the code from Section 3.1.2 in the lecture notes:*</p>

```{r, warning=FALSE, message=FALSE, fig.align='center', out.width='60%'}
LesMis_Count %>% with( wordcloud( word, n, max.words=30 ) )
```

<p style="color:blue"> *We again see that the 30 most common words include the main characters and the city of Paris, where most of the story takes place. The remaining words are hard to group within a single theme.*</p>


### **Tutorial Question 2 - Amazon Reviews** 

One important application area for sentiment analysis concerns the analysis of customer reviews. To explore how good the AFINN lexicon performs at capturing the overall sentiment of a review, we consider 1,000 Amazon product reviews. The data is stored in the file “AmazonReviews.csv”.

```{r}
Reviews_raw <- read.csv( "AmazonReviews.csv" )
```

The data provides the rating ("1 Star" to "5 Stars") and the accompanying text for each review. We want to derive the sentiment for each review and then compare it to the given score to assess how well we perform at capturing the sentiment of the text.

a) Using the AFINN sentiment lexicon, for each review, calculate the average sentiment per word.

<p style="color:blue"> *We start by splitting the reviews into individual words:*</p>

```{r}
Reviews <- Reviews_raw %>% unnest_tokens( word, Text )
```

<p style="color:blue"> *The question asks us to use th AFINN sentiment lexicon, and so we have to load it:*</p>

```{r}
AFINN <- read.csv("AFINN Sentiment Lexicon.csv")
```

<p style="color:blue"> *Instead of calculating the aggregate sentiment, we are asked to consider average sentiment per word. This means we cannot simply remove the words that are not in the sentiment lexicon, i.e, the anti_join() function would be a poor choice. Instead, we need to keep all the words and replace any NA entries by 0. This can, for instance, be achieved using the following code*</p>

```{r}
Reviews_Sentiment <- Reviews %>%
  left_join( AFINN, by="word" ) %>%
  mutate( value = case_when( is.na(value)==TRUE ~ 0, .default = value ) ) %>%
  group_by( Review ) %>%
  summarise( Sentiment=mean(value) )
```

b) Plot the average sentiments calculated in part a) against the star ratings. What do you conclude?  

<p style="color:blue"> *To create the plot, we first need to link up the sentiment score derived in part a) with the star ratings:*</p>

```{r}
Reviews_Sentiment <- Reviews_Sentiment %>% mutate( Score = Reviews_raw$Score )
```

<p style="color:blue"> *Now we can create box plots of the calculated sentiment scores for each category of the star rating:*</p>

```{r, fig.align='center', out.width='40%', fig.height=4, fig.width=4}
ggplot( Reviews_Sentiment, aes(x=Score, y=Sentiment) ) + 
  geom_boxplot() + theme_bw() + labs( x="Star Rating", y="Sentiment Score")
```

<p style="color:blue"> *We see that higher star ratings are associated with slightly higher sentiment scores. However, we should note that sentiment scores are quite variable, and thus the sentiment score is not necessarily a great predictor in terms of star rating.*</p>


### **Homework Question - The work of H.G. Wells**

We want to analyze and compare the novels *The Time Machine* and *War of the Worlds* by the British author Herbert George Wells. The books are provided in the files "Time Machine.csv" and "War of Worlds.csv".

a) Compare the two books in terms of the words used within them.

<p style="color:blue"> *We start by loading the two text data sets:*</p>

```{r}
Time_raw <- read.csv( "Time Machine.csv", fileEncoding = "UTF-8", as.is = TRUE )
War_raw <- read.csv( "War of Worlds.csv", fileEncoding = "UTF-8", as.is = TRUE )
```

<p style="color:blue"> *As in previous analyses, we split the texts into individual words and remove any underscores:*</p>

```{r}
Time <- Time_raw %>%
  unnest_tokens( word, text ) %>%
  mutate( word = gsub( "\\_", "", word ) ) 
War <- War_raw %>%
  unnest_tokens( word, text ) %>%
  mutate( word = gsub( "\\_", "", word ) ) 
```

<p style="color:blue"> *With the data ready for analysis, we calculate the number of occurrences and term frequency for each word before removing stop words:*</p>

```{r}
Time <- Time %>% 
  count( word, sort=TRUE ) %>%
  mutate( tf = n / sum(n) ) %>% 
  anti_join( stop_words, by="word") 
War <- War %>% 
  count( word, sort=TRUE ) %>%
  mutate( tf = n / sum(n) ) %>% 
  anti_join( stop_words, by="word") 
```

<p style="color:blue"> *To compare the two books, we merge the calculated term frequencies into one data frame and replace any missing entries (corresponding to the word only occurring in one book) with a value of 0:*</p>

```{r}
Time_War <- full_join( Time, War, by = "word" ) %>%
  rename( tf.Time=tf.x, tf.War=tf.y ) %>%
  mutate( tf.Time = case_when( is.na(tf.Time) == TRUE ~ 0, .default = tf.Time ),
          tf.War = case_when( is.na(tf.War) == TRUE ~ 0, .default = tf.War ) )
```

<p style="color:blue"> *As in Section 3.3.1, we plot the term frequencies for the different words against each other:*</p>

```{r, warning=FALSE, fig.align='center', out.width='50%'}
ggplot( Time_War, aes( x=tf.Time, y=tf.War ) ) + geom_point() + 
  geom_abline( color="gray40", linetype=2 ) +
  geom_text( aes(label=word), check_overlap = TRUE, vjust=1 ) + 
  coord_trans( x="sqrt", y="sqrt" ) + theme_bw() +
  labs( x="Term Frequency in The Time Machine",
        y="Term Frequency in The War of the Worlds" )
```

<p style="color:blue"> *We see that the term frequency of the most common words is quite similar across the two books. The plot also shows some points along the axes, which indicates that some words only occur relatively frequently in one of the books (of they occur in two books at all). To explore some aspects in more detail, we remove stop words and extract the ten most common words (excluding stop words) for each book:*</p>

```{r, out.width='50%', fig.align='center', fig.height=4, fig.width=7}
Time_Machine <- Time_War %>%
  slice_max( tf.Time, n=10 ) %>%
  mutate( word = reorder(word,tf.Time) ) %>%
  ggplot( aes( x=tf.Time, y=word ) ) + geom_col() + 
  labs( x="Term frequency", y="Word", title = "Most common words in The Time Machine" ) + 
  theme_bw() +
  theme( axis.title=element_text(size=17), axis.text=element_text(size=15) )
```

```{r}
War_of_Worlds <- Time_War %>%
  slice_max( tf.War, n=10 ) %>%
  mutate( word = reorder(word,tf.War) ) %>%
  ggplot( aes( x=tf.War, y=word ) ) + geom_col() + 
  labs( x="Term frequency", y="Word", title = "Most common words in War of the Worlds" ) + 
  theme_bw() +
  theme( axis.title=element_text(size=17), axis.text=element_text(size=15) )
```

<p style="color:blue"> *Let's place the two plots next to each other:*</p>

```{r, out.width='100%', fig.align='center', fig.height=4, fig.width=15}
library(patchwork)
Time_Machine + War_of_Worlds
```

<p style="color:blue"> *We find some words which may be quite specific to one of the two books: "martian", "morlocks" and "weena". The other words are likely less specific and occur in both books. So we may conclude that the two books differ in terms of words describing the setting and the character names, but the other words exhibit a similar term frequency.*</p>

b) How do "The Time Machine" and "The War of the Worlds" differ regarding their sentiment?

<p style="color:blue"> *We take the AFINN sentiment lexicon (but you can also take the Bing lexicon):*</p>

```{r}
AFINN <- read.csv("AFINN Sentiment Lexicon.csv")
```

<p style="color:blue"> *We aim to explore how the AFINN sentiment score, which we take as the sum of sentiments up to that point, evolves across the book. One option (not necessarily the best) is to produce plots as in Section 3.2.2:*</p>

```{r, out.width='90%', fig.align='center', fig.height=4, fig.width=8}
Time_sentiment <- Time_raw %>%
  mutate( line = row_number() ) %>%
  unnest_tokens( word, text ) %>%
  mutate( word = gsub( "_", "", word ) ) %>%
  inner_join( AFINN, by="word" ) %>%
  mutate( sentiment = cumsum( value ) ) %>%
  ggplot( aes( x=line, y=sentiment ) ) + geom_line( linewidth=1.2 ) +
  theme_bw() + labs( x="Line number", y="Sentiment",
                     title="The Time Machine" )

War_sentiment <- War_raw %>%
  mutate( line = row_number() ) %>%
  unnest_tokens( word, text ) %>%
  mutate( word = gsub( "_", "", word ) ) %>%
  inner_join( AFINN, by="word" ) %>%
  mutate( sentiment = cumsum( value ) ) %>%
  ggplot( aes( x=line, y=sentiment ) ) + geom_line( linewidth=1.2 ) +
  theme_bw() + labs( x="Line number", y="Sentiment", 
                     title="War of the Worlds" )

Time_sentiment + War_sentiment
```

<p style="color:blue"> *Looking at the plots, the analysis suggests that the two books are markedly different in terms of sentiment: The first half of "The Time Machine" seems positive, before turning "negative", while "War of the Worlds" seems to have a "negative" sentiment throughout. However, we have to be careful since the sentiment has been aggregated. For those of you familiar with the books, you may argue that these plots are not reflective of how the sentiment evolves and that it would be better to calculate sentiment for the individual chapters.*</p>


#### Bonus

<p style="color:blue">*Another option is to create bar plots which gives a sentiment score for every chapter of the book. Producing these plots is a bit challenging here since chapters do not start with the word "chapter". A close look at the text data, suggests that each chapter starts with string of the form "I.", "II.", .., that is, they are numbered using roman numerals. We can create these strings using *</p>

```{r}
ChapterNumber <- as.roman(1:99)
ChapterNumber <- paste( ChapterNumber, ".", sep="" )
```

<p style="color:blue">*Having created these strings, we can now identify the lines which correspond to the start of a chapter and subsequently derive the chapter number for each line of text*</p>

```{r}
War <- War_raw %>%
  mutate( Chapter = case_when( text %in% ChapterNumber ~ 1, .default = 0) ) %>%
  mutate( Chapter = cumsum(Chapter) )
```

<p style="color:blue">*In the same way wee find that in "The Time Machine" chapters start with " I.", " II.". So we follow a very similar strategy as for "War of the Worlds":*</p>

```{r}
ChapterNumber <- paste( " ", ChapterNumber, sep="" )
Time <- Time_raw %>%
  mutate( Chapter = case_when( text %in% ChapterNumber ~ 1, .default = 0) ) %>%
  mutate( Chapter = cumsum(Chapter) )
```

<p style="color:blue">*Now we can derive the sentiment scores for each of the two books using the same approach as in Section 3.2.2 of the lecture notes*</p>

```{r, message=FALSE}
Time_AFINN   <- Time %>%
  unnest_tokens( word, text ) %>%
  mutate( word = gsub("_", "", word ) ) %>%
  inner_join( AFINN, by="word" ) %>%
  group_by( Chapter ) %>%
  summarise( sentiment = sum( value ) )

War_AFINN   <- War %>%
  unnest_tokens( word, text ) %>%
  mutate( word = gsub("_", "", word ) ) %>%
  inner_join( AFINN, by="word" ) %>%
  group_by( Chapter ) %>%
  summarise( sentiment = sum( value ) )
```     

<p style="color:blue">*Having derived the sentiment score for each book and chapter, we visualize the results:*</p>

```{r, out.width='90%', fig.align='center', fig.height=4, fig.width=8}
Time_AFINN_plot <- ggplot( Time_AFINN, aes( x=Chapter, y=sentiment ) ) +
  geom_col() + theme_bw() + 
  labs( x="Chapter", y="AFINN sentiment score", title = "The Time Machine" )

War_AFINN_plot <- ggplot( War_AFINN, aes( x=Chapter, y=sentiment ) ) +
  geom_col() + theme_bw() + 
  labs( x="Chapter", y="AFINN sentiment score", title="War of the Worlds" )

Time_AFINN_plot + War_AFINN_plot
```
